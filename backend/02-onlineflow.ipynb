{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9b0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9bde539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.databases.management.longterm import LongTermManagement, LongTerm\n",
    "from package.databases.management.document import DocumentManagement, Document\n",
    "from package.databases.models.document import DocumentStatus\n",
    "from package.databases.management.term import TermManagement, Term\n",
    "from package.databases.session import Depends, get_session\n",
    "\n",
    "ltm = LongTermManagement()\n",
    "dm = DocumentManagement()\n",
    "tm = TermManagement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6056ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\broai-arai\\backend\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading model from: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 30002.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from package.embedding.baai import BAAIEmbedding\n",
    "\n",
    "embedder = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7431f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.cross_encoder.cross_encoder import ReRanker\n",
    "\n",
    "reranker = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b1636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.llm.ollama import BedrockOllamaChat\n",
    "\n",
    "model = BedrockOllamaChat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb24115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.agents.term_detector import TermDetector\n",
    "\n",
    "term_detector = TermDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30188e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(source='AcuRank Uncertainty-Aware Adaptive Computation for Listwise Reranking.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 13, 56, 29, 16229), id='af617836-a979-448c-bfcf-7d6385fd12a1', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 13, 56, 29, 16229)),\n",
       " Document(source='Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 13, 58, 1, 676670), id='06ad7bfc-20e0-40fb-a806-66c8f871e96a', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 13, 58, 1, 676670)),\n",
       " Document(source='ClueAnchor Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 13, 59, 6, 52446), id='44316211-6f8d-463a-ab44-fad555690d5c', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 13, 59, 6, 52446)),\n",
       " Document(source='DiscoVLA Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 0, 39, 661247), id='4cc494d9-fe5e-41b9-9b27-850b85a142bd', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 0, 39, 661247)),\n",
       " Document(source='EXP4FUSE A RANK FUSION FRAMEWORK FOR ENHANCED SPARSE RETRIEVAL USING LARGE LANGUAGE MODEL-BASED QUERY EXPANSION.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 1, 20, 305185), id='98c7c09d-3cd0-43b5-9167-c1341a3e3b16', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 1, 20, 305185)),\n",
       " Document(source='GainRAG Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 2, 2, 161345), id='52034070-d130-4886-bb30-3c3125c961cb', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 2, 2, 161345)),\n",
       " Document(source='GenKI Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 2, 38, 325921), id='d402aeb9-3e82-4c2a-a9ba-f5bdda533b58', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 2, 38, 325921)),\n",
       " Document(source='HippoRAG Neurobiologically Inspired Long-Term Memory for Large Language Models.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 3, 30, 755746), id='95517b4d-6753-415d-93b2-e31e21c26df2', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 3, 30, 755746)),\n",
       " Document(source='LlamaRec-LKG-RAG A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 5, 1, 855622), id='24f33744-2836-4c57-8108-465065b1f793', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 5, 1, 855622)),\n",
       " Document(source='LOGICOL Logically-Informed Contrastive Learning for Set-based Dense Retrieval.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 5, 49, 547401), id='f38e0305-f464-4954-9fb4-809765384259', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 5, 49, 547401)),\n",
       " Document(source='MASKSEARCH A Universal Pre-Training Framework to Enhance Agentic Search Capability.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 6, 22, 943063), id='fc0a57fd-6c7e-464d-b819-5e2c31a84ef4', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 6, 22, 943063)),\n",
       " Document(source='PAKTON A Multi-Agent Framework for Question Answering in Long Legal Agreements.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 7, 34, 854155), id='b3bc7aa5-d5b9-4d20-8e35-9a208a8e8b0e', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 7, 34, 854155)),\n",
       " Document(source='POQD Performance-Oriented Query Decomposer for Multi-vector retrieval.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 9, 23, 179433), id='18342d7e-0d2f-4b74-a1d9-0caadffddddf', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 9, 23, 179433)),\n",
       " Document(source='RARE Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 10, 14, 131480), id='3884cc52-33bd-43a2-aa39-8747a1b74257', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 10, 14, 131480)),\n",
       " Document(source='REARANK Reasoning Re-ranking Agent via Reinforcement Learning.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 11, 8, 975456), id='e809f8ae-921d-4130-bafd-f074319b3ed6', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 11, 8, 975456)),\n",
       " Document(source='SlideCoder Layout-aware RAG-enhanced Hierarchical Slide Generation from Design.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 12, 15, 166790), id='cdbe2918-c993-42ff-9354-eb4ffc266de0', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 12, 15, 166790)),\n",
       " Document(source='SORCE Small Object Retrieval in Complex Environments.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 13, 4, 246573), id='1736cec1-9901-4634-ac31-2bac14137406', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 13, 4, 246573)),\n",
       " Document(source='TracLLM A Generic Framework for Attributing Long Context LLMs.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 4, 14, 13, 48, 448215), id='ed281e35-924a-45e9-9fa8-d4238213437d', status=<DocumentStatus.PENDING: 'pending'>, updated_at=datetime.datetime(2025, 7, 4, 14, 13, 48, 448215))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = dm.read_documents(session=Depends(get_session))\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe760482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./dataset/trainset.json\", 'r', encoding='utf-8') as f:\n",
    "    trainset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654fcc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in trainset:\n",
    "    ts['metadata']['source'] = ts['metadata']['source'].replace(\":\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6fbec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Response(BaseModel):\n",
    "    question:str\n",
    "    ground_truth:str\n",
    "    predict:str | None\n",
    "    source:str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f13a20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def bare_model(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant.\"\n",
    "        query = f\"{question}\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5930631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:45<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "bare_response = bare_model(trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "266432a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "metric = \"rougeL\"\n",
    "scorer = rouge_scorer.RougeScorer(rouge_types=[metric], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40c6b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(method, responses):\n",
    "    scores = []\n",
    "    for response in responses:\n",
    "        score = scorer.score(target=response.ground_truth, prediction=response.predict)[metric]\n",
    "        scores.append(\n",
    "            dict(\n",
    "                method=method,\n",
    "                question=response.question,\n",
    "                ground_truth=response.ground_truth,\n",
    "                predict=response.predict,\n",
    "                precision=score.precision,\n",
    "                recall=score.recall,\n",
    "                fmeasure=score.fmeasure\n",
    "            )\n",
    "        )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba234f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bare_scores = get_score(\"bare_model\", bare_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a92a96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided CONTEXT.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        vector = embedder.run(sentences=[question])[0]\n",
    "        longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source])\n",
    "        contexts = \"\\n\".join([l.raw for l in longterms])\n",
    "        query = f\"CONTEXT:\\n\\n{contexts}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19518fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:53<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_response = simple_rag(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f3c88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_scores = get_score(method=\"simple_rag_score\", responses=simple_rag_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f36e3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_rerank(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided CONTEXT.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        vector = embedder.run(sentences=[question])[0]\n",
    "        longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source], limit=10)\n",
    "        reranked_longterms, _ = reranker.run(search_query=question, longterms=longterms, embed_method=\"raw\")\n",
    "        contexts = \"\\n\".join([l.raw for l in reranked_longterms])\n",
    "        query = f\"CONTEXT:\\n\\n{contexts}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0328d57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:57<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "rag_with_rerank_response = rag_with_rerank(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "157c7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_with_rerank_scores = get_score(method=\"rag_with_rerank\", responses=rag_with_rerank_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c09d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_term(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        potential_terms = term_detector.run(message=question)\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided TERM.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        terms = []\n",
    "        for term in potential_terms:\n",
    "            similar_terms = tm.read_similar_terms(term=term, session=Depends(get_session), document_ids=[document.id])\n",
    "            terms.extend([st for st in similar_terms if st])\n",
    "        evidences = \"\\n\".join([t.evidence for t in terms])\n",
    "\n",
    "        \n",
    "        # vector = embedder.run(sentences=[question])[0]\n",
    "    #     longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source])\n",
    "    #     contexts = \"\\n\".join([l.raw for l in longterms])\n",
    "        query = f\"TERM:\\n\\n{evidences}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6a6a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:19<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_term_response = simple_rag_term(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02fe2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_term_scores = get_score(method=\"simple_rag_term\", responses=simple_rag_term_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95eb1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_term_context(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        potential_terms = term_detector.run(message=question)\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided TERM and CONTEXT.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        terms = []\n",
    "        for term in potential_terms:\n",
    "            similar_terms = tm.read_similar_terms(term=term, session=Depends(get_session), document_ids=[document.id])\n",
    "            terms.extend([st for st in similar_terms if st])\n",
    "        evidences = \"\\n\".join([t.evidence for t in terms])\n",
    "        vector = embedder.run(sentences=[question])[0]\n",
    "        longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source])\n",
    "        contexts = \"\\n\".join([l.raw for l in longterms])\n",
    "        query = f\"TERM:\\n\\n{evidences}\\n\\nCONTEXT:\\n\\n{contexts}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebc1d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:39<00:00,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_term_context_response = simple_rag_term_context(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0713b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_term_context_scores = get_score(method=\"simple_rag_term_context\", responses=simple_rag_term_context_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "414b43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_term_context_with_rerank(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        potential_terms = term_detector.run(message=question)\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided TERM and CONTEXT.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        terms = []\n",
    "        for term in potential_terms:\n",
    "            similar_terms = tm.read_similar_terms(term=term, session=Depends(get_session), document_ids=[document.id])\n",
    "            terms.extend([st for st in similar_terms if st])\n",
    "        evidences = \"\\n\".join([t.evidence for t in terms])\n",
    "        vector = embedder.run(sentences=[question])[0]\n",
    "        longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source])\n",
    "        reranked_longterms, _ = reranker.run(search_query=question, longterms=longterms, embed_method=\"raw\")\n",
    "        contexts = \"\\n\".join([l.raw for l in reranked_longterms])        \n",
    "        query = f\"TERM:\\n\\n{evidences}\\n\\nCONTEXT:\\n\\n{contexts}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a37a142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:40<00:00,  3.04s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_term_context_with_rerank_response = simple_rag_term_context_with_rerank(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7007a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_term_context_with_rerank_scores = get_score(method=\"simple_rag_term_context_with_rerank\", responses=simple_rag_term_context_with_rerank_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b5f9e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fmeasure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rag_with_rerank</th>\n",
       "      <td>0.397900</td>\n",
       "      <td>0.825386</td>\n",
       "      <td>0.521420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_context_with_rerank</th>\n",
       "      <td>0.379236</td>\n",
       "      <td>0.824551</td>\n",
       "      <td>0.498211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_context</th>\n",
       "      <td>0.375268</td>\n",
       "      <td>0.840273</td>\n",
       "      <td>0.495892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_score</th>\n",
       "      <td>0.370164</td>\n",
       "      <td>0.800594</td>\n",
       "      <td>0.489102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term</th>\n",
       "      <td>0.346263</td>\n",
       "      <td>0.607447</td>\n",
       "      <td>0.414338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bare_model</th>\n",
       "      <td>0.042160</td>\n",
       "      <td>0.194086</td>\n",
       "      <td>0.065441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     precision    recall  fmeasure\n",
       "method                                                            \n",
       "rag_with_rerank                       0.397900  0.825386  0.521420\n",
       "simple_rag_term_context_with_rerank   0.379236  0.824551  0.498211\n",
       "simple_rag_term_context               0.375268  0.840273  0.495892\n",
       "simple_rag_score                      0.370164  0.800594  0.489102\n",
       "simple_rag_term                       0.346263  0.607447  0.414338\n",
       "bare_model                            0.042160  0.194086  0.065441"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datas = [\n",
    "    bare_scores, \n",
    "    simple_rag_scores, \n",
    "    rag_with_rerank_scores,\n",
    "    simple_rag_term_scores,\n",
    "    simple_rag_term_context_scores,\n",
    "    simple_rag_term_context_with_rerank_scores\n",
    "]\n",
    "\n",
    "experiments = pd.concat([pd.DataFrame(data) for data in datas])\n",
    "\n",
    "benchmark = experiments.groupby('method').agg(\n",
    "    precision=pd.NamedAgg(column='precision', aggfunc='mean'),\n",
    "    recall=pd.NamedAgg(column='recall', aggfunc='mean'),\n",
    "    fmeasure=pd.NamedAgg(column='fmeasure', aggfunc='mean'),\n",
    ").sort_values(\"fmeasure\", ascending=False)\n",
    "\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa9aae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.to_csv(\"./dataset/experiments.csv\", index=False)\n",
    "benchmark.to_csv(\"./dataset/benchmark.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "456ac038",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d4a3f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What does DiscoVLA stand for?\n",
      "ANSWER: Discrepancy Reduction in Vision, Language, and Alignment\n",
      "DiscoVLA stands for Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for \"Discrepancy Reduction in Vision, Language, and Alignment\" for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for \"Discrepancy Reduction in Vision, Language, and Alignment\" for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for \"Disc repancy Reducti o n in V ision, L anguage, and A lignment\".\n",
      "==========\n",
      "I couldn't find any information about \"DiscoVLA.\" However, I found that VLA stands for Very Large Array, which is a radio astronomy observatory in New Mexico, USA.\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>predict</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_term</td>\n",
       "      <td>DiscoVLA stands for Discrepancy Reduction in V...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rag_with_rerank</td>\n",
       "      <td>DiscoVLA stands for Discrepancy Reduction in V...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_term_context</td>\n",
       "      <td>DiscoVLA stands for \"Discrepancy Reduction in ...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_term_context_with_rerank</td>\n",
       "      <td>DiscoVLA stands for \"Discrepancy Reduction in ...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_score</td>\n",
       "      <td>DiscoVLA stands for \"Disc repancy Reducti o n ...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bare_model</td>\n",
       "      <td>I couldn't find any information about \"DiscoVL...</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                method  \\\n",
       "0                      simple_rag_term   \n",
       "0                      rag_with_rerank   \n",
       "0              simple_rag_term_context   \n",
       "0  simple_rag_term_context_with_rerank   \n",
       "0                     simple_rag_score   \n",
       "0                           bare_model   \n",
       "\n",
       "                                             predict  precision    recall  \\\n",
       "0  DiscoVLA stands for Discrepancy Reduction in V...   0.437500  1.000000   \n",
       "0  DiscoVLA stands for Discrepancy Reduction in V...   0.437500  1.000000   \n",
       "0  DiscoVLA stands for \"Discrepancy Reduction in ...   0.437500  1.000000   \n",
       "0  DiscoVLA stands for \"Discrepancy Reduction in ...   0.437500  1.000000   \n",
       "0  DiscoVLA stands for \"Disc repancy Reducti o n ...   0.125000  0.285714   \n",
       "0  I couldn't find any information about \"DiscoVL...   0.035714  0.142857   \n",
       "\n",
       "   fmeasure  \n",
       "0  0.608696  \n",
       "0  0.608696  \n",
       "0  0.608696  \n",
       "0  0.608696  \n",
       "0  0.173913  \n",
       "0  0.057143  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = trainset[idx]['question']\n",
    "ground_truth = trainset[idx]['answer']\n",
    "idx += 1\n",
    "print(\"QUESTION:\", check)\n",
    "print(\"ANSWER:\", ground_truth)\n",
    "check_df = experiments.loc[experiments['question']==check,:].drop([\"question\", \"ground_truth\"], axis=1).sort_values(\"fmeasure\", ascending=False)\n",
    "for p in check_df['predict'].tolist():\n",
    "    print(p)\n",
    "    print(\"=\"*10)\n",
    "check_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac668266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
