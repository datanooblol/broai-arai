{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9b0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9bde539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.databases.management.longterm import LongTermManagement, LongTerm\n",
    "from package.databases.management.document import DocumentManagement, Document\n",
    "from package.databases.models.document import DocumentStatus\n",
    "from package.databases.management.term import TermManagement, Term\n",
    "from package.databases.session import Depends, get_session\n",
    "\n",
    "ltm = LongTermManagement()\n",
    "dm = DocumentManagement()\n",
    "tm = TermManagement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6056ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\broai-arai\\backend\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading model from: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from package.embedding.baai import BAAIEmbedding\n",
    "\n",
    "embedder = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7431f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.cross_encoder.cross_encoder import ReRanker\n",
    "\n",
    "reranker = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b1636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.llm.ollama import BedrockOllamaChat\n",
    "\n",
    "model = BedrockOllamaChat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb24115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.agents.term_detector import TermDetector\n",
    "\n",
    "term_detector = TermDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30188e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='050e6a5c-8b0f-4649-bd05-547cf962f701', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='RARE Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 48, 50, 72407)),\n",
       " Document(id='0ab7ef49-011d-4409-ac69-254d6b88eba1', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='ClueAnchor Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 40, 10, 746752)),\n",
       " Document(id='16514705-ab87-4115-883c-8ebf8ed58f9d', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='DiscoVLA Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 41, 24, 117745)),\n",
       " Document(id='21a9b572-b1be-43e7-a143-b391ab52e966', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='GainRAG Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 42, 45, 868947)),\n",
       " Document(id='24a2dcc3-c8ec-445e-8336-816971796e30', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='MASKSEARCH A Universal Pre-Training Framework to Enhance Agentic Search Capability.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 46, 0, 48603)),\n",
       " Document(id='25d85652-170a-4bbe-8f9e-76c03bc4b75c', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='SORCE Small Object Retrieval in Complex Environments.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 50, 49, 790265)),\n",
       " Document(id='52a735db-6cf1-402c-be43-026754b90479', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='SlideCoder Layout-aware RAG-enhanced Hierarchical Slide Generation from Design.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 50, 12, 65129)),\n",
       " Document(id='77b299e1-3a35-45f1-9bee-b943d5764c10', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='REARANK Reasoning Re-ranking Agent via Reinforcement Learning.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 49, 26, 336569)),\n",
       " Document(id='9099006f-03c4-4b0d-a7f3-0538fea835c6', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='POQD Performance-Oriented Query Decomposer for Multi-vector retrieval.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 48, 11, 253872)),\n",
       " Document(id='93c2444d-f2c3-4609-8bd9-badaa0930ac2', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='GenKI Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 43, 13, 455326)),\n",
       " Document(id='a16527e3-50b2-41a2-8c59-9ee0968ce39d', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='TracLLM A Generic Framework for Attributing Long Context LLMs.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 51, 24, 874809)),\n",
       " Document(id='a57cc0be-4da7-4d4f-ba98-5619fe05dd5c', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 39, 28, 66601)),\n",
       " Document(id='bee0068f-e22d-42be-9139-064d76f655b9', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='LlamaRec-LKG-RAG A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 45, 0, 569345)),\n",
       " Document(id='c73c3a63-4d4a-438e-877c-54a7e6067d2d', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='EXP4FUSE A RANK FUSION FRAMEWORK FOR ENHANCED SPARSE RETRIEVAL USING LARGE LANGUAGE MODEL-BASED QUERY EXPANSION.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 42, 5, 827644)),\n",
       " Document(id='cbd3c93d-09d3-4cc2-bc95-d36c2e4aefa9', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='PAKTON A Multi-Agent Framework for Question Answering in Long Legal Agreements.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 46, 49, 968420)),\n",
       " Document(id='d62d0f7b-ea37-42f7-8402-cf68a4541717', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='HippoRAG Neurobiologically Inspired Long-Term Memory for Large Language Models.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 43, 53, 940690)),\n",
       " Document(id='d7d8642f-4151-42b1-b901-f821056f6e2d', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='LOGICOL Logically-Informed Contrastive Learning for Set-based Dense Retrieval.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 45, 34, 806668)),\n",
       " Document(id='f52dc846-5415-4792-934c-a5a01198e0c5', status=<DocumentStatus.COMPLETED: 'completed'>, updated_at=datetime.datetime(2025, 7, 3, 13, 44, 16, 562913), source='AcuRank Uncertainty-Aware Adaptive Computation for Listwise Reranking.pdf', type='pdf', created_at=datetime.datetime(2025, 7, 2, 17, 36, 21, 997893))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = dm.read_documents(session=Depends(get_session))\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe760482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./dataset/trainset.json\", 'r', encoding='utf-8') as f:\n",
    "    trainset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654fcc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in trainset:\n",
    "    ts['metadata']['source'] = ts['metadata']['source'].replace(\":\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6fbec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Response(BaseModel):\n",
    "    question:str\n",
    "    ground_truth:str\n",
    "    predict:str | None\n",
    "    source:str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f13a20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def bare_model(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant.\"\n",
    "        query = f\"{question}\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5930631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:37<00:00,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "bare_response = bare_model(trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "266432a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "metric = \"rougeL\"\n",
    "scorer = rouge_scorer.RougeScorer(rouge_types=[metric], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40c6b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(method, responses):\n",
    "    scores = []\n",
    "    for response in responses:\n",
    "        score = scorer.score(target=response.ground_truth, prediction=response.predict)[metric]\n",
    "        scores.append(\n",
    "            dict(\n",
    "                method=method,\n",
    "                question=response.question,\n",
    "                ground_truth=response.ground_truth,\n",
    "                predict=response.predict,\n",
    "                precision=score.precision,\n",
    "                recall=score.recall,\n",
    "                fmeasure=score.fmeasure\n",
    "            )\n",
    "        )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba234f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bare_scores = get_score(\"bare_model\", bare_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a92a96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided CONTEXT.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        vector = embedder.run(sentences=[question])[0]\n",
    "        longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source])\n",
    "        contexts = \"\\n\".join([l.raw for l in longterms])\n",
    "        query = f\"CONTEXT:\\n\\n{contexts}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19518fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:57<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_response = simple_rag(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f3c88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_scores = get_score(method=\"simple_rag_score\", responses=simple_rag_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f36e3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_rerank(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided CONTEXT.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        vector = embedder.run(sentences=[question])[0]\n",
    "        longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source], limit=10)\n",
    "        reranked_longterms, _ = reranker.run(search_query=question, longterms=longterms, embed_method=\"raw\")\n",
    "        contexts = \"\\n\".join([l.raw for l in reranked_longterms])\n",
    "        query = f\"CONTEXT:\\n\\n{contexts}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0328d57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:58<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "rag_with_rerank_response = rag_with_rerank(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "157c7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_with_rerank_scores = get_score(method=\"rag_with_rerank\", responses=rag_with_rerank_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c09d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_term(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        potential_terms = term_detector.run(message=question)\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided TERM.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        terms = []\n",
    "        for term in potential_terms:\n",
    "            similar_terms = tm.read_similar_terms(term=term, session=Depends(get_session), document_ids=[document.id])\n",
    "            terms.extend([st for st in similar_terms if st])\n",
    "        evidences = \"\\n\".join([t.evidence for t in terms])\n",
    "\n",
    "        \n",
    "        # vector = embedder.run(sentences=[question])[0]\n",
    "    #     longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source])\n",
    "    #     contexts = \"\\n\".join([l.raw for l in longterms])\n",
    "        query = f\"TERM:\\n\\n{evidences}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6a6a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:20<00:00,  2.44s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_term_response = simple_rag_term(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02fe2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_term_scores = get_score(method=\"simple_rag_term\", responses=simple_rag_term_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95eb1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_term_context(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        potential_terms = term_detector.run(message=question)\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided TERM and CONTEXT.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        terms = []\n",
    "        for term in potential_terms:\n",
    "            similar_terms = tm.read_similar_terms(term=term, session=Depends(get_session), document_ids=[document.id])\n",
    "            terms.extend([st for st in similar_terms if st])\n",
    "        evidences = \"\\n\".join([t.evidence for t in terms])\n",
    "        vector = embedder.run(sentences=[question])[0]\n",
    "        longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source])\n",
    "        contexts = \"\\n\".join([l.raw for l in longterms])\n",
    "        query = f\"TERM:\\n\\n{evidences}\\n\\nCONTEXT:\\n\\n{contexts}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebc1d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:35<00:00,  2.90s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_term_context_response = simple_rag_term_context(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0713b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_term_context_scores = get_score(method=\"simple_rag_term_context\", responses=simple_rag_term_context_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "414b43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_term_context_with_rerank(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        potential_terms = term_detector.run(message=question)\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided TERM and CONTEXT.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        terms = []\n",
    "        for term in potential_terms:\n",
    "            similar_terms = tm.read_similar_terms(term=term, session=Depends(get_session), document_ids=[document.id])\n",
    "            terms.extend([st for st in similar_terms if st])\n",
    "        evidences = \"\\n\".join([t.evidence for t in terms])\n",
    "        vector = embedder.run(sentences=[question])[0]\n",
    "        longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source])\n",
    "        reranked_longterms, _ = reranker.run(search_query=question, longterms=longterms, embed_method=\"raw\")\n",
    "        contexts = \"\\n\".join([l.raw for l in reranked_longterms])        \n",
    "        query = f\"TERM:\\n\\n{evidences}\\n\\nCONTEXT:\\n\\n{contexts}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a37a142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:39<00:00,  3.00s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_term_context_with_rerank_response = simple_rag_term_context_with_rerank(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7007a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_term_context_with_rerank_scores = get_score(method=\"simple_rag_term_context_with_rerank\", responses=simple_rag_term_context_with_rerank_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b5f9e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fmeasure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_context</th>\n",
       "      <td>0.404230</td>\n",
       "      <td>0.867416</td>\n",
       "      <td>0.531810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rag_with_rerank</th>\n",
       "      <td>0.396290</td>\n",
       "      <td>0.826829</td>\n",
       "      <td>0.518620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_score</th>\n",
       "      <td>0.368110</td>\n",
       "      <td>0.800594</td>\n",
       "      <td>0.487404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_context_with_rerank</th>\n",
       "      <td>0.362819</td>\n",
       "      <td>0.804194</td>\n",
       "      <td>0.481223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term</th>\n",
       "      <td>0.406814</td>\n",
       "      <td>0.607140</td>\n",
       "      <td>0.442018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bare_model</th>\n",
       "      <td>0.041691</td>\n",
       "      <td>0.194941</td>\n",
       "      <td>0.064541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     precision    recall  fmeasure\n",
       "method                                                            \n",
       "simple_rag_term_context               0.404230  0.867416  0.531810\n",
       "rag_with_rerank                       0.396290  0.826829  0.518620\n",
       "simple_rag_score                      0.368110  0.800594  0.487404\n",
       "simple_rag_term_context_with_rerank   0.362819  0.804194  0.481223\n",
       "simple_rag_term                       0.406814  0.607140  0.442018\n",
       "bare_model                            0.041691  0.194941  0.064541"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datas = [\n",
    "    bare_scores, \n",
    "    simple_rag_scores, \n",
    "    rag_with_rerank_scores,\n",
    "    simple_rag_term_scores,\n",
    "    simple_rag_term_context_scores,\n",
    "    simple_rag_term_context_with_rerank_scores\n",
    "]\n",
    "\n",
    "experiments = pd.concat([pd.DataFrame(data) for data in datas])\n",
    "\n",
    "benchmark = experiments.groupby('method').agg(\n",
    "    precision=pd.NamedAgg(column='precision', aggfunc='mean'),\n",
    "    recall=pd.NamedAgg(column='recall', aggfunc='mean'),\n",
    "    fmeasure=pd.NamedAgg(column='fmeasure', aggfunc='mean'),\n",
    ").sort_values(\"fmeasure\", ascending=False)\n",
    "\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa9aae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.to_csv(\"./dataset/experiments.csv\", index=False)\n",
    "benchmark.to_csv(\"./dataset/benchmark.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "456ac038",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4a3f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What does DiscoVLA stand for?\n",
      "ANSWER: Discrepancy Reduction in Vision, Language, and Alignment\n",
      "DiscoVLA stands for Discrepancy Reduction in Vision, Language, and Alignment.\n",
      "==========\n",
      "DiscoVLA stands for Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for \"Disc repancy Reducti o n in V ision, L anguage, and A lignment\".\n",
      "==========\n",
      "DiscoVLA stands for \"Disc repancy Reducti o n in V ision, L anguage, and A lignment\".\n",
      "==========\n",
      "I couldn't find any information on \"DiscoVLA\". However, I found that VLA stands for Very Large Array, which is a radio astronomy observatory in New Mexico, USA.\n",
      "==========\n",
      "Unfortunately, the provided TERM does not explicitly state what DiscoVLA stands for. However, based on the context, it seems to be a type of algorithm or model, possibly related to machine learning or data analysis.\n",
      "\n",
      "Given the mention of \"Œ±\" (alpha) and \"parameter insensitivity,\" it appears that DiscoVLA is a model that achieves optimal performance at a certain threshold (Œ± = 0.3) and becomes less sensitive to parameter changes beyond that point.\n",
      "\n",
      "If you have any more information or context about DiscoVLA, I may be able to provide a more specific answer.\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>predict</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_term_context</td>\n",
       "      <td>DiscoVLA stands for Discrepancy Reduction in V...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rag_with_rerank</td>\n",
       "      <td>DiscoVLA stands for Discrepancy Reduction in V...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_term_context_with_rerank</td>\n",
       "      <td>DiscoVLA stands for \"Disc repancy Reducti o n ...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_score</td>\n",
       "      <td>DiscoVLA stands for \"Disc repancy Reducti o n ...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bare_model</td>\n",
       "      <td>I couldn't find any information on \"DiscoVLA\"....</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_term</td>\n",
       "      <td>Unfortunately, the provided TERM does not expl...</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.020619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                method  \\\n",
       "0              simple_rag_term_context   \n",
       "0                      rag_with_rerank   \n",
       "0  simple_rag_term_context_with_rerank   \n",
       "0                     simple_rag_score   \n",
       "0                           bare_model   \n",
       "0                      simple_rag_term   \n",
       "\n",
       "                                             predict  precision    recall  \\\n",
       "0  DiscoVLA stands for Discrepancy Reduction in V...   0.700000  1.000000   \n",
       "0  DiscoVLA stands for Discrepancy Reduction in V...   0.437500  1.000000   \n",
       "0  DiscoVLA stands for \"Disc repancy Reducti o n ...   0.125000  0.285714   \n",
       "0  DiscoVLA stands for \"Disc repancy Reducti o n ...   0.125000  0.285714   \n",
       "0  I couldn't find any information on \"DiscoVLA\"....   0.035714  0.142857   \n",
       "0  Unfortunately, the provided TERM does not expl...   0.011111  0.142857   \n",
       "\n",
       "   fmeasure  \n",
       "0  0.823529  \n",
       "0  0.608696  \n",
       "0  0.173913  \n",
       "0  0.173913  \n",
       "0  0.057143  \n",
       "0  0.020619  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = trainset[idx]['question']\n",
    "ground_truth = trainset[idx]['answer']\n",
    "idx += 1\n",
    "print(\"QUESTION:\", check)\n",
    "print(\"ANSWER:\", ground_truth)\n",
    "check_df = experiments.loc[experiments['question']==check,:].drop([\"question\", \"ground_truth\"], axis=1).sort_values(\"fmeasure\", ascending=False)\n",
    "for p in check_df['predict'].tolist():\n",
    "    print(p)\n",
    "    print(\"=\"*10)\n",
    "check_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac668266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
