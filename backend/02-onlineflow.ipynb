{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9b0cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9bde539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.databases.management.longterm import LongTermManagement, LongTerm\n",
    "from package.databases.management.document import DocumentManagement, Document\n",
    "from package.databases.models.document import DocumentStatus\n",
    "from package.databases.management.term import TermManagement, Term\n",
    "from package.databases.session import Depends, get_session\n",
    "\n",
    "ltm = LongTermManagement()\n",
    "dm = DocumentManagement()\n",
    "tm = TermManagement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b6056ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\broai-arai\\backend\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading model from: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 30 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from package.embedding.baai import BAAIEmbedding\n",
    "\n",
    "embedder = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7431f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.cross_encoder.cross_encoder import ReRanker\n",
    "\n",
    "reranker = ReRanker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b1636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.llm.ollama import BedrockOllamaChat\n",
    "\n",
    "model = BedrockOllamaChat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb24115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.agents.term_detector import TermDetector\n",
    "\n",
    "term_detector = TermDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30188e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = dm.read_documents(session=Depends(get_session))\n",
    "# documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe760482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./dataset/trainset.json\", 'r', encoding='utf-8') as f:\n",
    "    trainset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654fcc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in trainset:\n",
    "    ts['metadata']['source'] = ts['metadata']['source'].replace(\":\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6fbec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Response(BaseModel):\n",
    "    question:str\n",
    "    ground_truth:str\n",
    "    predict:str | None\n",
    "    source:str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f13a20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def bare_model(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant.\"\n",
    "        query = f\"{question}\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5930631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [02:12<00:00,  4.02s/it]\n"
     ]
    }
   ],
   "source": [
    "bare_response = bare_model(trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "266432a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "metric = \"rougeL\"\n",
    "scorer = rouge_scorer.RougeScorer(rouge_types=[metric], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40c6b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_score(method, responses):\n",
    "#     scores = []\n",
    "#     for response in responses:\n",
    "#         score = scorer.score(target=response.ground_truth, prediction=response.predict)[metric]\n",
    "#         scores.append(\n",
    "#             dict(\n",
    "#                 method=method,\n",
    "#                 question=response.question,\n",
    "#                 ground_truth=response.ground_truth,\n",
    "#                 predict=response.predict,\n",
    "#                 precision=score.precision,\n",
    "#                 recall=score.recall,\n",
    "#                 fmeasure=score.fmeasure\n",
    "#             )\n",
    "#         )\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02241378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # normalize whitespace\n",
    "    return text\n",
    "\n",
    "def get_score(method, responses):\n",
    "    scores = []\n",
    "    for response in responses:\n",
    "        # Preprocess both texts\n",
    "        ground_truth = preprocess_text(response.ground_truth)\n",
    "        predict = preprocess_text(response.predict)\n",
    "        \n",
    "        score = scorer.score(target=ground_truth, prediction=predict)[metric]\n",
    "        scores.append(dict(\n",
    "            method=method,\n",
    "            question=response.question,\n",
    "            ground_truth=ground_truth,\n",
    "            predict=predict,\n",
    "            precision=score.precision,\n",
    "            recall=score.recall,\n",
    "            fmeasure=score.fmeasure\n",
    "        ))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba234f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bare_scores = get_score(\"bare_model\", bare_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a92a96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided CONTEXT.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        vector = embedder.run(sentences=[question])[0]\n",
    "        longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source])\n",
    "        contexts = \"\\n\".join([l.raw for l in longterms])\n",
    "        query = f\"CONTEXT:\\n\\n{contexts}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19518fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:52<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_response = simple_rag(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f3c88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_scores = get_score(method=\"simple_rag_score\", responses=simple_rag_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f36e3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_rerank(trainset):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided CONTEXT.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        vector = embedder.run(sentences=[question])[0]\n",
    "        longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source], limit=10)\n",
    "        reranked_longterms, _ = reranker.run(search_query=question, longterms=longterms, embed_method=\"raw\")\n",
    "        contexts = \"\\n\".join([l.raw for l in reranked_longterms])\n",
    "        query = f\"CONTEXT:\\n\\n{contexts}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0328d57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [00:52<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "rag_with_rerank_response = rag_with_rerank(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "157c7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_with_rerank_scores = get_score(method=\"rag_with_rerank\", responses=rag_with_rerank_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c09d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_term(trainset, method=\"evidence\"):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        potential_terms = term_detector.run(message=question)\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided TERM.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        terms = []\n",
    "        for term in potential_terms:\n",
    "            similar_terms = tm.read_similar_terms(term=term, session=Depends(get_session), document_ids=[document.id])\n",
    "            terms.extend([st for st in similar_terms if st])\n",
    "        if method == \"evidence\":\n",
    "            evidences = \"\\n\".join([t.evidence for t in terms])\n",
    "        else:\n",
    "            evidences = \"\\n\".join([t.explanation for t in terms])\n",
    "        query = f\"TERM:\\n\\n{evidences}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6a6a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:14<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_term_response = simple_rag_term(trainset, method=\"evidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02fe2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_term_scores = get_score(method=\"simple_rag_term\", responses=simple_rag_term_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32995adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:13<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_term_explanation_response = simple_rag_term(trainset, method=\"explanation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6ae8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_term_explanation_scores = get_score(method=\"simple_rag_term_explanation\", responses=simple_rag_term_explanation_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95eb1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_term_context(trainset, method=\"evidence\"):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        potential_terms = term_detector.run(message=question)\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided TERM and CONTEXT.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        terms = []\n",
    "        for term in potential_terms:\n",
    "            similar_terms = tm.read_similar_terms(term=term, session=Depends(get_session), document_ids=[document.id])\n",
    "            terms.extend([st for st in similar_terms if st])\n",
    "        if method == \"evidence\":\n",
    "            evidences = \"\\n\".join([t.evidence for t in terms])\n",
    "        else:\n",
    "            evidences = \"\\n\".join([t.explanation for t in terms])\n",
    "        vector = embedder.run(sentences=[question])[0]\n",
    "        longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source])\n",
    "        contexts = \"\\n\".join([l.raw for l in longterms])\n",
    "        query = f\"TERM:\\n\\n{evidences}\\n\\nCONTEXT:\\n\\n{contexts}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebc1d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:33<00:00,  2.82s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_term_context_response = simple_rag_term_context(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0713b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_term_context_scores = get_score(method=\"simple_rag_term_context\", responses=simple_rag_term_context_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40a4210c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:30<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_term_explanation_context_response = simple_rag_term_context(trainset, method=\"explanation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07a603ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_term_explanation_context_scores = get_score(method=\"simple_rag_term_explanation_context\", responses=simple_rag_term_explanation_context_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "414b43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_term_context_with_rerank(trainset, method=\"evidence\"):\n",
    "    responses = []\n",
    "    for ts in tqdm(trainset):\n",
    "        source = ts['metadata']['source']\n",
    "        question = ts['question']\n",
    "        potential_terms = term_detector.run(message=question)\n",
    "        ground_truth = ts['answer']\n",
    "        system_prompt=\"You are a helpful assistant. Read QUESTION carefully and give an answer based on the provided TERM and CONTEXT.\"\n",
    "        document = dm.read_document_by_source(source, session=Depends(get_session))\n",
    "        terms = []\n",
    "        for term in potential_terms:\n",
    "            similar_terms = tm.read_similar_terms(term=term, session=Depends(get_session), document_ids=[document.id])\n",
    "            terms.extend([st for st in similar_terms if st])\n",
    "        if method == \"evidence\":\n",
    "            evidences = \"\\n\".join([t.evidence for t in terms])\n",
    "        else:\n",
    "            evidences = \"\\n\".join([t.explanation for t in terms])\n",
    "        vector = embedder.run(sentences=[question])[0]\n",
    "        longterms = ltm.read_similar_text_with_like_source(vector, embed_method=\"raw\", session=Depends(get_session), sources=[document.source])\n",
    "        reranked_longterms, _ = reranker.run(search_query=question, longterms=longterms, embed_method=\"raw\")\n",
    "        contexts = \"\\n\".join([l.raw for l in reranked_longterms])        \n",
    "        query = f\"TERM:\\n\\n{evidences}\\n\\nCONTEXT:\\n\\n{contexts}\\n\\nQUESTION:\\n\\n{question}\\n\\n\"\n",
    "        messages = [model.UserMessage(text=query)]\n",
    "        predict = model.run(system_prompt=system_prompt, messages=messages)\n",
    "        responses.append(\n",
    "            Response(\n",
    "                question=question,\n",
    "                ground_truth=ground_truth,\n",
    "                predict=predict,\n",
    "                source=source\n",
    "            )\n",
    "        )\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a37a142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:31<00:00,  2.77s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_term_context_with_rerank_response = simple_rag_term_context_with_rerank(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7007a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_term_context_with_rerank_scores = get_score(method=\"simple_rag_term_context_with_rerank\", responses=simple_rag_term_context_with_rerank_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5265e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [01:33<00:00,  2.85s/it]\n"
     ]
    }
   ],
   "source": [
    "simple_rag_term_explanation_context_with_rerank_response = simple_rag_term_context_with_rerank(trainset, method=\"explanation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c69068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_term_explanation_context_with_rerank_scores = get_score(method=\"simple_rag_term_explanation_context_with_rerank\", responses=simple_rag_term_explanation_context_with_rerank_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b5f9e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fmeasure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rag_with_rerank</th>\n",
       "      <td>0.396685</td>\n",
       "      <td>0.828041</td>\n",
       "      <td>0.519255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_context_with_rerank</th>\n",
       "      <td>0.379236</td>\n",
       "      <td>0.824551</td>\n",
       "      <td>0.498211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_context</th>\n",
       "      <td>0.372400</td>\n",
       "      <td>0.841716</td>\n",
       "      <td>0.491798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_score</th>\n",
       "      <td>0.360697</td>\n",
       "      <td>0.802277</td>\n",
       "      <td>0.477898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_explanation_context_with_rerank</th>\n",
       "      <td>0.351793</td>\n",
       "      <td>0.768794</td>\n",
       "      <td>0.458704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_explanation_context</th>\n",
       "      <td>0.348908</td>\n",
       "      <td>0.780887</td>\n",
       "      <td>0.451930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term</th>\n",
       "      <td>0.347323</td>\n",
       "      <td>0.610977</td>\n",
       "      <td>0.416818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_explanation</th>\n",
       "      <td>0.294213</td>\n",
       "      <td>0.497340</td>\n",
       "      <td>0.329696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bare_model</th>\n",
       "      <td>0.042348</td>\n",
       "      <td>0.189722</td>\n",
       "      <td>0.065825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 precision    recall  fmeasure\n",
       "method                                                                        \n",
       "rag_with_rerank                                   0.396685  0.828041  0.519255\n",
       "simple_rag_term_context_with_rerank               0.379236  0.824551  0.498211\n",
       "simple_rag_term_context                           0.372400  0.841716  0.491798\n",
       "simple_rag_score                                  0.360697  0.802277  0.477898\n",
       "simple_rag_term_explanation_context_with_rerank   0.351793  0.768794  0.458704\n",
       "simple_rag_term_explanation_context               0.348908  0.780887  0.451930\n",
       "simple_rag_term                                   0.347323  0.610977  0.416818\n",
       "simple_rag_term_explanation                       0.294213  0.497340  0.329696\n",
       "bare_model                                        0.042348  0.189722  0.065825"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datas = [\n",
    "    bare_scores, \n",
    "    simple_rag_scores, \n",
    "    rag_with_rerank_scores,\n",
    "    simple_rag_term_scores,\n",
    "    simple_rag_term_context_scores,\n",
    "    simple_rag_term_context_with_rerank_scores,\n",
    "    simple_rag_term_explanation_scores,\n",
    "    simple_rag_term_explanation_context_scores,\n",
    "    simple_rag_term_explanation_context_with_rerank_scores,\n",
    "]\n",
    "\n",
    "experiments = pd.concat([pd.DataFrame(data) for data in datas])\n",
    "\n",
    "benchmark = experiments.groupby('method').agg(\n",
    "    precision=pd.NamedAgg(column='precision', aggfunc='mean'),\n",
    "    recall=pd.NamedAgg(column='recall', aggfunc='mean'),\n",
    "    fmeasure=pd.NamedAgg(column='fmeasure', aggfunc='mean'),\n",
    ").sort_values(\"fmeasure\", ascending=False)\n",
    "\n",
    "benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa9aae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments.to_csv(\"./dataset/experiments.csv\", index=False)\n",
    "benchmark.to_csv(\"./dataset/benchmark.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "456ac038",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4a3f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What does DiscoVLA stand for?\n",
      "ANSWER: Discrepancy Reduction in Vision, Language, and Alignment\n",
      "DiscoVLA stands for Discrepancy Reduction in Vision, Language, and Alignment.\n",
      "==========\n",
      "DiscoVLA stands for \"Discrepancy Reduction in Vision, Language, and Alignment\" for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for \"Discrepancy Reduction in Vision, Language, and Alignment\" for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for \"Disc repancy Reducti o n in V ision, L anguage, and A lignment\".\n",
      "==========\n",
      "DiscoVLA stands for \"Disc repancy Reducti o n in V ision, L anguage, and A lignment\".\n",
      "==========\n",
      "I couldn't find any information about \"DiscoVLA.\" However, I found that VLA stands for Very Large Array, which is a radio astronomy observatory in New Mexico, USA.\n",
      "==========\n",
      "Unfortunately, the provided TERM does not explicitly state what DiscoVLA stands for. It seems to be a model or system related to video-text retrieval, but its full form is not mentioned.\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>predict</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_term_explanation_context</td>\n",
       "      <td>DiscoVLA stands for Discrepancy Reduction in V...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_term_context_with_rerank</td>\n",
       "      <td>DiscoVLA stands for \"Discrepancy Reduction in ...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rag_with_rerank</td>\n",
       "      <td>DiscoVLA stands for Discrepancy Reduction in V...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_term_context</td>\n",
       "      <td>DiscoVLA stands for \"Discrepancy Reduction in ...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_term</td>\n",
       "      <td>DiscoVLA stands for Discrepancy Reduction in V...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_term_explanation_context_with_rerank</td>\n",
       "      <td>DiscoVLA stands for \"Disc repancy Reducti o n ...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_score</td>\n",
       "      <td>DiscoVLA stands for \"Disc repancy Reducti o n ...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bare_model</td>\n",
       "      <td>I couldn't find any information about \"DiscoVL...</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple_rag_term_explanation</td>\n",
       "      <td>Unfortunately, the provided TERM does not expl...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            method  \\\n",
       "0              simple_rag_term_explanation_context   \n",
       "0              simple_rag_term_context_with_rerank   \n",
       "0                                  rag_with_rerank   \n",
       "0                          simple_rag_term_context   \n",
       "0                                  simple_rag_term   \n",
       "0  simple_rag_term_explanation_context_with_rerank   \n",
       "0                                 simple_rag_score   \n",
       "0                                       bare_model   \n",
       "0                      simple_rag_term_explanation   \n",
       "\n",
       "                                             predict  precision    recall  \\\n",
       "0  DiscoVLA stands for Discrepancy Reduction in V...   0.700000  1.000000   \n",
       "0  DiscoVLA stands for \"Discrepancy Reduction in ...   0.437500  1.000000   \n",
       "0  DiscoVLA stands for Discrepancy Reduction in V...   0.437500  1.000000   \n",
       "0  DiscoVLA stands for \"Discrepancy Reduction in ...   0.437500  1.000000   \n",
       "0  DiscoVLA stands for Discrepancy Reduction in V...   0.437500  1.000000   \n",
       "0  DiscoVLA stands for \"Disc repancy Reducti o n ...   0.125000  0.285714   \n",
       "0  DiscoVLA stands for \"Disc repancy Reducti o n ...   0.125000  0.285714   \n",
       "0  I couldn't find any information about \"DiscoVL...   0.035714  0.142857   \n",
       "0  Unfortunately, the provided TERM does not expl...   0.000000  0.000000   \n",
       "\n",
       "   fmeasure  \n",
       "0  0.823529  \n",
       "0  0.608696  \n",
       "0  0.608696  \n",
       "0  0.608696  \n",
       "0  0.608696  \n",
       "0  0.173913  \n",
       "0  0.173913  \n",
       "0  0.057143  \n",
       "0  0.000000  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = trainset[idx]['question']\n",
    "ground_truth = trainset[idx]['answer']\n",
    "idx += 1\n",
    "print(\"QUESTION:\", check)\n",
    "print(\"ANSWER:\", ground_truth)\n",
    "check_df = experiments.loc[experiments['question']==check,:].drop([\"question\", \"ground_truth\"], axis=1).sort_values(\"fmeasure\", ascending=False)\n",
    "for p in check_df['predict'].tolist():\n",
    "    print(p)\n",
    "    print(\"=\"*10)\n",
    "check_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac668266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "experiments = pd.read_csv(\"./dataset/experiments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9127c420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['method', 'question', 'ground_truth', 'predict', 'precision', 'recall',\n",
       "       'fmeasure'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bbc1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "acronyms = [obj for idx, obj in enumerate(trainset) if obj['type']=='acronym']\n",
    "# acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76ec7483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What does DiscoVLA stand for?\n",
      "ANSWER: Discrepancy Reduction in Vision, Language, and Alignment\n",
      "DiscoVLA stands for Discrepancy Reduction in Vision, Language, and Alignment.\n",
      "==========\n",
      "DiscoVLA stands for \"Discrepancy Reduction in Vision, Language, and Alignment\" for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for \"Discrepancy Reduction in Vision, Language, and Alignment\" for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval.\n",
      "==========\n",
      "DiscoVLA stands for \"Disc repancy Reducti o n in V ision, L anguage, and A lignment\".\n",
      "==========\n",
      "DiscoVLA stands for \"Disc repancy Reducti o n in V ision, L anguage, and A lignment\".\n",
      "==========\n",
      "I couldn't find any information about \"DiscoVLA.\" However, I found that VLA stands for Very Large Array, which is a radio astronomy observatory in New Mexico, USA.\n",
      "==========\n",
      "Unfortunately, the provided TERM does not explicitly state what DiscoVLA stands for. It seems to be a model or system related to video-text retrieval, but its full form is not mentioned.\n",
      "==========\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>predict</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>simple_rag_term_explanation_context</td>\n",
       "      <td>DiscoVLA stands for Discrepancy Reduction in V...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>simple_rag_term_context_with_rerank</td>\n",
       "      <td>DiscoVLA stands for \"Discrepancy Reduction in ...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>rag_with_rerank</td>\n",
       "      <td>DiscoVLA stands for Discrepancy Reduction in V...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>simple_rag_term_context</td>\n",
       "      <td>DiscoVLA stands for \"Discrepancy Reduction in ...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>simple_rag_term</td>\n",
       "      <td>DiscoVLA stands for Discrepancy Reduction in V...</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>simple_rag_term_explanation_context_with_rerank</td>\n",
       "      <td>DiscoVLA stands for \"Disc repancy Reducti o n ...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>simple_rag_score</td>\n",
       "      <td>DiscoVLA stands for \"Disc repancy Reducti o n ...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bare_model</td>\n",
       "      <td>I couldn't find any information about \"DiscoVL...</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.057143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>simple_rag_term_explanation</td>\n",
       "      <td>Unfortunately, the provided TERM does not expl...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              method  \\\n",
       "231              simple_rag_term_explanation_context   \n",
       "165              simple_rag_term_context_with_rerank   \n",
       "66                                   rag_with_rerank   \n",
       "132                          simple_rag_term_context   \n",
       "99                                   simple_rag_term   \n",
       "264  simple_rag_term_explanation_context_with_rerank   \n",
       "33                                  simple_rag_score   \n",
       "0                                         bare_model   \n",
       "198                      simple_rag_term_explanation   \n",
       "\n",
       "                                               predict  precision    recall  \\\n",
       "231  DiscoVLA stands for Discrepancy Reduction in V...   0.700000  1.000000   \n",
       "165  DiscoVLA stands for \"Discrepancy Reduction in ...   0.437500  1.000000   \n",
       "66   DiscoVLA stands for Discrepancy Reduction in V...   0.437500  1.000000   \n",
       "132  DiscoVLA stands for \"Discrepancy Reduction in ...   0.437500  1.000000   \n",
       "99   DiscoVLA stands for Discrepancy Reduction in V...   0.437500  1.000000   \n",
       "264  DiscoVLA stands for \"Disc repancy Reducti o n ...   0.125000  0.285714   \n",
       "33   DiscoVLA stands for \"Disc repancy Reducti o n ...   0.125000  0.285714   \n",
       "0    I couldn't find any information about \"DiscoVL...   0.035714  0.142857   \n",
       "198  Unfortunately, the provided TERM does not expl...   0.000000  0.000000   \n",
       "\n",
       "     fmeasure  \n",
       "231  0.823529  \n",
       "165  0.608696  \n",
       "66   0.608696  \n",
       "132  0.608696  \n",
       "99   0.608696  \n",
       "264  0.173913  \n",
       "33   0.173913  \n",
       "0    0.057143  \n",
       "198  0.000000  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "check = trainset[idx]['question']\n",
    "ground_truth = trainset[idx]['answer']\n",
    "# idx += 1\n",
    "print(\"QUESTION:\", check)\n",
    "print(\"ANSWER:\", ground_truth)\n",
    "check_df = experiments.loc[experiments['question']==check,:].drop([\"question\", \"ground_truth\"], axis=1).sort_values(\"fmeasure\", ascending=False)\n",
    "for p in check_df['predict'].tolist():\n",
    "    print(p)\n",
    "    print(\"=\"*10)\n",
    "check_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ca9e01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fmeasure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_context</th>\n",
       "      <td>0.538474</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.688497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_context_with_rerank</th>\n",
       "      <td>0.538474</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.688497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rag_with_rerank</th>\n",
       "      <td>0.522312</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.664521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_explanation_context</th>\n",
       "      <td>0.520180</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.663047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_score</th>\n",
       "      <td>0.510065</td>\n",
       "      <td>0.912338</td>\n",
       "      <td>0.648971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_explanation_context_with_rerank</th>\n",
       "      <td>0.467907</td>\n",
       "      <td>0.912338</td>\n",
       "      <td>0.603991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term</th>\n",
       "      <td>0.454561</td>\n",
       "      <td>0.911157</td>\n",
       "      <td>0.598384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_explanation</th>\n",
       "      <td>0.314397</td>\n",
       "      <td>0.721074</td>\n",
       "      <td>0.422351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bare_model</th>\n",
       "      <td>0.005719</td>\n",
       "      <td>0.043979</td>\n",
       "      <td>0.009659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 precision    recall  fmeasure\n",
       "method                                                                        \n",
       "simple_rag_term_context                           0.538474  0.977273  0.688497\n",
       "simple_rag_term_context_with_rerank               0.538474  0.977273  0.688497\n",
       "rag_with_rerank                                   0.522312  0.931818  0.664521\n",
       "simple_rag_term_explanation_context               0.520180  0.977273  0.663047\n",
       "simple_rag_score                                  0.510065  0.912338  0.648971\n",
       "simple_rag_term_explanation_context_with_rerank   0.467907  0.912338  0.603991\n",
       "simple_rag_term                                   0.454561  0.911157  0.598384\n",
       "simple_rag_term_explanation                       0.314397  0.721074  0.422351\n",
       "bare_model                                        0.005719  0.043979  0.009659"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_type = [obj for idx, obj in enumerate(trainset) if obj['type']=='acronym']\n",
    "mask = experiments['question'].isin([qt['question'] for qt in question_type])\n",
    "experiments.loc[mask,:].groupby('method').agg(\n",
    "    precision=pd.NamedAgg(column='precision', aggfunc='mean'),\n",
    "    recall=pd.NamedAgg(column='recall', aggfunc='mean'),\n",
    "    fmeasure=pd.NamedAgg(column='fmeasure', aggfunc='mean'),\n",
    ").sort_values(\"fmeasure\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe3984be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>fmeasure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rag_with_rerank</th>\n",
       "      <td>0.333871</td>\n",
       "      <td>0.776153</td>\n",
       "      <td>0.446621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_context_with_rerank</th>\n",
       "      <td>0.299617</td>\n",
       "      <td>0.748191</td>\n",
       "      <td>0.403068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_context</th>\n",
       "      <td>0.289362</td>\n",
       "      <td>0.773937</td>\n",
       "      <td>0.393448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_score</th>\n",
       "      <td>0.286013</td>\n",
       "      <td>0.747247</td>\n",
       "      <td>0.392362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_explanation_context_with_rerank</th>\n",
       "      <td>0.293737</td>\n",
       "      <td>0.697022</td>\n",
       "      <td>0.386061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_explanation_context</th>\n",
       "      <td>0.263273</td>\n",
       "      <td>0.682694</td>\n",
       "      <td>0.346371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term</th>\n",
       "      <td>0.293705</td>\n",
       "      <td>0.460886</td>\n",
       "      <td>0.326035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple_rag_term_explanation</th>\n",
       "      <td>0.284122</td>\n",
       "      <td>0.385472</td>\n",
       "      <td>0.283368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bare_model</th>\n",
       "      <td>0.060662</td>\n",
       "      <td>0.262593</td>\n",
       "      <td>0.093908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 precision    recall  fmeasure\n",
       "method                                                                        \n",
       "rag_with_rerank                                   0.333871  0.776153  0.446621\n",
       "simple_rag_term_context_with_rerank               0.299617  0.748191  0.403068\n",
       "simple_rag_term_context                           0.289362  0.773937  0.393448\n",
       "simple_rag_score                                  0.286013  0.747247  0.392362\n",
       "simple_rag_term_explanation_context_with_rerank   0.293737  0.697022  0.386061\n",
       "simple_rag_term_explanation_context               0.263273  0.682694  0.346371\n",
       "simple_rag_term                                   0.293705  0.460886  0.326035\n",
       "simple_rag_term_explanation                       0.284122  0.385472  0.283368\n",
       "bare_model                                        0.060662  0.262593  0.093908"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_type = [obj for idx, obj in enumerate(trainset) if obj['type']!='acronym']\n",
    "mask = experiments['question'].isin([qt['question'] for qt in question_type])\n",
    "experiments.loc[mask,:].groupby('method').agg(\n",
    "    precision=pd.NamedAgg(column='precision', aggfunc='mean'),\n",
    "    recall=pd.NamedAgg(column='recall', aggfunc='mean'),\n",
    "    fmeasure=pd.NamedAgg(column='fmeasure', aggfunc='mean'),\n",
    ").sort_values(\"fmeasure\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b109343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: tune prompt all, and update model to be bigger e.g. maverick"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
