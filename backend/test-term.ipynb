{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a270c17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50d07077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.databases.management.longterm import LongTermManagement\n",
    "from package.databases.management.document import DocumentManagement\n",
    "from package.agents.dev_term_extractor import TermExtractor\n",
    "from package.databases.management.term import TermManagement, Term\n",
    "from package.databases.session import Depends, get_session\n",
    "from package.databases.engine import engine\n",
    "\n",
    "ltm = LongTermManagement()\n",
    "dm = DocumentManagement()\n",
    "term_extractor = TermExtractor()\n",
    "tm = TermManagement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11769bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dm.read_documents(session=Depends(get_session)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8588cabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1903"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ltm.read_longterms(session=Depends(get_session)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edf795b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6706"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tm.read_terms(session=Depends(get_session)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8efaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790adf29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c5dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dm.read_documents(session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cbf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [document for document in documents if \"discovla\" in document.source.lower()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fcf8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "longterms = ltm.read_longterms_by_document(document.id, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb5f00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongTerm(enrich='This paper proposes DiscoVLA, a method that mitigates vision, language, and alignment discrepancies in video-text retrieval, outperforming previous methods by 1.5% in R@1 on MSRVTT with CLIP (ViT-B/16).', id='c69a31d6-5c4a-489a-bfec-9a98edda5d3d', combo='This paper proposes DiscoVLA, a method that mitigates vision, language, and alignment discrepancies in video-text retrieval, outperforming previous methods by 1.5% in R@1 on MSRVTT with CLIP (ViT-B/16).\\n\\nAbstract\\n\\nThe parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on imagelevel vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from imagelevel to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Disc repancy Reducti o n in V ision, L anguage, and A lignment ( DiscoVLA ), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5 %in R@1, reaching a final score of 50.5 %R@1. The code is available at https://github.com/LunarShen/DsicoVLA .\\n', meta={'type': 'pdf', 'source': 'sources\\\\DiscoVLA Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval.pdf', 'section': 'Abstract', 'sequence': 1}, created_at=datetime.datetime(2025, 7, 2, 17, 41, 46, 963744), raw='Abstract\\n\\nThe parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on imagelevel vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from imagelevel to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Disc repancy Reducti o n in V ision, L anguage, and A lignment ( DiscoVLA ), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5 %in R@1, reaching a final score of 50.5 %R@1. The code is available at https://github.com/LunarShen/DsicoVLA .\\n', raw_embedding=array([-0.01311493, -0.0239563 , -0.01600647, ..., -0.03189087,\n",
       "        0.00724411,  0.02714539], shape=(1024,), dtype=float32), enrich_embedding=array([-0.01979065, -0.02607727, -0.01783752, ..., -0.04135132,\n",
       "        0.01597595,  0.00763702], shape=(1024,), dtype=float32), combo_embedding=array([-0.0151062 , -0.02575684, -0.02626038, ..., -0.03430176,\n",
       "        0.00204659,  0.02453613], shape=(1024,), dtype=float32), document_id='16514705-ab87-4115-883c-8ebf8ed58f9d', updated_at=datetime.datetime(2025, 7, 2, 17, 42, 3, 813255))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract = [longterm for longterm in longterms if \"abstract\" in longterm.meta.get(\"section\").lower()][0]\n",
    "abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5f15a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c69a31d6-5c4a-489a-bfec-9a98edda5d3d',\n",
       " '16514705-ab87-4115-883c-8ebf8ed58f9d')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract.id, abstract.document_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06f6c467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExtractedTerm(term='CLIP', type='Acronym', evidence='The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research.', explanation='CLIP is an image-text pretraining model that is being adapted for video-text retrieval.'),\n",
       " ExtractedTerm(term='DiscoVLA', type='Acronym', evidence='In this paper, we propose Disc repancy Reducti o n in V ision, L anguage, and A lignment ( DiscoVLA ), which simultaneously mitigates all three discrepancies.', explanation='DiscoVLA stands for Discrepancy Reduction in Vision, Language, and Alignment, a method proposed to mitigate vision, language, and alignment discrepancies.'),\n",
       " ExtractedTerm(term='MSRVTT', type='Proper Name', evidence='In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5 %in R@1, reaching a final score of 50.5 %R@1.', explanation='MSRVTT is a dataset or benchmark used to evaluate the performance of DiscoVLA.'),\n",
       " ExtractedTerm(term='ViT-B/16', type='Proper Name', evidence='In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5 %in R@1, reaching a final score of 50.5 %R@1.', explanation='ViT-B/16 is a specific configuration or variant of the CLIP model used in the experiments.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = abstract.raw\n",
    "terms = term_extractor.run(context)\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb848075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongTerm(enrich='The information is not proper for summarization.', raw='Open Information Extraction\\n\\nInstruction:\\n\\nYour task is to construct  an RDF (Resource  Description Framework) graph from the given passages and named entity lists.\\n\\nRespond with a JSON list of triples, with each triple representing a relationship in the RDF graph. Pay attention to the following requirements:\\n\\n- - Each triple should contain at least one, but preferably two, of the named entities in the list for each passage.\\n- - Clearly resolve pronouns  to their specific names to maintain clarity.\\n\\nConvert the paragraph into a JSON dict, it has a named entity list and a triple list.\\n', raw_embedding=array([-0.02757263, -0.01230621, -0.01899719, ..., -0.02218628,\n",
       "       -0.02259827,  0.02259827], shape=(1024,), dtype=float32), combo='The information is not proper for summarization.\\n\\nOpen Information Extraction\\n\\nInstruction:\\n\\nYour task is to construct  an RDF (Resource  Description Framework) graph from the given passages and named entity lists.\\n\\nRespond with a JSON list of triples, with each triple representing a relationship in the RDF graph. Pay attention to the following requirements:\\n\\n- - Each triple should contain at least one, but preferably two, of the named entities in the list for each passage.\\n- - Clearly resolve pronouns  to their specific names to maintain clarity.\\n\\nConvert the paragraph into a JSON dict, it has a named entity list and a triple list.\\n', meta={'type': 'pdf', 'source': 'sources\\\\HippoRAG Neurobiologically Inspired Long-Term Memory for Large Language Models.pdf', 'section': 'Open Information Extraction', 'sequence': 143}, created_at=datetime.datetime(2025, 7, 2, 17, 44, 21, 494139), id='ea6f8bb9-5470-4be1-ad2a-8bef17df8e73', enrich_embedding=array([-0.01542664,  0.01927185, -0.07489014, ...,  0.00551605,\n",
       "       -0.0196228 ,  0.01125336], shape=(1024,), dtype=float32), combo_embedding=array([-0.01899719, -0.00465012, -0.01669312, ..., -0.0165863 ,\n",
       "       -0.02716064,  0.01570129], shape=(1024,), dtype=float32), document_id='d62d0f7b-ea37-42f7-8402-cf68a4541717', updated_at=datetime.datetime(2025, 7, 2, 17, 44, 55, 74219))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longterms = ltm.read_longterms(session=Depends(get_session))\n",
    "longterms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a4aa69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1903"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(longterms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b82042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlmodel import Session, delete\n",
    "\n",
    "def delete_all_records(model_class):\n",
    "    \"\"\"\n",
    "    Delete all records from the specified model table\n",
    "    Args:\n",
    "        model_class: SQLModel class to delete records from\n",
    "    \"\"\"\n",
    "    with Session(engine) as session:\n",
    "        # Delete all records from the specified table\n",
    "        statement = delete(model_class)\n",
    "        # Execute the delete statement to remove all records from the database\n",
    "        session.exec(statement)\n",
    "        # Commit the transaction to persist the changes\n",
    "        session.commit()\n",
    "delete_all_records(Term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test() -> int:\n",
    "    \"\"\"\n",
    "    A simple function that returns the sum of 1+1.\n",
    "\n",
    "    Returns:\n",
    "        int: The sum of 1+1 (2)\n",
    "    \"\"\"\n",
    "    return 1+1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
