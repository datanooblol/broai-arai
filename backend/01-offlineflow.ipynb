{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ca6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b1cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.databases.initialize import initialize_memories\n",
    "initialize_memories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9280462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\broai-arai\\backend\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from package.utils.data_loder import PDFLoader\n",
    "from package.interface import SourceOptions\n",
    "from package.flows.offline import OfflineFlow\n",
    "from package.databases.management.longterm import LongTermManagement\n",
    "from package.databases.management.user import UserManagement\n",
    "from package.databases.management.document import DocumentManagement\n",
    "from package.databases.management.project import ProjectManagement\n",
    "from package.databases.management.jargon import JargonManagement\n",
    "from package.databases.session import Session, get_session, Depends\n",
    "from package.databases.models.user import User\n",
    "from package.databases.models.document import Document\n",
    "from package.databases.models.project import Project\n",
    "from package.databases.models.jargon import Jargon\n",
    "from package.databases.models.longterm import LongTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "896fa865",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = User(\n",
    "    username=\"bank\",\n",
    "    password=\"555\",\n",
    "    email=\"bank@bank.com\"\n",
    ")\n",
    "\n",
    "um = UserManagement()\n",
    "user = um.create_user(user, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb4fb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ef460369-7219-4325-8239-0f89310fe6ec'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d193b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = Document(source=\"./sources/storm.pdf\", type=\"pdf\")\n",
    "\n",
    "dm = DocumentManagement()\n",
    "document1 = dm.create_document(document1, session=Depends(get_session)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f7cfd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fe8d0d2b-5e8f-49ae-922c-026eabedd8d3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document1.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec989c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.read_document_longterms(document_id=document1.id, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cee90623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown headings: max(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\broai-arai\\backend\\package\\utils\\data_loder.py:22: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: split_markdown\n",
      "  chunks = split_markdown(text)\n",
      "d:\\broai-arai\\backend\\package\\utils\\data_loder.py:23: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: consolidate_markdown\n",
      "  consolidated_chunks = consolidate_markdown(chunks)\n",
      "d:\\broai-arai\\backend\\package\\utils\\data_loder.py:24: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: get_markdown_sections\n",
      "  sections = get_markdown_sections(consolidated_chunks)\n",
      "d:\\broai-arai\\backend\\package\\utils\\data_loder.py:30: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: split_overlap\n",
      "  new_contexts = split_overlap(contexts, max_tokens=max_tokens, overlap=overlap)\n"
     ]
    }
   ],
   "source": [
    "source_ops = SourceOptions(path=\"./sources/storm.pdf\", type=\"pdf\")\n",
    "loader = PDFLoader(\n",
    "    source=source_ops\n",
    ")\n",
    "contexts = loader.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8683b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "olf = OfflineFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbeefae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "longterms = olf.run(document_id=document1.id, contexts=contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a211ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm = LongTermManagement()\n",
    "ltm.create_raws(longterms, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e44c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading model from: BAAI/bge-m3\n"
     ]
    }
   ],
   "source": [
    "from package.embedding.baai import BAAIEmbedding\n",
    "embedding = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3608a159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fe8d0d2b-5e8f-49ae-922c-026eabedd8d3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document1.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae7e36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "longterms = ltm.read_longterms_by_document(document_id=document1.id, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5effe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "sentences = [longterm.raw for longterm in longterms]\n",
    "vectors = embedding.run(sentences=sentences)\n",
    "for longterm, vector in zip(longterms, vectors):\n",
    "    longterm.raw_embedding = vector\n",
    "ltm.update_longterms(longterms=longterms, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57733bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"What does ROUGE Score do?\"\"\"\n",
    "query = \"\"\"How was FreshWiki created?\"\"\"\n",
    "vector = embedding.run(sentences=[query])[0]\n",
    "results = ltm.read_similar_text(vector, limit=15, embed_method=\"raw\", sources=[document1.source], session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0ddf6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2 FreshWiki', 'sequence': 5}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2.1 The FreshWiki Dataset', 'sequence': 7}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'A Dataset Details', 'sequence': 40}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '8 Conclusion', 'sequence': 29}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Abstract', 'sequence': 2}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Limitations', 'sequence': 30}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4 Experiments', 'sequence': 14}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '## Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models', 'sequence': 0}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Ethics Statement', 'sequence': 32}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'D Human Evaluation Details', 'sequence': 59}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '1 Introduction', 'sequence': 3}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '3.2 Simulating Conversations', 'sequence': 11}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2 FreshWiki', 'sequence': 6}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'E Error Analysis', 'sequence': 60}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '6 Human Evaluation', 'sequence': 27}\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0c2480f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1ad51d70-b53c-4ffc-b7b5-d523ead048d6'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55e1fc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongTerm(raw=\"2 FreshWiki\\n\\nWe study generating Wikipedia-like articles from scratch, placing emphasis on the pre-writing stage (Rohman, 1965), which involves the demanding sub-tasks of gathering and curating relevant information ('research'). This models the human\\n\\n1 Our resources and code are released at https://github. com/stanford-oval/storm .\\n\\nTable 1: Comparison of different Wikipedia generation setups in existing literature. Generating one paragraph does not need an article outline.\\n\\n|                            | Domain   | Scope        | Given Outline?   | Given Refs?   |\\n|----------------------------|----------|--------------|------------------|---------------|\\n| Balepur et al. (2023)      | One      | One para.    | /                | Yes           |\\n| Qian et al. (2023)         | All      | One para.    | /                | No            |\\n| Fan and Gardent (2022)     | One      | Full article | Yes              | No            |\\n| Liu et al. (2018)          | All      | One para.    | /                | Yes           |\\n| Sauper and Barzilay (2009) | Two      | Full article | No               | No            |\\n| Ours                       | All      | Full article | No               | No            |\\n\\nwriting approach which has prompted some educators to view Wikipedia article writing as an educational exercise for academic training (Tardy, 2010).\\n\\nTable 1 compares our work against prior benchmarks for Wikipedia generation. Existing work has generally focused on evaluating the generation of shorter snippets ( e.g. , one paragraph), within a narrower scope ( e.g. , a specific domain or two), or when an explicit outline or reference documents are supplied. A\", raw_embedding=array([-0.00113964,  0.00158501, -0.04284668, ...,  0.03170776,\n",
       "        0.0015192 ,  0.01742554], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='ffe89150-71e7-4fcb-b968-66bad360c85c', updated_at=datetime.datetime(2025, 6, 29, 10, 21, 23, 495197), enrich=None, id='1ad51d70-b53c-4ffc-b7b5-d523ead048d6', combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2 FreshWiki', 'sequence': 5}, created_at=datetime.datetime(2025, 6, 29, 10, 21, 18, 321057))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltm.read_longterm(longterm_id=results[0].id, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1c35ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All tables dropped.\n"
     ]
    }
   ],
   "source": [
    "from package.databases.destroy import drop_all_tables\n",
    "\n",
    "drop_all_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa901e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
