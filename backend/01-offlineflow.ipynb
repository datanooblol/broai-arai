{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ca6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b1cd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Trigger already exists, skipping.\n"
     ]
    }
   ],
   "source": [
    "from package.databases.initialize import initialize_memories\n",
    "initialize_memories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d2a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlmodel import Session\n",
    "# from sqlalchemy import text\n",
    "# from your_app.db import engine  # adjust as needed\n",
    "\n",
    "# with Session(engine) as session:\n",
    "#     # Enable extensions\n",
    "#     session.exec(text(\"CREATE EXTENSION IF NOT EXISTS pg_trgm;\"))\n",
    "#     session.exec(text(\"CREATE EXTENSION IF NOT EXISTS unaccent;\"))\n",
    "\n",
    "#     # Optional: check for existence of the built-in trigger function\n",
    "#     result = session.exec(\n",
    "#         text(\"SELECT proname FROM pg_proc WHERE proname = 'tsvector_update_trigger';\")\n",
    "#     ).all()\n",
    "\n",
    "#     if not result:\n",
    "#         print(\"âš ï¸ tsvector_update_trigger not found!\")\n",
    "#     else:\n",
    "#         print(\"âœ… tsvector_update_trigger is available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea0daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from package.databases.models.jargon import Jargon\n",
    "# from sqlmodel import Session\n",
    "# from package.databases.engine import engine\n",
    "\n",
    "# with Session(engine) as session:\n",
    "#     j = Jargon(\n",
    "#         jargon=\"neural architecture\",\n",
    "#         evidence=\"Deep learning uses this\",\n",
    "#         explanation=\"An arrangement of artificial neurons\",\n",
    "#         document_id='ab2900cc-0b68-4eda-837e-ad818069e403'\n",
    "#     )\n",
    "#     session.add(j)\n",
    "#     session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2940fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import select, func\n",
    "# from sqlmodel import Session\n",
    "\n",
    "# def search_jargon(term: str):\n",
    "#     ts_query = func.to_tsquery(\"english\", term)\n",
    "#     with Session(engine) as session:\n",
    "#         stmt = select(Jargon).where(Jargon.search_vector.op('@@')(ts_query))\n",
    "#         results = session.exec(stmt).all()\n",
    "#         return results\n",
    "\n",
    "\n",
    "# results = search_jargon(\"neural & architecture\")\n",
    "# results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9280462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\broai-arai\\backend\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from package.utils.data_loder import PDFLoader\n",
    "from package.interface import SourceOptions\n",
    "from package.flows.offline import OfflineFlow\n",
    "from package.databases.management.longterm import LongTermManagement\n",
    "from package.databases.management.user import UserManagement\n",
    "from package.databases.management.document import DocumentManagement\n",
    "from package.databases.management.project import ProjectManagement\n",
    "from package.databases.management.jargon import JargonManagement\n",
    "from package.databases.session import Session, get_session, Depends\n",
    "from package.databases.models.user import User\n",
    "from package.databases.models.document import Document\n",
    "from package.databases.models.project import Project\n",
    "from package.databases.models.jargon import Jargon\n",
    "from package.databases.models.longterm import LongTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "896fa865",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = User(\n",
    "    username=\"bank\",\n",
    "    password=\"555\",\n",
    "    email=\"bank@bank.com\"\n",
    ")\n",
    "\n",
    "um = UserManagement()\n",
    "user = um.create_user(user, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deb4fb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9157bd9f-7eda-4fd9-bf0b-c53526984560'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d193b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = Document(source=\"./sources/storm.pdf\", type=\"pdf\")\n",
    "\n",
    "dm = DocumentManagement()\n",
    "document1 = dm.create_document(document1, session=Depends(get_session)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f7cfd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'97bad39a-538e-4135-b87d-88c7252b1962'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document1.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec989c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.read_document_longterms(document_id=document1.id, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee90623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown headings: max(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\broai-arai\\backend\\package\\utils\\data_loder.py:22: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: split_markdown\n",
      "  chunks = split_markdown(text)\n",
      "d:\\broai-arai\\backend\\package\\utils\\data_loder.py:23: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: consolidate_markdown\n",
      "  consolidated_chunks = consolidate_markdown(chunks)\n",
      "d:\\broai-arai\\backend\\package\\utils\\data_loder.py:24: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: get_markdown_sections\n",
      "  sections = get_markdown_sections(consolidated_chunks)\n",
      "d:\\broai-arai\\backend\\package\\utils\\data_loder.py:30: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: split_overlap\n",
      "  new_contexts = split_overlap(contexts, max_tokens=max_tokens, overlap=overlap)\n"
     ]
    }
   ],
   "source": [
    "source_ops = SourceOptions(path=\"./sources/storm.pdf\", type=\"pdf\")\n",
    "loader = PDFLoader(\n",
    "    source=source_ops\n",
    ")\n",
    "contexts = loader.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8683b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "olf = OfflineFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbeefae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "longterms = olf.run(document_id=document1.id, contexts=contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a211ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm = LongTermManagement()\n",
    "ltm.create_raws(longterms, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e44c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from package.embedding.baai import BAAIEmbedding\n",
    "# embedding = BAAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3608a159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'97bad39a-538e-4135-b87d-88c7252b1962'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document1.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae7e36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "longterms = ltm.read_longterms_by_document(document_id=document1.id, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5effe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [longterm.raw for longterm in longterms]\n",
    "# vectors = embedding.run(sentences=sentences)\n",
    "# for longterm, vector in zip(longterms, vectors):\n",
    "#     longterm.raw_embedding = vector\n",
    "# ltm.update_longterms(longterms=longterms, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57733bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # query = \"\"\"What does ROUGE Score do?\"\"\"\n",
    "# query = \"\"\"How was FreshWiki created?\"\"\"\n",
    "# vector = embedding.run(sentences=[query])[0]\n",
    "# results = ltm.read_similar_text(vector, limit=15, embed_method=\"raw\", sources=[document1.source], session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0ddf6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for result in results:\n",
    "#     print(result.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22271162",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks = [{\"document_id\": document1.id, \"longterm_id\": longterm.id} for longterm in longterms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d8143af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15ef6fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': '97bad39a-538e-4135-b87d-88c7252b1962',\n",
       " 'longterm_id': '654998b2-2e99-4c8b-b1e0-79acbf38801c'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74c198dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution started!\n",
      "Execution ARN: arn:aws:states:ap-southeast-1:112557628841:execution:broai-arai-enrich-fleet:test-execution-2214e3d3-0d0f-4053-84f2-8fed9448d23f\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# Replace this with your actual Step Function ARN\n",
    "STATE_MACHINE_ARN = \"arn:aws:states:ap-southeast-1:112557628841:stateMachine:broai-arai-enrich-fleet\"\n",
    "\n",
    "# Create a unique name for this execution\n",
    "execution_name = f\"test-execution-{uuid.uuid4()}\"\n",
    "\n",
    "# Input payload\n",
    "input_payload = {\n",
    "    \"document_chunks\": document_chunks[:]\n",
    "}\n",
    "\n",
    "# Create Step Functions client\n",
    "sfn = boto3.client(\"stepfunctions\", region_name=\"ap-southeast-1\")\n",
    "\n",
    "# Start execution\n",
    "response = sfn.start_execution(\n",
    "    stateMachineArn=STATE_MACHINE_ARN,\n",
    "    name=execution_name,\n",
    "    input=json.dumps(input_payload)\n",
    ")\n",
    "\n",
    "print(\"Execution started!\")\n",
    "print(\"Execution ARN:\", response[\"executionArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2b866a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution status: RUNNING\n",
      "Execution status: RUNNING\n",
      "Execution status: RUNNING\n",
      "Execution status: RUNNING\n",
      "Execution status: RUNNING\n",
      "Execution status: RUNNING\n",
      "Execution status: SUCCEEDED\n",
      "Execution output: {\"status\":\"success\",\"results\":[{\"Payload\":{\"processed\":\"654998b2-2e99-4c8b-b1e0-79acbf38801c\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"5ff3a9b3-6f97-47ef-983d-677a7f13cdcb\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"d3bd387b-6a12-4837-903c-8bbc76c9c2fa\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"c6be679e-fff3-4842-b969-472c2daa2e5f\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"3dfa1242-2bc6-4f00-a2b1-719f82e671fa\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"21c08f0a-1758-4887-a8f7-7dfd6bed606d\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"52b791b0-3792-4512-ad00-d6a0ccce589e\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"7b10e213-b247-411a-b8dd-a8fa5dd846ba\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"f98f0364-af28-48c0-b09a-b8a3f0d2209d\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"bb5f5496-6e66-4c3e-ac02-b62ba8b00138\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"ad0d3c87-3bfa-48f9-95e2-055f85863595\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"823694cf-6638-43a4-95fb-d6bba2b972af\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"779b7f84-b85b-4c01-9fd6-fd821e6af5b1\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"8a6a94eb-d41e-4ffa-89b6-5fab97c2eaec\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"58558cd0-f7b7-4bc4-8334-ddca7f985208\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"6aa4653d-6564-47f3-aeac-8c71d6a73f20\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"16599d18-518e-435a-b208-4243ff9db56f\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"e7bb5663-76df-48b6-91cf-593aaefddbf2\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"0d67354d-93b9-4509-9a9c-647e0adc2b05\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"bbe4e4b7-6113-4bd2-b90b-85609d3f111b\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"a57a1039-ed95-4e71-8355-fa036ac8a27e\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"e8ac79c4-11b5-40f5-af36-84ad38b9e226\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"147c5cdb-5990-4977-ba8d-a4b785031884\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"d6806a7f-7990-4a95-9e83-9fc587376fcd\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"91b06b9e-1afe-48a2-ae16-26a1c333f5c0\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"5a9fc780-c814-4c86-a796-60ca5bafa94e\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"f54b745d-9d82-44d6-97f8-c08a30889dee\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"cd756145-13f2-4079-96d1-7b95e77466e9\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"ad5e21c2-a087-4a1a-911e-17808803c6de\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"6518b8f1-aed0-4452-8389-fdfedf5db5b2\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"77c80617-770e-4985-aadd-12f40bfcfb95\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"bb270b6e-746d-41bd-8f21-ba8348f89baf\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"47253414-91c3-48b0-b69c-35769e14236c\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"2864a836-00cd-451c-8376-987603538790\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"255c902d-3bff-4aef-963d-79023cf26e80\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"2074f714-c9d2-4b36-81ac-31eda930191d\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"e921e4cf-b637-483e-a63c-e45d9faec9a6\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"56f20256-2917-477b-8bc1-a69f5af04602\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"dadecce0-86c5-4a91-81f0-0c2578ce1583\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"b5ee06b1-7f4d-49d3-88fe-6d286b9f6cd2\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"cf91422c-b3e7-4d73-a297-388afed9e22d\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"f3eae798-7aaf-40cc-bb32-0614720d0fad\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"a4333e3b-c43d-4de9-8ba4-1312146a21fa\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"0c8e945c-c086-472a-81e0-8aac581f244f\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"50ea9409-07e4-4b74-9a79-e59403bb04e7\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"53fd0ff9-894e-47e1-aca2-cb954f2448bc\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"7cbebe22-cdcf-4cb6-bbcc-313e26176e68\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"c1696268-1606-456a-a52f-696b404bba34\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"ae3dcff4-642b-440e-ab35-6c4aaf71e998\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"b836c75b-2683-4cb0-96ed-eb2c51c616e6\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"75bc2160-0109-46df-a51d-73746556e8bc\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"3258ba76-730e-4556-939c-8351d80e0429\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"5478e24d-076c-407c-a146-ca0593fb1622\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"05752d36-f376-4f54-9c73-80c60f942df9\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"3e89bce0-f0c3-4f69-b190-99fbe760f323\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"3297d9ec-ace2-4f40-90c8-b1eb4b07e352\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"bedce49a-5de9-4de9-b928-35937bfea406\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"97eeee6c-f270-4b0b-9051-30e3e6c95ea4\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"dd41e5f4-34f5-430e-88fd-2ae402660682\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"52021155-a694-4b32-bad4-8e7f9e1b6637\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"c5b55bf8-a9ed-470f-97f1-8b03f1b1a1b1\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"4e429d98-3be5-42cf-9465-8fd02955dfe9\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"6115f80f-6b76-46d6-ba80-09cc0ea356dc\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"86b41cfe-7589-412a-ba2c-d50fefff49e4\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"6e404b1a-4ce9-4485-88e0-e8000e040550\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"f635ddb2-1238-4195-8785-500d135b55c4\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"97b60a28-c44b-4a4e-a06c-e5c9d8d37c2c\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"62aa0804-ef93-48ef-936f-b69a5f4109df\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"0a9f2c9b-0778-482e-bc42-6b9f02f6e995\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"42e2bcbd-d20f-484d-a868-e9b0f2cf3be1\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"656fb399-2c86-4687-8a22-0e1cdf5c78e8\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"457220ab-d6bc-427c-be40-741caa6db061\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"7cedd4a1-270e-4267-b79a-741539fea367\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"2eadbd59-9c69-49d0-8d16-9c833d014ea4\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"b8b24279-330d-4307-a4b1-1134fb683c20\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"cdf97445-8850-4340-bf2d-84425bcf3400\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"5f36f816-1841-424f-8234-984281a12f1c\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"6fc02231-4e6e-4eda-a782-74862eee46cc\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"62981f6b-afba-49ec-9498-cbc04526f5f5\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"b7c3beca-25b3-4013-abb0-feadd70dbbad\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"f513c727-a3b2-4340-9a24-1f5ff6e75707\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"d07ddc6c-d636-4003-884f-e0eab0ea862f\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"b29497f3-d8e0-41cd-8e67-55fb9155c4fe\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"a06102c1-7ebb-4504-92e0-010573d0f91e\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"d0888635-0031-417b-965a-6c432651c078\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"a6570c84-5a16-4393-a41d-fe07b66c5865\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}},{\"Payload\":{\"processed\":\"ce20d75b-44e0-4ae6-8d74-05048b20ddb3\",\"document_id\":\"97bad39a-538e-4135-b87d-88c7252b1962\"}}]}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "execution_arn = response[\"executionArn\"]\n",
    "\n",
    "while True:\n",
    "    desc = sfn.describe_execution(executionArn=execution_arn)\n",
    "    status = desc[\"status\"]\n",
    "    print(f\"Execution status: {status}\")\n",
    "    if status in (\"SUCCEEDED\", \"FAILED\", \"TIMED_OUT\", \"ABORTED\"):\n",
    "        break\n",
    "    time.sleep(2)\n",
    "\n",
    "if status == \"SUCCEEDED\":\n",
    "    output = desc.get(\"output\")\n",
    "    print(\"Execution output:\", output)\n",
    "else:\n",
    "    print(f\"Execution ended with status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7036bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "jm = JargonManagement()\n",
    "dm = DocumentManagement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5962258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jm.read_jargons(session=Depends(get_session)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edf53547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_longterms = ltm.read_longterms_by_document(document_id=document1.id, session=Depends(get_session))\n",
    "len(_longterms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "461a6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.databases.utils import now_utc\n",
    "def embed(longterms, embedder, session: Session = Depends(get_session)):\n",
    "    updated_at = now_utc()\n",
    "    raws = [longterm.raw for longterm in longterms]\n",
    "    raw_vectors = embedder.run(sentences=raws)\n",
    "    enrichs = [longterm.enrich for longterm in longterms]\n",
    "    enrich_vectors = embedder.run(sentences=enrichs)\n",
    "    combos = [longterm.combo for longterm in longterms]\n",
    "    combo_vectors = embedder.run(sentences=combos)\n",
    "    for longterm, vector in zip(longterms, raw_vectors):\n",
    "        longterm.raw_embedding = vector\n",
    "    for longterm, vector in zip(longterms, enrich_vectors):\n",
    "        longterm.enrich_embedding = vector\n",
    "    for longterm, vector in zip(longterms, combo_vectors):\n",
    "        longterm.combo_embedding = vector\n",
    "        longterm.updated_at = updated_at\n",
    "    ltm.update_longterms(longterms=longterms, session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "178eadf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Loading model from: BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from package.embedding.baai import BAAIEmbedding\n",
    "embedder = BAAIEmbedding()\n",
    "embed(_longterms, embedder=embedder, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "020dc9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongTerm(raw=\"2 FreshWiki\\n\\nWe study generating Wikipedia-like articles from scratch, placing emphasis on the pre-writing stage (Rohman, 1965), which involves the demanding sub-tasks of gathering and curating relevant information ('research'). This models the human\\n\\n1 Our resources and code are released at https://github. com/stanford-oval/storm .\\n\\nTable 1: Comparison of different Wikipedia generation setups in existing literature. Generating one paragraph does not need an article outline.\\n\\n|                            | Domain   | Scope        | Given Outline?   | Given Refs?   |\\n|----------------------------|----------|--------------|------------------|---------------|\\n| Balepur et al. (2023)      | One      | One para.    | /                | Yes           |\\n| Qian et al. (2023)         | All      | One para.    | /                | No            |\\n| Fan and Gardent (2022)     | One      | Full article | Yes              | No            |\\n| Liu et al. (2018)          | All      | One para.    | /                | Yes           |\\n| Sauper and Barzilay (2009) | Two      | Full article | No               | No            |\\n| Ours                       | All      | Full article | No               | No            |\\n\\nwriting approach which has prompted some educators to view Wikipedia article writing as an educational exercise for academic training (Tardy, 2010).\\n\\nTable 1 compares our work against prior benchmarks for Wikipedia generation. Existing work has generally focused on evaluating the generation of shorter snippets ( e.g. , one paragraph), within a narrower scope ( e.g. , a specific domain or two), or when an explicit outline or reference documents are supplied. A\", raw_embedding=array([-0.00113964,  0.00158501, -0.04284668, ...,  0.03170776,\n",
       "        0.0015192 ,  0.01742554], shape=(1024,), dtype=float32), enrich_embedding=array([-0.03579712,  0.01971436, -0.04818726, ...,  0.01107025,\n",
       "       -0.00141239,  0.01531982], shape=(1024,), dtype=float32), combo_embedding=array([-0.02534485,  0.00213432, -0.03997803, ...,  0.02580261,\n",
       "       -0.00312805,  0.01620483], shape=(1024,), dtype=float32), document_id='97bad39a-538e-4135-b87d-88c7252b1962', updated_at=datetime.datetime(2025, 6, 30, 15, 38, 42, 736477), id='21c08f0a-1758-4887-a8f7-7dfd6bed606d', enrich='The paper studies generating Wikipedia-like articles from scratch, focusing on the pre-writing stage, and compares its work against prior benchmarks for Wikipedia generation.', combo=\"The paper studies generating Wikipedia-like articles from scratch, focusing on the pre-writing stage, and compares its work against prior benchmarks for Wikipedia generation.\\n\\n2 FreshWiki\\n\\nWe study generating Wikipedia-like articles from scratch, placing emphasis on the pre-writing stage (Rohman, 1965), which involves the demanding sub-tasks of gathering and curating relevant information ('research'). This models the human\\n\\n1 Our resources and code are released at https://github. com/stanford-oval/storm .\\n\\nTable 1: Comparison of different Wikipedia generation setups in existing literature. Generating one paragraph does not need an article outline.\\n\\n|                            | Domain   | Scope        | Given Outline?   | Given Refs?   |\\n|----------------------------|----------|--------------|------------------|---------------|\\n| Balepur et al. (2023)      | One      | One para.    | /                | Yes           |\\n| Qian et al. (2023)         | All      | One para.    | /                | No            |\\n| Fan and Gardent (2022)     | One      | Full article | Yes              | No            |\\n| Liu et al. (2018)          | All      | One para.    | /                | Yes           |\\n| Sauper and Barzilay (2009) | Two      | Full article | No               | No            |\\n| Ours                       | All      | Full article | No               | No            |\\n\\nwriting approach which has prompted some educators to view Wikipedia article writing as an educational exercise for academic training (Tardy, 2010).\\n\\nTable 1 compares our work against prior benchmarks for Wikipedia generation. Existing work has generally focused on evaluating the generation of shorter snippets ( e.g. , one paragraph), within a narrower scope ( e.g. , a specific domain or two), or when an explicit outline or reference documents are supplied. A\", meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2 FreshWiki', 'sequence': 5}, created_at=datetime.datetime(2025, 6, 30, 15, 38, 21, 433717))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltm.read_longterms_by_document(document_id=document1.id, session=Depends(get_session))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fda3bcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2 FreshWiki', 'sequence': 5}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2.1 The FreshWiki Dataset', 'sequence': 7}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'A Dataset Details', 'sequence': 40}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '8 Conclusion', 'sequence': 29}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Abstract', 'sequence': 2}\n"
     ]
    }
   ],
   "source": [
    "vector = embedder.run(sentences=[\"How was FreshWiki created?\"])[0]\n",
    "response = ltm.read_similar_text(vector=vector, embed_method=\"raw\", session=Depends(get_session))\n",
    "for r in response:\n",
    "    print(r.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "427ea7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2.1 The FreshWiki Dataset', 'sequence': 7}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'A Dataset Details', 'sequence': 40}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4 Experiments', 'sequence': 14}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '## Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models', 'sequence': 0}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2 FreshWiki', 'sequence': 5}\n"
     ]
    }
   ],
   "source": [
    "# vector = embedder.run(sentences=[\"How was FreshWiki created?\"])[0]\n",
    "response = ltm.read_similar_text(vector=vector, embed_method=\"enrich\", session=Depends(get_session))\n",
    "for r in response:\n",
    "    print(r.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea85c8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2.1 The FreshWiki Dataset', 'sequence': 7}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'A Dataset Details', 'sequence': 40}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2 FreshWiki', 'sequence': 5}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '8 Conclusion', 'sequence': 29}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4 Experiments', 'sequence': 14}\n"
     ]
    }
   ],
   "source": [
    "# vector = embedder.run(sentences=[\"How was FreshWiki created?\"])[0]\n",
    "response = ltm.read_similar_text(vector=vector, embed_method=\"combo\", session=Depends(get_session))\n",
    "for r in response:\n",
    "    print(r.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4015898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from package.databases.engine import engine\n",
    "from sqlalchemy import select, func, or_\n",
    "from sqlmodel import Session\n",
    "\n",
    "def search_jargon(term: str):\n",
    "    ts_query = func.to_tsquery(\"english\", term)\n",
    "    with Session(engine) as session:\n",
    "        stmt = select(Jargon).where(\n",
    "            or_(\n",
    "                Jargon.search_vector.op('@@')(ts_query),\n",
    "                func.similarity(Jargon.jargon, term) > 0.3\n",
    "                \n",
    "            )\n",
    "            )\n",
    "        results = session.exec(stmt).all()\n",
    "        return results\n",
    "\n",
    "\n",
    "results = search_jargon(\"STARM\")\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1ac52e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We propose STORM , a writing system for the S ynthesis of T opic O utlines through R etrieval and M ulti-perspective Question Asking'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[5][0].evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6500ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [result[0].evidence for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2bc8caae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORM simulates a conversation between a Wikipedia writer and a topic expert\n",
      "STORM is capable of researching complicated topics and writing long articles from detailed outlines\n",
      "STORM creates an outline before the actual writing starts\n",
      "Given the input topic t , STORM discovers different perspectives by surveying existing articles from similar topics\n",
      "We present STORM to automate the pre-writing stage\n",
      "We propose STORM , a writing system for the S ynthesis of T opic O utlines through R etrieval and M ulti-perspective Question Asking\n",
      "Algorithm 1: STORM\n",
      "we introduce STORM, a framework that automates the pre-writing stage\n",
      "articles produced by STORM\n",
      "STORM prompts LLMs to ask effective questions by discovering specific perspectives and simulating multi-turn conversations\n",
      "we propose the STORM paradigm for the S ynthesis of T opic O utlines through R etrieval and M ulti-perspective Question Asking\n",
      "STORM             | 45.82\n",
      "4.4 STORM Implementation\n",
      "STORM\n",
      "articles produced by STORM\n",
      "We propose STORM, an LLM-based writing system\n",
      "Although STORM discovers different perspectives in researching the given topic\n",
      "|         | STORM            | 92.73 â€                | 45.91                   |\n",
      "Our approach still outperforms it, STORM still has much room for improvement\n",
      "To better understand the strengths and weaknesses of STORM, we conduct human evaluation\n",
      "STORM also outperforms the best baseline in pairwise comparison\n",
      "STORM's generated article for 'Taylor Hawkins'\n",
      "We use the chat model gpt-3.5-turbo for question asking and use gpt-3.5-turbo-instruct for other parts of STORM\n",
      "we propose the STORM paradigm for the S ynthesis\n",
      "After the evaluation finishes, they are further requested to compare an article produced by our method, which they have just reviewed, with its human-written counterpart, and report their perceived usefulness of STORM using a 1-5 Likert scale\n",
      "STORM and the best baseline, i.e., oRAG\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f1c35ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All tables dropped.\n"
     ]
    }
   ],
   "source": [
    "from package.databases.destroy import drop_all_tables\n",
    "\n",
    "drop_all_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f5651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
