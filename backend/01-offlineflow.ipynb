{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61ca6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b1cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.databases.initialize import initialize_memories\n",
    "initialize_memories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9280462e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\broai-arai\\backend\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\broai-arai\\backend\\package\\databases\\management\\longterm.py:9: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: BAAIEmbedding\n",
      "  em = BAAIEmbedding()\n",
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 29987.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from package.utils.data_loder import PDFLoader\n",
    "from package.interface import SourceOptions\n",
    "from package.flows.offline import OfflineFlow\n",
    "from package.databases.management.longterm import LongTermManagement\n",
    "from package.databases.management.user import UserManagement\n",
    "from package.databases.management.document import DocumentManagement\n",
    "from package.databases.management.project import ProjectManagement\n",
    "from package.databases.management.jargon import JargonManagement\n",
    "from package.databases.session import Session, get_session, Depends\n",
    "from package.databases.models.user import User\n",
    "from package.databases.models.document import Document\n",
    "from package.databases.models.project import Project\n",
    "from package.databases.models.jargon import Jargon\n",
    "from package.databases.models.longterm import LongTerm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "896fa865",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = User(\n",
    "    username=\"bank\",\n",
    "    password=\"555\",\n",
    "    email=\"bank@bank.com\"\n",
    ")\n",
    "\n",
    "um = UserManagement()\n",
    "user = um.create_user(user, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deb4fb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de8f4577-7800-4be5-a4e8-86f7df3ad2aa'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d193b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = Document(source=\"./sources/storm.pdf\", type=\"pdf\")\n",
    "\n",
    "dm = DocumentManagement()\n",
    "document1 = dm.create_document(document1, session=Depends(get_session)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f7cfd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3ec94e05-d642-433c-b4c5-74740b3f636d'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document1.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec989c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LongTerm(id='bb41b6f8-04ea-4025-a6d9-91171260835c', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '1 Introduction', 'sequence': 4}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 785699), raw=\"outlines or even entire articles ( Direct Gen ). However, this approach is limited by a lack of details and hallucinations (Xu et al., 2023), particularly in addressing long-tail topics (Kandpal et al., 2023). This underscores the importance of leveraging external sources, and current strategies often involve retrieval-augmented generation ( RAG ), which circles back to the problem of researching the topic in the pre-writing stage, as much information cannot be surfaced through simple topic searches.\\n\\nHuman learning theories (Tawfik et al., 2020; Booth et al., 2003) highlight asking effective questions in information acquisition. Although instruction-tuned models (Ouyang et al., 2022) can be prompted directly to generate questions, we find that they typically produce basic 'What', 'When', and 'Where' questions (Figure 1 (A)) which often only address surface-level facts about the topic. To endow LLMs with the capacity to conduct better research, we propose the STORM paradigm for the S ynthesis of T opic O utlines through R etrieval and M ulti-perspective Question Asking.\\n\\nThe design of STORM is based on two hypotheses: (1) diverse perspectives lead to varied questions; (2) formulating in-depth questions requires iterative research. Building upon these hypotheses, STORM employs a novel multi-stage approach. It first discovers diverse perspectives by retrieving and analyzing Wikipedia articles from similar topics and then personifies the LLM with specific perspectives for question asking (Figure 1 (B)). Next, to elicit follow-up questions for iterative research\\n\\n(Figure 1 (C)), STORM simulates multi-turn conversations where the answers to the generated questions are grounded on the Internet. Finally, based on the LLM's internal knowledge and the collected information, STORM creates an outline that can be expanded section by section to develop a fulllength Wikipedia-like article.\\n\\nWe evaluate STORM using our FreshWiki dataset (§2.1) which curates recent, high-quality Wikipedia articles to avoid data leakage during pretraining. 1 To facilitate the study of the pre-writing stage, we define metrics for evaluating the outline quality against human-written articles.\\n\\nWe further invited a group of experienced Wikipedia editors for expert evaluation. The editors found STORM outperforms an outline-driven RAGbaseline, especially regarding the breadth and organization of the articles. They also identified challenges for future research, including addressing cases where: (1) the bias on the Internet affects the generated articles; (2) LLMs fabricate connections between unrelated facts. These challenges present new frontiers to grounded writing systems.\\n\\nOur main contributions include:\\n\\n- · To evaluate the capacity of LLM systems at generating long-form grounded articles from scratch, and the pre-writing challenge in particular, we curate the FreshWiki dataset and establish evaluation criteria for both outline and final article quality.\\n- · We propose STORM, a novel system that automates the pre-writing stage. STORM researches the topic and creates an outline by using LLMs to ask incisive questions and retrieving trusted information from the Internet.\\n- · Both automatic and human evaluation demonstrate the effectiveness of our approach. Expert feedback further reveals new challenges in generating grounded long-form articles.\\n\", raw_embedding=array([-0.00073814,  0.01217651, -0.02708435, ..., -0.01152802,\n",
       "         0.04190063, -0.04598999], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='b1373fb7-0696-42dc-a785-edd37c3a5953', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '7 Related Works', 'sequence': 28}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw=\"7 Related Works\\n\\nRetrieval-Augmented Generation (RAG) Augmenting language models (LMs) with retrieval at inference time is a typical way to leverage external knowledge stores (Ram et al., 2023; Izacard et al., 2023). While some works use retrieval to construct demonstrations for in-context learning (Li et al., 2023; Liu et al., 2022; Agrawal et al., 2023; Poesia et al., 2022; Shi et al., 2022; Khattab et al., 2022), another line of works uses retrieval to provide additional information for LMs to ground on. Lewis et al. (2020) study RAG on knowledgeintensive NLP tasks and find it improves diversity and factuality. Semnani et al. (2023) designs a RAG-based chatbot grounded on English Wikipedia to stop LLM-based chatbots from hallucination. Besides, RAG can be used to generate text with citations (Menick et al., 2022; Gao et al., 2023) and build attributed question answering systems (Bohnet et al., 2023). While RAG is widely studied in question answering, how to use it for long-form article generation is less investigated.\\n\\nAs a general framework, RAG is flexible in both the retrieval source and time. The retrieval sources can vary from domain databases (Zakka et al., 2023), code documentation (Zhou et al., 2023), to the whole Internet (Nakano et al., 2022; Komeili et al., 2022). Regarding the time, besides a onetime retrieval before generation, the system can be designed to self-decide when to retrieve across the course of the generation (Jiang et al., 2023b; Parisi et al., 2022; Shuster et al., 2022; Yao et al., 2023). Automatic Expository Writing Different from other types of long-form generation (Yang et al., 2022; Feng et al., 2018), automatic expository writing requires grounding on external documents and leveraging the interplay between reading and writing. Balepur et al. (2023) propose the ImitateRetrieve-Paraphrase framework for expository writing at the paragraph level to address the challenges in synthesizing information from multiple sources. Beyond summarizing sources, Shen et al. (2023) highlight that expository writing requires the author's sensemaking process over source documents and good outline planning. We tackle these challenges by focusing on the pre-writing stage.\\n\\nQuestion Asking in NLP Question asking capabilities in NLP systems have expanded across several fronts, including generating clarification questions to understand user intents (Aliannejadi et al., 2019; Rahmani et al., 2023), and breaking large questions into smaller ones to improve compositional reasoning (Press et al., 2023). While humans usually ask questions to learn new knowledge (Tawfik et al., 2020; Booth et al., 2003), how to optimize question informativeness and specificity in information-seeking conversations remains less explored. The closest work is Qi et al. (2020) which defines the question informativeness using the unigram precision function and uses reinforcement learning to increase the question informativeness.\\n\", raw_embedding=array([-0.02796936, -0.00827026,  0.01646423, ..., -0.00125504,\n",
       "         0.0458374 , -0.01782227], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='5de40437-1454-469e-bde4-c5c335def5a0', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'References', 'sequence': 38}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw=\"writing tasks.\\n\\nWeijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer. 2022. Nearest neighbor zero-shot inference. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 3254-3265, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. 2022. Language models that seek for knowledge: Modular search &amp; generation for dialogue and prompt completion. In Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 373-393, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pages 3784-3803, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nChristine M Tardy. 2010. Writing for the world: Wikipedia as an introduction to academic writing. In English teaching forum , volume 48, page 12. ERIC.\\n\\nAndrew A Tawfik, Arthur Graesser, Jessica Gatewood, and Jaclyn Gishbaugher. 2020. Role of questions in inquiry-based instruction: towards a design taxonomy for question-asking and implications for design. Educational Technology Research and Development , 68:653-678.\\n\\nCharles A Weaver III and Walter Kintsch. 1991. Expository text.\\n\\nKarsten Wenzlaff and Sebastian Spaeth. 2022. Smarter than humans? validating how openai's chatgpt model explains crowdfunding, alternative finance and community finance. Validating how OpenAI's ChatGPT model explains Crowdfunding, Alternative Finance and Community Finance.(December 22, 2022) .\\n\\nFangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023. A critical evaluation of evaluations for long-form question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3225-3245, Toronto, Canada. Association for Computational Linguistics.\\n\\nKevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. 2023. DOC: Improving long story coherence with detailed outline control. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3378-3465, Toronto, Canada. Association for Computational Linguistics.\\n\\nKevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. 2022. Re3: Generating longer stories with recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 4393-4479, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations .\\n\\nCyril Zakka, Akash Chaurasia, Rohan Shad, Alex R Dalal, Jennifer L Kim, Michael Moor, Kevin Alexander, Euan Ashley, Jack Boyd, Kathleen Boyd, et al. 2023. Almanac: Retrieval-augmented language models for clinical medicine. Research Square .\\n\\nShuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig. 2023. Docprompting: Generating code by retrieving the docs. In The Eleventh International Conference on Learning Representations .\\n\\nTable 7: Statistics of the dataset used in our experiments.\\n\\n| Average Numer of Sections            |    8.4 |\\n|--------------------------------------|--------|\\n| Average Number of All-level Headings |   15.8 |\\n| Average Length of a Section\", raw_embedding=array([-0.06750488, -0.00054407,  0.01161194, ...,  0.04238892,\n",
       "         0.01416779, -0.02980042], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='6c66dae4-fe13-43e8-9f03-dae1f0a959c5', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'References', 'sequence': 33}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw='References\\n\\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2023. Incontext examples selection for machine translation. In Findings of the Association for Computational Linguistics: ACL 2023 , pages 8857-8873, Toronto, Canada. Association for Computational Linguistics.\\n\\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. 2019. FLAIR: An easy-to-use framework for state-of-theart NLP. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) , pages 54-59, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nMohammad Aliannejadi, Hamed Zamani, Fabio Crestani, and W Bruce Croft. 2019. Asking clarifying questions in open-domain information-seeking conversations. In Proceedings of the 42nd international acm sigir conference on research and development in information retrieval , pages 475-484.\\n\\nNishant Balepur, Jie Huang, and Kevin Chang. 2023. Expository text generation: Imitate, retrieve, paraphrase. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 11896-11919, Singapore. Association for Computational Linguistics.\\n\\nSiddhartha Banerjee and Prasenjit Mitra. 2015. WikiKreator: Improving Wikipedia stubs automatically. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 867-877, Beijing, China. Association for Computational Linguistics.\\n\\nBernd Bohnet, Vinh Q. Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Massimiliano Ciaramita, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, Tom Kwiatkowski, Ji Ma, Jianmo Ni, Lierni Sestorain Saralegui, Tal Schuster, William W. Cohen, Michael Collins, Dipanjan Das, Donald Metzler, Slav Petrov, and Kellie Webster. 2023. Attributed question answering: Evaluation and modeling for attributed large language models.\\n\\nWayne C Booth, Gregory G Colomb, and Joseph M Williams. 2003. The craft of research . University of Chicago press.\\n\\nLaura Dietz and John Foley. 2019. Trec car y3: Complex answer retrieval overview. In Proceedings of Text REtrieval Conference (TREC) .\\n\\nChristina S Doyle. 1994. Information literacy in an information society: A concept for the information age . Diane Publishing.\\n\\nAnn-Marie Eriksson and Åsa Mäkitalo. 2015. Supervision at the outline stage: Introducing and encountering issues of sustainable development through aca- demic writing assignments. Text &amp; Talk , 35(2):123153.\\n\\nAngela Fan and Claire Gardent. 2022. Generating biographies on Wikipedia: The impact of gender bias on the retrieval-based generation of women biographies. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8561-8576, Dublin, Ireland. Association for Computational Linguistics.\\n\\nXiaocheng Feng, Ming Liu, Jiahao Liu, Bing Qin, Yibo Sun, and Ting Liu. 2018. Topic-to-essay generation with neural networks. In IJCAI , pages 4078-4084.\\n\\nTira Nur Fitria. 2023. Artificial intelligence (ai) technology in openai chatgpt application: A review of chatgpt in writing english essay. In ELT Forum: Journal of English Language Teaching , volume 12, pages 44-58.\\n\\nPasi Fränti and Radu Mariescu-Istodor. 2023. Soft precision and recall. Pattern Recognition Letters , 167:115121.\\n\\n- R Edward Freeman, Jeffrey S Harrison, Andrew C Wicks, Bidhan L Parmar, and Simone De Colle. 2010. Stakeholder theory: The state of the art.\\n\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text', raw_embedding=array([-0.0579834 ,  0.01215363, -0.02140808, ...,  0.04745483,\n",
       "         0.05505371, -0.00232124], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='7ecaaf6b-57d4-4e5a-9ee5-aebbaaec3986', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'end', 'sequence': 46}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 789699), raw='end\\n\\n28\\n\\nconvos.append(convo\\\\_history)\\n\\n- 29 end\\n- 30 // Create the outline.\\n- 31 O D ← direct\\\\_gen\\\\_outline ( t )\\n- 32 O ← refine\\\\_outline ( t , O D, convos)\\n- 33 return O , R\\n\\n<!-- formula-not-decoded -->\\n\\nThe soft heading recall is calculated as\\n\\n<!-- formula-not-decoded -->\\n\\nwhere the cardinality of intersection is defined via the union as follows:\\n\\n<!-- formula-not-decoded -->\\n', raw_embedding=array([-0.03070068, -0.0171814 , -0.00347519, ...,  0.01597595,\n",
       "         0.03530884, -0.00684738], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='50e589b1-9a01-404e-9459-0b66de93b717', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Taylor Hawkins', 'sequence': 74}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw=\"Taylor Hawkins\\n\\nOliver Taylor Hawkins (1972-2022) was an American musician, best known as the drummer for the rock band Foo Fighters[1]. Born in Fort Walton, Texas, Hawkins' love for music was ignited at a young age, particularly after watching a Queen concert in 1982[2][3][5]. He kick-started his professional career as the drummer for Alanis Morissette's band during her world tour for the hit album 'Jagged Little Pill'[8][9]. His talents were recognized by Dave Grohl, who invited him to join the Foo Fighters in 1997, marking the beginning of his impactful tenure with the band[7][8].\\n\\nHawkins was celebrated for his versatile drumming style, drawing influence from renowned drummers like Roger Taylor, Neil Peart, Phil Collins, Alex Van Halen, and Stewart Copeland[14]. His performances, marked by a unique energy and aggressive style of play, earned him recognition as one of the top rock drummers of his era[15]. Apart from his role in the Foo Fighters, Hawkins' passion for music saw him involved in numerous side projects and collaborations, cementing his place in the world of rock music[10].\\n\\nOutside of his professional life, Hawkins was known for his resilience and dedication to his family. Despite personal struggles, including a near-fatal drug overdose in 2001, Hawkins remained committed to his musical career[4][9]. His legacy continues to inspire musicians and fans alike, as his contributions to rock music, coupled with his indomitable spirit, made him an unforgettable icon in the music industry[13].\\n\\nHawkins' sudden death in 2022 while on tour in Bogotá, Colombia, sent shockwaves through the music world[34]. Tributes poured in from around the globe, reflecting the respect and admiration Hawkins had garnered during his lifetime[21][31]. His life and career were honored at a star-studded tribute concert in Los Angeles, attesting to the enduring impact of his music[22].\\n\", raw_embedding=array([-0.00629807, -0.03173828, -0.05041504, ...,  0.03341675,\n",
       "         0.01391602,  0.00273323], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='a0ba1d16-d1db-419b-a0cf-c7d9d8cda459', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '# Discography', 'sequence': 80}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 792700), raw='# Discography\\n\\nTaylor Hawkins also led a notable music career through his own side projects and collaborations[10]. Aside from his work with the Foo Fighters, Hawkins formed and fronted the band Taylor Hawkins &amp; The Coattail Riders, a project which originated from jamming sessions with his friend Drew Hester[10].\\n', raw_embedding=array([ 0.00459671, -0.0166626 , -0.03433228, ...,  0.05352783,\n",
       "         0.05905151, -0.01887512], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='02dde186-3b8a-4fcb-bde1-047526308b6e', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 56}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw=\"                                                                                                     | Source                                                                                                                                                                                            |\\n|------------------------------|-------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| Improper Inferential Linking | Lahaina, Hawaii               | Throughout its history, religion has remained the paramount aspect of Hawaiian life in Lahaina , permeating every daily activity and significant event[5].                   | [5] 'Religion, Beliefs &Spirituality' (The source discusses religion as part of Hawaiian life but does not mention Lahania .)                                                                     |\\n| Inaccurate Paraphrasing      | 2022 Crimean Bridge explosion | Completed in June 2020 , the bridge serves as a major supply route for Russian forces in the region and is significant to Russia's claim over the disputed territory[2][11]. | [2] 'Crimean Bridge - Wikipedia' (The source says 'The first scheduled passenger train crossed the bridge\", raw_embedding=array([ 0.0231781 ,  0.0103302 , -0.0252533 , ...,  0.02185059,\n",
       "        -0.01902771,  0.01130676], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='08e992ff-315d-483c-9b5a-988b3d92b9c1', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '## Get the Money', 'sequence': 83}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 792700), raw='## Get the Money\\n\\nGet the Money, the third album from Taylor Hawkins &amp; The Coattail Riders, was released on November 8, 2019[29]. The album\\'s first single, \"Crossed the Line\", released on October 15, 2019, featured Dave Grohl and Jon Davison, the frontman of Yes[29]. The music video for the single \"I Really Blew It\" also featured appearances from Grohl and Perry Farrell[29].\\n', raw_embedding=array([ 0.00939941, -0.01681519, -0.04769897, ...,  0.05673218,\n",
       "        -0.01055908, -0.04971313], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='06c6433f-4eee-4cdb-bb00-db93eb893ea8', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '8 Conclusion', 'sequence': 29}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw='8 Conclusion\\n\\nWe propose STORM, an LLM-based writing system that automates the pre-writing stage for creating Wikipedia-like articles from scratch. We curate the FreshWiki dataset and establish evaluation criteria to study the generation of grounded longform articles. Experimental results demonstrate that the question asking mechanism in STORM improves both the outline and article quality. With the improved breadth and depth, STORM helps surface new challenges for grounded writing systems through expert evaluation. The experienced Wikipedia editors in our study unanimously agree that STORM is helpful for their pre-writing stage.\\n', raw_embedding=array([-0.01628113,  0.00331879, -0.01730347, ..., -0.01514435,\n",
       "        -0.00361824,  0.01136017], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='113e9102-2a57-427f-b55d-08833237a41d', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 57}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw=\"  | [5] 'Religion, Beliefs &Spirituality' (The source discusses religion as part of Hawaiian life but does not mention Lahania .)                                                                     |\\n| Inaccurate Paraphrasing      | 2022 Crimean Bridge explosion | Completed in June 2020 , the bridge serves as a major supply route for Russian forces in the region and is significant to Russia's claim over the disputed territory[2][11]. | [2] 'Crimean Bridge - Wikipedia' (The source says 'The first scheduled passenger train crossed the bridge on 25 December 2019, while the bridge was opened for freight trains on 30 June 2020 '.) |\\n| Citing Irrelevant Sources    | LK-99                         | For example, comparisons have been drawn between the performance of LK-9 and the dynamic resolution capabilities of video games such as Battlefield 2042[22].                | [22] 'Battlefield 2042 PC performance guide: The best settings for a high frame rate' ( The source is irrelevant to LK-99. )                                                                      |\\n\\nWeuse Mistral 7B-Instruct 15 (Jiang et al., 2023a) to examine whether the cited passages entail the generated sentence. Table 4 reports the citation quality of articles produced by our approach, showing that around 15% sentences in generated articles are unsupported by citations. We further investigate the failure cases by randomly sampling 10 articles and an author manually examines all the unsupported sentences in these articles. Besides sentences that are incorrectly split 16 , lack citations, or are deemed supported by the author's judgment, our analysis identifies three main error categories (examples are given in Table 9): improper inferential linking , inaccurate paraphrasing , and citing irrelevant sources .\\n\\nWe show the error distribution in Figure 6. Notably, the most common errors stem from the tendency of LLMs to form improper inferential links between different pieces of information presented in the context window. Our analysis of citation quality suggests that, in addition to avoiding hallucinations, future research in grounded text generation should also focus on preventing LLMs from making overly inferential\", raw_embedding=array([-0.01088715, -0.02172852, -0.02848816, ..., -0.02122498,\n",
       "        -0.01145935, -0.01303101], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='121fc3a9-443f-4e03-8a97-be27c07ce3af', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.4 STORM Implementation', 'sequence': 18}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 787707), raw='            | 2.87             | 4.60             | 3.10             | 4.16             |\\n| RAG               | 28.52                                    | 13.18                                    | 7.57                                     | 3.14             | 4.22             | 3.05             | 4.08             |\\n| oRAG              | 44.26                                    | 16.51                                    | 12.57                                    | 3.90             | 4.79             | 4.09             | 4.70             |\\n| STORM             | 45.82                                    | 16.70             ', raw_embedding=array([-0.02622986,  0.03631592, -0.02732849, ..., -0.01705933,\n",
       "        -0.014534  ,  0.01855469], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='142d6907-4eea-4f21-a29a-f678d27f3e9b', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '3 Method', 'sequence': 9}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 786707), raw=\"3 Method\\n\\nWe present STORM to automate the pre-writing stage by researching a given topic via effective question asking (§3.1, §3.2) and creating an outline (§3.3). The outline will be extended to a fulllength article grounded on the collected references\\n\\n5 https://en.wikipedia.org/wiki/Wikipedia: Stand-alone\\\\_lists\\n\\n6 Since language models process and produce sequences, we can linearize O by adding '#' to indicate section titles, '##' to indicate subsection titles, etc.\\n\\nFigure 2: The overview of STORM that automates the pre-writing stage. Starting with a given topic, STORM identifies various perspectives on covering the topic by surveying related Wikipedia articles ( 1 - 2 ). It then simulates conversations between a Wikipedia writer who asks questions guided by the given perspective and an expert grounded on trustworthy online sources ( 3 - 6 ). The final outline is curated based on the LLM's intrinsic knowledge and the gathered conversations from different perspectives ( 7 - 8 ).\\n\\n<!-- image -->\\n\\n(§3.4). Figure 2 gives an overview of STORM and we include the pseudo code in Appendix B.\\n\", raw_embedding=array([ 0.00162315,  0.0008812 , -0.01061249, ..., -0.0074501 ,\n",
       "         0.03933716, -0.00043154], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='173e4069-d359-4ddb-8edb-3e28def3d3d5', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '# Personal Life', 'sequence': 78}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 792700), raw=\"# Personal Life\\n\\nTaylor Hawkins married Alison Hawkins, an American celebrity and entrepreneur, in 2005[18]. The couple had three children, Oliver, Annabelle, and Everleigh[19]. Hawkins' commitment to his family was evident; in fact, he even wrote a song for his middle child, Annabelle[9].\\n\\nIn his personal life, Hawkins had also struggled with drug use, which nearly claimed his life in a 2001 overdose[9][7][4]. However, he managed to overcome this challenge, and later expressed gratitude for the experience as a lesson that allowed him to realize the destructive path he was on[7].\\n\\nOutside of his main role in the Foo Fighters, Hawkins also pursued various side projects including the Birds of Satan, NHC, and Chevy Metal. His motivation for such ventures was a constant drive to create and his love for music[7]. Hawkins was also known for his unabashed fanboy nature, often vocalizing his admiration for fellow musicians and his heroes[7].\\n\", raw_embedding=array([ 0.03053284,  0.00016391, -0.02316284, ...,  0.04336548,\n",
       "         0.00461197, -0.01589966], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='176e2ed0-7870-4365-8628-8900a5882590', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Interest Level', 'sequence': 67}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw=\"|\\n| Red herring fallacy, associating unrelated sources | 11               | Polling from America shouldn't be included and links to climate change shouldn't be made unless explicitly connected by the source. (comment on article Typhoon Hinnamnor ) Sourcing seems mostly fine, though some aren't directly related (Ex. 39,40). (comment on article Gehraiyaan )                                                                                                                                                                                                                                                  |\\n| Missing important information                      | 6                | should be linked to. (comment on article 2022 AFL Grand Final ) 'One study, conducted by Sinéad Griffin, a physicist at the Lawrence Berkeley National Laboratory, provided some analysis of LK-99's abilities using supercomputer simulations[20].' This is not enough information about the analysis, which would have been very useful in the article. (comment on article LK-99 )                                                                                             \", raw_embedding=array([-0.06057739,  0.03320312, -0.02047729, ...,  0.00761795,\n",
       "         0.01548004, -0.00607681], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='28c3c29c-d96a-4bfd-9ee7-0b63d4d18199', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '5 Results and Analysis', 'sequence': 23}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 787707), raw=\"5 Results and Analysis\\n\\n5.1 Main Results\\n\\nWeuse outline coverage as a proxy to assess the prewriting stage (see §2.2). Table 3 shows the heading soft recall and entity recall. Outlines directly generated by LLMs ( Direct Gen ) already demonstrate\\n\\n10 https://documentation.you.com/api-reference/ search\\n\\nWe further evaluate the full-length article quality. As shown in Table 2, oRAG significantly outperforms RAG , highlighting the effectiveness of using outlines for structuring full-length article generation. Despite this method's advantages in leveraging retrieval and outlining, our approach still outperforms it. The effective question asking mechanism enhances the articles with greater entity recall. The evaluator LLM also rates these articles with significantly higher scores in the aspects of 'Interest Level', 'Relevance and Focus', and 'Coverage'. Nonetheless, we acknowledge the possibility of the evaluator LLM overrating machine-generated text. Our careful human evaluation (§6) reveals that STORM still has much room for improvement.\\n\\nAlthough this work primarily focuses on the prewriting stage and does not optimize generating text with citations, we still examine the citation quality of articles produced by our approach. As reported\\n\\n|       |   Citation Recall |   Citation Precision |\\n|-------|-------------------|----------------------|\\n| STORM |             84.83 |                85.18 |\\n\\nTable 4: Citation quality judged by Mistral 7B-Instruct.\\n\\n|     |   STORM |   w/o Perspective |   w/o Conversation |\\n|-----|---------|-------------------|--------------------|\\n| |R| |   99.83 |             54.36 |              39.56 |\\n\\nTable 5: Average number of unique references ( |R| ) collected using different methods.\\n\\nin Table 4, Mistral 7B-Instruct judges 84.83% of the sentences are supported by their citations. Appendix C.3 investigates the unsupported sentences and reveals that the primary issues stem from drawing improper inferences and inaccurate paraphrasing, rather than hallucinating non-existent contents.\\n\", raw_embedding=array([-0.0275116 , -0.00557327, -0.01234436, ..., -0.00057173,\n",
       "         0.02140808, -0.04675293], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='2be03be0-af4a-4bc4-98e2-b49909fbe2fb', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Interest Level', 'sequence': 71}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw='                                                                                                                                                                                                                                                                                               |\\n\\nSelect a key\\n\\nSelected Key:\\n\\nTitle: Taylor Hawkins marshall com\\n\\nSnippets;\\n\\n1995, off the back of her hit album \\'Jagged Little Pill; Canadian American superstar Alanis Morissette recruited him to be her touring drummer for her 18 month Ibum tour; along with him featuring in the music videos for \"You Oughta Know\" \"AllI Really Want\\' and \"You Learn It was during this tour that he met his musical soul mate; Dave Grohl. The tour with Alanis ended and hed heard that Dave and Foo Fighters were looking new drummer; s0 Taylor enquired Dave initially thought that Taylor wouldn\\'t be interested in joining Alanis Morissette was much bigger than the Foo Fighters at that time, but Taylor jumped at the chance due his overwhelming desire bein rock band.It was from that that the bromance between them began andon March 1Bth 1997 Taylor was announced as their new drummer point\\n\\nLogout\\n\\nSelect an option:\\n\\n- (11, \\'Taylor Hawkins Quite Richard] )\\n\\nYou selected: Taylor Hawkins [Quite Richard]\\n', raw_embedding=array([ 0.00235367, -0.02790833, -0.04098511, ...,  0.04067993,\n",
       "         0.04162598,  0.01370239], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='2cb28d72-b700-45b3-aee4-8595ae10503e', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 48}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 789699), raw=\"C.3 More Discussion of the Citation Quality\\n\\nFigure 6: Error analysis of unsupported sentences in 10 sampled articles.\\n\\n<!-- image -->\\n\\nTable 8: Scoring rubrics on a 1-5 scale for the evaluator LLM.\\n\\n| Criteria Description   | Interest Level : How engaging and thought-provoking is the article?                                                                                                                      |\\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| Score 1 Description    | Not engaging at all; no attempt to capture the reader's attention.                                                                                                                       |\\n| Score 2 Description    | Fairly engaging with a basic narrative but lacking depth.                                                                                                                                |\\n| Score 3 Description    | Moderately engaging with several interesting points.                                           \", raw_embedding=array([-0.05389404, -0.03497314,  0.01122284, ..., -0.02416992,\n",
       "         0.04388428,  0.0189209 ], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='31583f66-96fe-4e2f-8465-0a66e558f20e', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Limitations', 'sequence': 30}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw='Limitations\\n\\nIn this work, we explore generating Wikipedialike articles from scratch as a way to push the frontier of automatic expository writing and longform article generation. While our approach significantly outperforms baseline methods in both automatic and human evaluations, the quality of machine-written articles still lags behind wellrevised human-authored articles, specifically in aspects of neutrality and verifiability. Although STORM discovers different perspectives in researching the given topic, the collected information may still be biased towards dominant sources on the Internet and may contain promotional content. Moreover, the verifiability issues identified in this work go beyond factual hallucination, which highlights new challenges to grounded writing systems.\\n\\nAnother limitation of this work is that although we focus on the task of generating Wikipedia-like articles from scratch, our task setup is still simpli- fied to only consider the generation of free-form text. Human-authored high-quality Wikipedia articles usually contain structured data and multimodal information. We leave the exploration of generating multi-modal grounded articles for future work.\\n', raw_embedding=array([-0.03356934, -0.01992798, -0.01499176, ...,  0.00279236,\n",
       "        -0.04443359,  0.02378845], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='3323a3f2-7c31-4496-b0f8-a47879371fe9', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '# Career', 'sequence': 76}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 792700), raw=\"# Career\\n\\nTaylor Hawkins began his professional music career playing in Alanis Morissette's band during her 18-month world tour in support of the hit album 'Jagged Little Pill' from 1995 to 1997[8][9]. His performances not only in the tour but also in the music videos for 'You Oughta Know', 'All I Really Want' and 'You Learn' introduced him to the world of rock music and ultimately led to his meeting with Dave Grohl[8]. Throughout this time, Hawkins contributed significantly to the band's sound and performance, transforming the songs from their original drum loop format to a rock-band vibe that resonated with audiences[1][7].\\n\\nIn 1997, Hawkins was asked by Grohl to join the Foo Fighters, an invitation that he readily accepted[7][8]. At the time, Grohl thought it was a long shot to recruit Hawkins given that Morissette was at the height of her career, but Hawkins' desire to be a part of a rock band compelled him to make the move[7]. This marked the beginning of Hawkins' tenure as the drummer of the Foo Fighters, a role that he would play until his passing[6][9].\\n\\nApart from his work with Morissette and the Foo Fighters, Hawkins had an array of other musical experiences[10]. He drummed for Sass Jordan before joining Morissette's touring band[10]. He was part of an ad hoc drum supergroup called SOS Allstars and filled the void for Coheed and Cambria's 2007 album after their drummer Josh Eppard left the group[10]. In addition, Hawkins formed his own side project, the Coattail Riders, in 2005, through which he recorded his own music and took the project on the road, performing in small clubs despite the Foo Fighters' arena-status[7]. His son, Shane Hawkins, has since taken on his father's legacy, joining the Foo Fighters for a performance during the Boston Calling Music Festival in 2023[6].\\n\", raw_embedding=array([-0.00357246, -0.01084137, -0.03469849, ...,  0.04541016,\n",
       "         0.03796387,  0.01498413], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='33cb5faf-8def-439b-801a-1f8072adbb3c', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '3.1 Perspective-Guided Question Asking', 'sequence': 10}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 786707), raw=\"3.1 Perspective-Guided Question Asking\\n\\nRohman (1965) defines pre-writing as the stage of discovery in the writing process. In parallel with stakeholder theory in business (Freeman et al., 2010), where diverse stakeholders prioritize varying facets of a company, individuals with distinct perspectives may concentrate on different aspects when researching the same topic and discover multifaceted information. Further, the specific perspectives can serve as prior knowledge, guiding individuals to ask more in-depth questions. For example, an event planner might ask about the 'transportation arrangements' and 'budget' for 'the 2022 Winter Olympics opening ceremony', whereas a layperson might ask more general questions about the event's basic information (Figure 1 (A)).\\n\\nGiven the input topic t , STORM discovers different perspectives by surveying existing articles from similar topics and uses these perspectives to control the question asking process. Specifically, STORM prompts an LLM to generate a list of related topics and subsequently extracts the tables of contents from their corresponding Wikipedia articles, if such articles can be obtained through Wikipedia API 7 (Figure 2 1 ). These tables of contents are concatenated to create a context to prompt the LLM to identify N perspectives P = { p 1 , ..., p N } that can collectively contribute to a comprehensive article on t (Figure 2 2 ). To ensure that the basic information about t is also covered, we add p 0 as 'basic fact writer focusing on broadly covering the basic facts about the topic' into P . Each perspective p ∈ P will be utilized to guide the LLM in the process of question asking in parallel.\\n\", raw_embedding=array([-0.0531311 , -0.002882  , -0.01478577, ..., -0.02531433,\n",
       "         0.02845764, -0.02459717], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='342cd386-7ba5-4995-be2e-eb74d8061947', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Verifiability', 'sequence': 64}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw='Verifiability\\n\\n- 1: No supporting evidence; claims are unsubstantiated.\\n- 2: Rarely supported with evidence; many claims are unsubstantiated.\\n\\n3: Inconsistently verified; some claims are supported; evidence is occasionally provided.\\n\\n- 4: Generally verified; claims are usually supported with evidence; however, there might be a few instances where verification is lacking\\n- 5: Well-supported; claims are very well supported with credible evidence, and instances of unsupported claims are rare.\\n- 6: Very well-supported; almost every claim is substantiated with credible evidence, showing a high level of thorough verification.\\n- 7: Exemplary verification; each claim is supported by robust, credible evidence from authoritative sources, reflecting strict adherence to the no original research policy.\\n', raw_embedding=array([ 0.00188351,  0.01506805,  0.00403595, ..., -0.01100159,\n",
       "         0.00466919, -0.03231812], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='3c09f210-283d-424f-9f15-9a07e2070446', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '## Tributes and Remembrances', 'sequence': 86}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 792700), raw='## Tributes and Remembrances\\n\\nIn the wake of Hawkins\\' death, tributes from fans and colleagues alike poured in from around the world[21][31]. Among the many paying their respects were legendary rock and roll musicians like Roger Taylor, the drummer of Queen, who Hawkins credited with inspiring his own career behind the drum set[21]. In heartfelt social media posts, Taylor described Hawkins as an \"inspirational mentor\" and a \"kind brilliant man\"[21], while Led Zeppelin\\'s Jimmy Page reminisced about sharing the stage with Hawkins and praised his \"technique, energy and spirited enthusiasm\"[21].\\n\\nThere were also numerous onstage tributes to Hawkins. Notably, Miley Cyrus expressed her grief and sent peaceful wishes to the Foo Fighters and the Hawkins family during a performance at Lollapalooza[31]. Similarly, Liam Gallagher of Oasis dedicated one of the band\\'s biggest hits to Hawkins during a concert at the Royal Albert Hall in London[31].\\n\\nFans gathered outside the hotel where Hawkins died, lighting candles, leaving flowers, and singing the band\\'s songs in his honor[31].\\n\\nHawkins\\' life and career were celebrated in a star-studded tribute concert in Los Angeles, which saw performances from over 50 musicians, including his former bands and colleagues from Def Leppard, Queen, and Foo Fighters[22].\\n\\nTable 12: STORM\\'s generated article for \\'Taylor Hawkins\\'. \\'#\\', \\'##\\' indicate the section title and subsection title respectively. Numbers in brackets indicate the cited references.', raw_embedding=array([-0.00276375,  0.00399399, -0.0390625 , ...,  0.00631332,\n",
       "         0.03494263,  0.00570297], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='3c81a1c7-fc0e-4f04-8a65-40273f2f7476', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '# Musical Style and Influences', 'sequence': 77}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 792700), raw='# Musical Style and Influences\\n\\nTaylor Hawkins was a profound drummer, with his musical style and influences spreading across a wide array of rock genres[11]. Known for his passionate fandom of groups that came before him, Hawkins regularly expressed his admiration for bands like Rush, Genesis, and the Police, all of which featured some of the greatest drummers in rock history like Neil Peart, Phil Collins, and Stewart Copeland[11].\\n\\nHe was heavily influenced by his love for classic rock, as evidenced by his performances, where he covered songs from bands like Van Halen[11].\\n\\nHawkins drew influences from a variety of drumming styles, developing a signature style inspired by greats like Roger Taylor, Neil Peart, Phil Collins, Alex Van Halen, and Stewart Copeland[14]. This distinctive style and influence extended to his drum kit, which incorporated elements like rototoms and concert toms[14].\\n\\nBeyond his influences, Hawkins had a unique energy that made him stand out as a drummer. His performances were recognized for their power, and he was known for his enthusiastic and aggressive style of play[15]. This earned him recognition as one of the top rock drummers of his time, with his passion for music living on through his performances[14].\\n\\nThrough his career, Hawkins left an indelible mark on rock music, through his distinct style, passion, and contributions to the music industry[13]. His love for music and dedication to his craft made him an unforgettable icon in the world of rock music[13].\\n', raw_embedding=array([-0.01380157, -0.00843811, -0.04580688, ...,  0.02503967,\n",
       "         0.00640488, -0.01409149], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='3cf4ce23-b7e7-4508-8024-51b7c1972fa7', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Broad Coverage', 'sequence': 63}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw=\"Broad Coverage\\n\\n- 1: Severely lacking; offers little to no coverage of the topic's primary aspects, resulting in a very narrow perspective.\\n- 2: Minimal coverage; addresses only a small selection of the topic's main aspects, with significant omissions.\\n- 3: Partial coverage; includes some of the topic's main aspects but misses others, resulting in an incomplete portrayal.\\n- 4: Acceptable breadth; covers most main aspects, though it may stray into minor unnecessary details or overlook some relevant points.\\n- 5: Good coverage; achieves broad coverage of the topic, hitting on all major points with minimal extraneous information.\\n- 6: Comprehensive; provides thorough coverage of all significant aspects of the topic, with a well-balanced focus.\\n- 7: Exemplary in breadth; delivers outstanding coverage, thoroughly detailing all crucial aspects of the topic without including irrelevant information.\\n\", raw_embedding=array([-0.05148315,  0.01647949, -0.00024867, ..., -0.0362854 ,\n",
       "         0.00195503, -0.0423584 ], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='3ecb8ac5-10f6-48b0-b571-2645370f8d9e', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 53}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw='                                       |\\n| Score 2 Description    | Somewhat on topic but with several digressions; the core subject is evident but not consistently adhered to.                                                                             |\\n| Score 3 Description    | Generally on topic, despite a few unrelated details.                                                                                                                                     |\\n| Score 4 Description    | Mostly on topic and focused; the narrative has a consistent relevance to the core subject with infrequent digressions.                                                                   |\\n| Score 5 Description    | Exceptionally focused and entirely on topic; the article is tightly centered on the subject, with every piece of information contributing to a comprehensive understanding of the topic. |\\n| Criteria Description   | Broad Coverage : Does the article provide an in-depth exploration of the topic and have good coverage?                                                              ', raw_embedding=array([-0.05273438, -0.02159119, -0.00674438, ..., -0.03704834,\n",
       "         0.01576233, -0.00919342], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='40147eee-6010-4506-b15d-0ddb4d42d4b9', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '6 Human Evaluation', 'sequence': 27}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw=\"'I found that the AI articles had more depth compared to the Wikipedia articles'. STORM also outperforms the best baseline in pairwise comparison.\\n\\nMore information in |R| poses challenges beyond factual hallucination. We examine 14 pairwise comparison responses where editors prefer oRAG outputs over STORM. Excluding 3 cases where pairwise preferences do not align with their ratings, editors assign lower Verifiability scores to articles from our approach in over 50% of the cases. Through analyzing the articles and editors' freeform feedback, we discover that low Verifiability scores stem from red herring fallacy or overspeculation issues . These arise when the generated articles introduce unverifiable connections between different pieces of information in |R| or between the information and the topic (examples included in Table 11). Compared to the widely discussed factual hallucination (Shuster et al., 2021; Huang et al., 2023), addressing such verifiability issues is more nuanced, surpassing basic fact-checking (Min et al., 2023).\\n\\nGenerated articles trail behind well-revised human works. While STORM outperforms the oRAGbaseline, editors comment that the generated articles are less informative than actual Wikipedia pages . Another major issue identified is the transfer of bias and tone from Internet sources to the generated article , with 7 out of 10 editors mentioning that the STORM-generated articles sound 'emotional' or 'unneutral'. More analysis is discussed in Appendix E. This feedback suggests that reducing the retrieval bias in the pre-writing stage is a worthwhile direction for future work.\\n\\nGenerated articles are a good starting point. As shown in Figure 3, editors are unanimous in agreeing that STORM can aid them in their pre-writing stage. It is gratifying to know that the tool is helpful to experienced editors. 80% of the editors think that STORM can help them edit a Wikipedia article for a new topic. More reservation is expressed to the usefulness of STORM for the Wikipedia community at large; nonetheless, 70% of the editors think it is useful, with only 10% disagreeing.\\n\", raw_embedding=array([-0.03314209, -0.01425934, -0.01118469, ..., -0.00063467,\n",
       "        -0.01386261, -0.00163746], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='43ab55e0-944f-4254-879b-a65f14a9afc0', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'E Error Analysis', 'sequence': 60}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw=\"E Error Analysis\\n\\nWhile articles produced by STORM are preferred by both automatic metrics and human evaluation, experienced editors still identified multiple problems with the machine-generated articles. We analyze the free-form comments and summarize the major issues in Table 11.\\n\\nThe primary issue raised is that the generated articles often contain emotional language and lack neutrality, primarily due to the source material. STORM currently retrieves grounding sources from the Internet which is not neutral and contains considerable promotional content on its own. Addressing this bias in the pre-writing stage represents a valuable direction for future research. Another major issue is the red herring fallacy or the over-association of unrelated facts. Addressing this challenge calls for high-level sensemaking rather than mere fact-level verification.\\n\\n- 1: Not engaging at all; no attempt to capture the reader's attention.\\n- 2: Slightly engaging with rare moments that capture attention.\\n- 3: Fairly engaging with a basic narrative but lacking depth.\\n- 4: Moderately engaging with several interesting points.\\n- 5: Quite engaging with a well-structured narrative and noteworthy points that frequently capture and retain attention.\\n- 6: Very engaging with a compelling narrative that captures and mostly retains attention.\\n- 7: Exceptionally engaging throughout, with a compelling narrative that consistently stimulates interest.\\n\", raw_embedding=array([-0.03305054, -0.02880859, -0.01280212, ...,  0.02554321,\n",
       "        -0.01570129,  0.00254059], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='49811882-1f89-4af8-8a88-df6816b008a2', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C Automatic Evaluation Details', 'sequence': 43}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 789699), raw='expert to get information. Ask good questions to get more useful information. 24 When you have no more question to ask, say \"Thank you so much for your help!\" to end the conversation. 25 Please only ask one question at a time and don\\'t ask what you have asked before. Your questions should be related to the topic you want to write. 26 \"\"\" 27 28 topic = dspy.InputField(prefix=\\'Topic you want to write: \\', format=str) 29 persona = dspy.InputField(prefix=\\'Your specific perspective: \\', format=str) 30 conv = dspy.InputField(prefix=\\'Conversation history:\\\\n\\', format=str) 31 question = dspy.OutputField() 32 33 class GenQueriesPrompt(dspy.Signature): 34 \"\"\" 35 You want to answer the question using Google search. What do you type in the search box? 36 Write the queries you will use in the following format:query 1\\\\nquery 2\\\\n... 37 \"\"\" 38 39 topic = dspy.InputField(prefix=\\'Topic you are discussing about: \\', format=str) 40 question = dspy.InputField(prefix=\\'Question you want to answer: \\', format=str) 41 queries = dspy.OutputField()\\n```\\n\\nListing 1: Prompts used in STORM, corresponding to Line 4, 11, 19, 22 in Algorithm 1.\\n\\n```\\n1 class GenAnswerPrompt(dspy.Signature): 2 \"\"\" 3 You are an expert who can use information effectively. You are chatting with a Wikipedia writer who wants to write a Wikipedia page on topic you know. You have gathered the related information and will now use the information to form a response. 4 Make your response as informative as possible and make sure every sentence is supported by the gathered information. 5 \"\"\" 6 7 topic = dspy.InputField(prefix=\\'Topic you are discussing about:\\', format=str) 8 conv = dspy.InputField(prefix=\\'Question:\\\\n\\', format=str) 9 info = dspy.InputField( 10 prefix=\\'Gathered information:\\\\n\\', format=str) 11 answer = dspy.OutputField(prefix=\\'Now give your response:\\\\n\\') 12 13 14 class DirectGenOutlinePrompt(dspy.Signature): 15 \"\"\" 16 Write an outline for a Wikipedia page. 17 Here is the format of your writing: 18 1. Use \"#\" Title\" to indicate section title , \"##\" Title\" to indicate subsection title , \"###\" Title\" to indicate subsubsection title , and so on. 19 2. Do not include other information. 20 \"\"\" 21 22 topic = dspy.InputField(prefix=\"Topic you want to write: \", format=str) 23 outline = dspy.OutputField(prefix=\"Write the Wikipedia page outline:\\\\n\") 24 25 26 class RefineOutlinePrompt(dspy.Signature): 27 \"\"\" 28 Improve an outline for a Wikipedia page. You already have a draft outline that covers the general information. Now you want to improve it based on the information learned from an information -seeking conversation to make it more comprehensive. 29 Here is the format of your writing: 30 1. Use \"#\" Title\" to indicate section title , \"##\" Title\" to indicate subsection title , \"###\" Title\" to indicate subsubsection title , and so on. 31 2. Do not include other information. 32 \"\"\" 33 34 topic = dspy.InputField(prefix=\"Topic you want to write: \", format=str) 35 conv = dspy.InputField(prefix=\"Conversation history:\\\\n\", format=str) 36 old_outline = dspy.OutputField(prefix=\"Current outline:\\\\n\", format=str) 37 outline = dspy.OutputField( 38 prefix=\\'Write the Wikipedia page outline:\\\\n\\')\\n```\\n\\nListing 2: Prompts used in STORM (continue), corresponding to Line 24, 31, 32 in Algorithm 1.\\n', raw_embedding=array([-5.8650970e-05, -1.4467239e-03,  6.6490173e-03, ...,\n",
       "        -4.1442871e-02,  2.6464462e-04,  2.4124146e-02],\n",
       "       shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='4b53fafb-2bce-45e8-9113-9174e1d372b5', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.4 STORM Implementation', 'sequence': 21}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 787707), raw='                |\\n|         | w/o Perspective  | 84.49                 | 40.12                   |\\n|         | w/o Conversation | 77.97                 | 31.98                   |\\n| GPT-4   | Direct Gen       | 87.66                 | 34.78                   |\\n| GPT-4   | RAG/oRAG         | 89.55                 | 42.38                   |\\n|         | RAG-expand       | 91.36                 | 43.53                   |\\n|         | STORM            | 92.73 †               | 45.91                   |\\n|         | w/o Perspective  | 92.39                 | 42.70                   |\\n|         | w/o Conversation | 88.75                 | 39.30                   |\\n\\nTable 3: Results of outline quality evaluation (%). † denotes significant differences ( p &lt; 0 . 05 ) from a paired t -test between STORM and baselines.\\n\\nin STORM are both set as 5. We use the chat model gpt-3.5-turbo for question asking and use gpt-3.5-turbo-instruct for other parts of STORM. We also experiment with using gpt-4 for drafting and refining the outline (Figure 2', raw_embedding=array([-0.02436829, -0.02220154, -0.0158844 , ..., -0.00459671,\n",
       "         0.03872681, -0.05023193], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='5304f846-1cbe-47ac-86ad-7d75da76618f', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '# Tragic Passing', 'sequence': 85}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 792700), raw='# Tragic Passing\\n\\nTaylor Hawkins, the esteemed drummer of the alt-rock band Foo Fighters, passed away suddenly on March 25, 2022, while on tour with his band in Bogotá, Colombia[34]. The official cause of death was cardiac arrest, though inquiries were raised concerning the presence of drugs in his system and their potential contribution to his death[33][34]. On the night of his passing, paramedics were called to the Four Seasons hotel in Bogotá due to reports of chest pain from an unnamed guest, later revealed to be Hawkins[34]. Unfortunately, resuscitation efforts were unsuccessful, and Hawkins was declared dead at the scene[34].\\n\\nThe news of Hawkins\\' sudden demise was announced on the morning of March 25th, 2022, which left the music world in shock[32]. The band confirmed the news with a short statement, expressing their devastation at the loss of Hawkins, whose \"musical spirit and infectious laughter\" would live on forever[32].\\n\\nAs a result of Hawkins\\' untimely passing, the band canceled their ongoing South American tour[33]. The festival stage at the Estéreo Picnic Festival, where the Foo Fighters were scheduled to perform that night, was transformed into a candlelight vigil in memory of Hawkins[33].\\n', raw_embedding=array([ 0.01065826, -0.03109741, -0.03833008, ...,  0.01695251,\n",
       "         0.01255798,  0.00990295], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='5f19caf0-9512-4b97-bcd8-4c755254400e', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '10 end', 'sequence': 45}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 789699), raw='10 end\\n\\n- 11 P ← gen\\\\_perspectives ( t , tocs)\\n- 12 P ← [P0] + P [: N ]\\n- 13 // Simulate conversations.\\n- 14 convos ← [ ]\\n- 15 foreach p in P do\\n\\n16\\n\\nconvo\\\\_history\\n\\n17\\n\\n18\\n\\n19\\n\\n20\\n\\n21\\n\\n22\\n\\n23\\n\\n24\\n\\n25\\n\\n26\\n\\n27\\n\\n←\\n\\n[ ]\\n\\nfor i = 1 to M do\\n\\n// Question asking.\\n\\nq ← gen\\\\_qn ( t , p , dlg\\\\_history) convo\\\\_history.append( q )\\n\\n//\\n\\nQuestion answering.\\n\\nqueries ← gen\\\\_queries ( t , q) sources ←\\n\\nsearch\\\\_and\\\\_sift (queries)\\n\\na\\n\\n←\\n\\ngen\\\\_ans\\n\\n(\\n\\nt\\n\\n, q, sources)\\n\\nconvo\\\\_history.append( a )\\n\\nR .append(sources)\\n', raw_embedding=array([-1.9210815e-02, -2.4734497e-02, -5.9604645e-05, ...,\n",
       "        -6.2164307e-02,  5.2429199e-02, -3.0242920e-02],\n",
       "       shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='63e49f18-e501-4b5f-87f7-328f662db292', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 49}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw='                                                                                             |\\n| Score 3 Description    | Moderately engaging with several interesting points.                                                                                                                                     |\\n| Score 4 Description    | Quite engaging with a well-structured narrative and noteworthy points that frequently capture and retain attention.                                                                      |\\n| Score 5 Description    | Exceptionally engaging throughout, with a compelling narrative that consistently stimulates interest.                                                                                    |\\n| Criteria Description   | Coherence and Organization : Is the article well-organized and logically structured?                                                  ', raw_embedding=array([-0.01939392, -0.01889038,  0.0034523 , ..., -0.0109024 ,\n",
       "         0.0034008 ,  0.01660156], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='67be0d39-7d46-4814-9057-1f66453ac9cd', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Acknowledgements', 'sequence': 31}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw='Acknowledgements\\n\\nWe thank You.com for generously providing the search API that supported our experiments. We also thank Sina J. Semnani, Shicheng Liu, Eric Zelikman for providing helpful feedback and the ACL ARR reviewers for their valuable comments. This work is supported in part by the Verdant Foundation and Microsoft Azure AI credits. Yijia Shao is supported by a Stanford School of Engineering Fellowship.\\n', raw_embedding=array([-0.07751465, -0.00616074, -0.01663208, ..., -0.00466156,\n",
       "        -0.01901245,  0.02490234], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='703ece8f-03c4-49bd-b49f-3a60858f2141', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '# Legacy and Impact', 'sequence': 79}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 792700), raw='# Legacy and Impact\\n\\nTaylor Hawkins was known for his raw and authentic drumming style, described as \"courageous, damaged and unflinchingly authentic\"[20]. His work with the Foo Fighters, as well as his various collaborations and side projects, made him a celebrated figure in rock \\'n\\' roll[10].\\n\\nHawkins\\' death in 2022 was met with heartfelt tributes from colleagues and fans around the world. Notable tributes came from rock legends like Roger Taylor of Queen, who considered Hawkins as a kind, brilliant man and an inspirational mentor, likening his death to \"losing a younger favourite brother\"[21]. Similarly, Led Zeppelin\\'s Jimmy Page admired his technique, energy and spirited enthusiasm[21].\\n\\nAn LA tribute concert held in his honor included guest drummers like Lars Ulrich of Metallica, Travis Barker of blink-182, and Brad Wilk of Rage Against the Machine. Singers like Miley Cyrus and Alanis Morissette also performed at the concert[22].\\n\\nApart from his music, Taylor Hawkins also contributed to charities Music Support and MusiCares, both of which were chosen by the Hawkins family[23]. He had received numerous accolades throughout his career, including 27 Grammy nominations, of which he won 14[2]. In 2021, the Foo Fighters were inducted into the Rock and Roll Hall of Fame[9].\\n', raw_embedding=array([-0.01132202, -0.00805664, -0.00754547, ...,  0.0246582 ,\n",
       "         0.03613281,  0.00728989], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='705ab5b3-1787-4e1c-8d2e-22157df15f1a', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '# Early Life and Background', 'sequence': 75}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 792700), raw='# Early Life and Background\\n\\nOliver Taylor Hawkins, known as Taylor Hawkins, was born and raised in Fort Walton, Texas[3]. His family moved to Laguna Beach, California when he was four years old[3]. He has two younger siblings, a brother named Jason, and a sister named Heather[3]. As a child, Hawkins was particularly influenced by his paternal grandmother, Josie Hawkins, who had grown up during the Great Depression and lived in Jackson, Mississippi[1].\\n\\nDuring his high school days at Laguna Beach High School, from where he graduated in 1990, he became friends with Jon Davison, who later became the lead vocalist of the band Yes[2][3]. His interest in music was nurtured from an early age, particularly after watching a Queen concert in 1982 which inspired him to learn to play the drums[2][5]. He noted that music was a constant presence in his family home[5].\\n\\nDespite facing certain hardships during his upbringing, including his mother\\'s struggles with \"demons\", Hawkins pursued his musical ambitions[4]. He credits his older sister Heather for taking care of the family during difficult times[4].\\n\\nHis first major musical experience came from playing drums for Alanis Morissette\\'s album, Jagged Little Pill, and accompanying her on the subsequent tour[3]. This marked the beginning of his professional career in the music industry.\\n', raw_embedding=array([ 0.02334595, -0.00484848, -0.04016113, ...,  0.04547119,\n",
       "         0.02775574, -0.01846313], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='7196022c-94e8-4de6-8955-2ff972569530', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'References', 'sequence': 35}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw='PMLR.\\n\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive NLP. arXiv preprint arXiv:2212.14024 .\\n\\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714 .\\n\\nSeungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. 2023. Prometheus: Inducing fine-grained evaluation capability in language models. arXiv preprint arXiv:2310.08491 .\\n\\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022. Internet-augmented dialogue generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8460-8478, Dublin, Ireland. Association for Computational Linguistics.\\n\\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. LongEval: Guidelines for human evaluation of faithfulness in long-form summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics , pages 1650-1669, Dubrovnik, Croatia. Association for Computational Linguistics.\\n\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459-9474.\\n\\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023. Unified demonstration retriever for incontext learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 4644-4668, Toronto, Canada. Association for Computational Linguistics.\\n\\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out , pages 74-81, Barcelona, Spain. Association for Computational Linguistics.\\n\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures , pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics.\\n\\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations .\\n\\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy CampbellGillingham, Geoffrey Irving, and Nat McAleese. 2022. Teaching language models to support answers with verified quotes.\\n\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 12076-12100, Singapore. Association for Computational Linguistics.\\n\\nJulià Minguillón, Maura Lerga, Eduard Aibar, Josep Lladós-Masllorens, and Antoni Meseguer-Artola. 2017. Semi-automatic generation of a corpus of wikipedia articles on science and technology. Profesional de la Información , 26(5):995-1005.\\n\\nRosa Munoz-Luna. 2015. Main ingredients', raw_embedding=array([-0.04751587, -0.0269165 , -0.00187874, ..., -0.00575256,\n",
       "         0.03213501,  0.00166225], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='720c667f-325f-494a-b9d2-a8bfbedc4554', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 50}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw='                                                                                   |\\n| Criteria Description   | Coherence and Organization : Is the article well-organized and logically structured?                                                                                                     |\\n| Score 1 Description    | Disorganized; lacks logical structure and coherence.                                                                                                                                     |\\n| Score 2 Description    | Fairly organized; a basic structure is present but not consistently followed.                                                                                                            |\\n| Score 3 Description    | Organized; a clear structure is mostly followed with some lapses in coherence.        ', raw_embedding=array([-1.0246277e-02, -1.0383606e-02, -6.1798096e-03, ...,\n",
       "        -9.0777874e-05,  3.8940430e-02, -3.0212402e-03],\n",
       "       shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='733866f5-184d-4443-802d-5cb212c0d6e8', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '### Red Light Fever', 'sequence': 82}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 792700), raw=\"### Red Light Fever\\n\\nRed Light Fever, released on April 19, 2010, was the band's first album[29][30]. Prior to its release, Hawkins revealed in an interview that the album had completed the recording and production stages, but its title and release date were yet to be determined[29]. Red Light Fever was recorded at the Foo Fighters' Studio 606 in California and featured guest musicians such as Brian May and Roger Taylor of Queen, Dave Grohl of Foo Fighters, and Elliot Easton of The Cars[29][30].\\n\", raw_embedding=array([ 0.00566483, -0.04571533, -0.05828857, ...,  0.03283691,\n",
       "         0.04101562,  0.02946472], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='756070a3-106a-4bec-af50-ecfa9dc8ad3d', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Relevance and Focus', 'sequence': 62}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw='Relevance and Focus\\n\\n- 1: Off-topic; the content does not align with the headline or core subject.\\n- 2: Mostly off-topic with some relevant points.\\n- 3: Somewhat on topic but with several digressions; the core subject is evident but not consistently adhered to.\\n- 4: Generally on topic, despite a few unrelated details.\\n- 5: Mostly on topic and focused; the narrative has a consistent relevance to the core subject with infrequent digressions.\\n- 6: Highly relevant with a focused narrative and purpose.\\n- 7: Exceptionally focused and entirely on topic; the article is tightly centered on the subject, with every piece of information contributing to a comprehensive understanding of the topic.\\n', raw_embedding=array([ 0.00170612,  0.00014055,  0.02284241, ..., -0.0224762 ,\n",
       "         0.02807617, -0.06045532], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='7a580128-5802-49bd-83cb-3df62c581a50', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C Automatic Evaluation Details', 'sequence': 42}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 789699), raw='C Automatic Evaluation Details\\n\\nC.1 Soft Heading Recall\\n\\nWe calculate the soft heading recall between the multi-level headings in the generated outline, considered as the prediction P , and those in the humanwritten article, considered as the ground truth G . The calculation is based on the soft recall definition in Fränti and Mariescu-Istodor (2023). Given a set A = { Ai } K i =1 , soft count of an item is defined as the inverse of the sum of its similarity to other items in the set:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere embed( · ) in Equation (1) is parameterized by paraphrase-MiniLM-L6-v2 provided in the Sentence-Transformers library 13 . The cardinality\\n\\n13 https://huggingface.co/sentence-transformers/ paraphrase-MiniLM-L6-v2\\n\\n```\\n1 class GenRelatedTopicsPrompt(dspy.Signature): 2 \"\"\" 3 I\\'m writing a Wikipedia page for a topic mentioned below. Please identify and recommend some Wikipedia pages on closely related subjects. I\\'m looking for examples that provide insights into interesting aspects commonly associated with this topic , or examples that help me understand the typical content and structure included in Wikipedia pages for similar topics. 4 Please list the urls in separate lines. 5 \"\"\" 6 7 topic = dspy.InputField(prefix=\"Topic of interest:\", format=str) 8 related_topics = dspy.OutputField() 9 10 class GenPerspectivesPrompt(dspy.Signature): 11 \"\"\" 12 You need to select a group of Wikipedia editors who will work together to create a comprehensive article on the topic. Each of them represents a different perspective , role , or affiliation related to this topic. You can use other Wikipedia pages of related topics for inspiration. For each editor , add description of what they will focus on. 13 Give your answer in the following format: 1. short summary of editor 1: description\\\\n2. short summary of editor 2: description\\\\n... 14 \"\"\" 15 16 topic = dspy.InputField(prefix=\\'Topic of interest:\\', format=str) 17 examples = dspy.InputField(prefix=\\'Wiki page outlines of related topics for inspiration:\\\\n\\', format=str) 18 perspectives = dspy.OutputField() 19 20 class GenQnPrompt(dspy.Signature): 21 \"\"\" 22 You are an experienced Wikipedia writer and want to edit a specific page. Besides your identity as a Wikipedia writer , you have a specific focus when researching the topic. 23 Now, you are chatting with an expert to get information. Ask good questions to get more useful information. 24 When you have no more question to ask, say \"Thank you so much for your help!\" to end the conversation. 25 Please only ask one question at a time and don\\'t ask what you have asked before. Your questions should be related to the topic you want to write. 26 \"\"\" 27 28 topic = dspy.InputField(prefix=\\'Topic you want to write: \\', format=str) 29 persona = dspy.InputField(prefix=\\'Your specific perspective: \\', format=str) 30 conv = dspy.InputField(prefix=\\'Conversation history:\\\\n\\', format=str) 31 question = dspy.OutputField() 32 33 class GenQueriesPrompt(dspy.Signature): 34 \"\"\" 35 You want to answer the question using Google search. What do you type in the search box? 36 Write the queries you will use in the following format:query 1\\\\nquery 2\\\\n... 37 \"\"\" 38 39 topic = dspy.InputField(prefix=\\'Topic you are discussing about: \\', format=str) 40 question = dspy.InputField(prefix=\\'Question you want to', raw_embedding=array([-0.03045654, -0.01221466, -0.01759338, ...,  0.00225449,\n",
       "        -0.00214958,  0.03890991], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='7c0419d1-5b0a-43d9-acfc-4870964fc99e', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2 FreshWiki', 'sequence': 5}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 786707), raw=\"2 FreshWiki\\n\\nWe study generating Wikipedia-like articles from scratch, placing emphasis on the pre-writing stage (Rohman, 1965), which involves the demanding sub-tasks of gathering and curating relevant information ('research'). This models the human\\n\\n1 Our resources and code are released at https://github. com/stanford-oval/storm .\\n\\nTable 1: Comparison of different Wikipedia generation setups in existing literature. Generating one paragraph does not need an article outline.\\n\\n|                            | Domain   | Scope        | Given Outline?   | Given Refs?   |\\n|----------------------------|----------|--------------|------------------|---------------|\\n| Balepur et al. (2023)      | One      | One para.    | /                | Yes           |\\n| Qian et al. (2023)         | All      | One para.    | /                | No            |\\n| Fan and Gardent (2022)     | One      | Full article | Yes              | No            |\\n| Liu et al. (2018)          | All      | One para.    | /                | Yes           |\\n| Sauper and Barzilay (2009) | Two      | Full article | No               | No            |\\n| Ours                       | All      | Full article | No               | No            |\\n\\nwriting approach which has prompted some educators to view Wikipedia article writing as an educational exercise for academic training (Tardy, 2010).\\n\\nTable 1 compares our work against prior benchmarks for Wikipedia generation. Existing work has generally focused on evaluating the generation of shorter snippets ( e.g. , one paragraph), within a narrower scope ( e.g. , a specific domain or two), or when an explicit outline or reference documents are supplied. A\", raw_embedding=array([-0.00113964,  0.00158501, -0.04284668, ...,  0.03170776,\n",
       "         0.0015192 ,  0.01742554], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='7ee80245-99fb-4231-accb-2fbb0b3ca02e', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.4 STORM Implementation', 'sequence': 17}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 787707), raw='4.4 STORM Implementation\\n\\nWe build STORM with zero-shot prompting using the DSPy framework (Khattab et al., 2023). Appendix B includes the pseudo code and corresponding prompts. The hyperparameters N and M\\n\\n9 https://en.wikipedia.org/wiki/Wikipedia: Good\\\\_article\\\\_criteria\\n\\n|                   | Comparsion with Human-written Articles   | Comparsion with Human-written Articles   | Comparsion with Human-written Articles   | Rubric Grading   | Rubric Grading   | Rubric Grading   | Rubric Grading   |\\n|-------------------|------------------------------------------|------------------------------------------|------------------------------------------|------------------|------------------|------------------|------------------|\\n|                   | ROUGE-1                                  | ROUGE-L                                  | Entity Recall                            | Interest Level   | Organization     | Relevance        | Coverage         |\\n| Direct Gen        | 25.62                                    | 12.63                                    | 5.08                                     | 2.87             | 4.60             | 3.10             | 4.16             |\\n| RAG               | 28.52                                    | 13.18                           ', raw_embedding=array([-0.02685547,  0.0333252 , -0.00766373, ..., -0.00791931,\n",
       "        -0.01046753,  0.00085163], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='822a62cd-8988-468d-b018-9ccd47cb7b08', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4 Experiments', 'sequence': 14}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 786707), raw='4 Experiments\\n\\n4.1 Article Selection\\n\\nSTORM is capable of researching complicated topics and writing long articles from detailed outlines. However, in this controlled experiment, we limit the final output to at most 4000 tokens (roughly 3000 words). For a meaningful comparison, we randomly select 100 samples from the FreshWiki dataset (see §2.1) that have human-written articles not exceeding 3000 words.\\n', raw_embedding=array([ 0.00655746,  0.00632095,  0.02082825, ..., -0.02149963,\n",
       "        -0.03662109,  0.02368164], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='829cae64-265f-46aa-9f46-3e9b4cf9c188', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Taylor Hawkins', 'sequence': 72}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw=\"Taylor Hawkins\\n\\nOliver Taylor Hawkins (1972-2022) was an American musician, best known as the drummer for the rock band Foo Fighters[1]. Born in Fort Walton, Texas, Hawkins' lovelor musicwas ignited at ayoung age, particularly after watching Queen concert in 1982[2][3][5]. He kick-started his professional Career the drummer for Alanis Morissette' band during her world tour for the hit album 'Jagged Little\\n\\nHawkins was celebrated for his versatile drumming style, drawing influence from renowned drummers like Taylor; Neil Peart, Phil Collins, Alex Van Halen, and Stewart Copeland[14]. His performances; marked by unique energy and aggressive style of play; earned him recognition as one of the top rock drummers ofhis era(15] Apart from his role in the Foo Fighters, Hawkins' passion for music saw him involved in numerous side projects and collaborations; cementing his place in the world of rock music[10] Roger\\n\\nOutside of his professional life, Hawkins was known for his resilience and dedication to his family. Despite personal struggles, including near-fatal \\\\_ overdose in 2001, Hawkins remained committed industry[13] drug\\n\\nHawkins' sudden death in 2022 while on tour in Bogotá, Colombia, sent shockwaves through the music world[34]. Tributes poured in from around the globe, reflecting the respect and admiration Hawkins had garnered his lifetime[21][31]. His life and career honored at star-studded tribute concert in Los les, attesting the enduring impact of his music[22] during Were Ange\\n\", raw_embedding=array([ 0.00394821, -0.03161621, -0.05279541, ...,  0.03063965,\n",
       "         0.02081299,  0.00469971], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='8376a72f-b7f5-4241-ab16-e068e951d864', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.2 Automatic Metrics', 'sequence': 15}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 786707), raw='4.2 Automatic Metrics\\n\\nAs discussed in §2.2, we evaluate the outline quality to assess the pre-writing stage by calculating the heading soft recall and heading entity recall . A higher recall score signifies a more comprehensive outline relative to the human-written article.\\n\\nTo assess the full-length article quality, we adopt ROUGE scores (Lin, 2004) and compute the entity recall in the article level based on FLAIR NER results. Moreover, based on Wikipedia criteria 9 , we evaluate the article from the aspects of (1) Interest Level , (2) Coherence and Organization , (3) Relevance and Focus , (4) Coverage , and (5) Verifiability . For aspects (1)-(4), we use Prometheus (Kim et al., 2023), a 13B evaluator LLM to score the article based on a 5-point rubric collaboratively developed with two experienced Wikipedia editors (see Appendix C.2). For verifiability, we calculate the citation recall and citation precision based on the definition in Gao et al. (2023). We use Mistral 7BInstruct (Jiang et al., 2023a) to examine whether the cited passages entail the generated sentence.\\n', raw_embedding=array([-0.02102661, -0.01681519,  0.00219345, ..., -0.00449753,\n",
       "         0.00170612,  0.0071907 ], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='847b570f-4996-468d-9726-93f1cc002325', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '3.2 Simulating Conversations', 'sequence': 11}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 786707), raw=\"3.2 Simulating Conversations\\n\\nThe theory of questions and question asking (Ram, 1991) highlights that while answers to existing questions contribute to a more comprehensive understanding of a topic, they often simultaneously give rise to new questions. To kick off this dynamic process, STORM simulates a conversation between a Wikipedia writer and a topic expert. In the i -th round of the conversation, the LLM-powered Wikipedia writer generates a single question q i based on the topic t , its assigned perspective p ∈ P , and the conversation history { q 1 , a 1 , ..., q i -1 , a i -1 } where a j denotes the simulated expert's answer. The conversation history enables the LLM to update its understanding of the topic and ask follow-up questions. In practice, we limit the conversation to at most M rounds.\\n\\nTo ensure that the conversation history provides factual information, we use trusted sources from the Internet to ground the answer a i to each query q i . Since q i can be complicated, we first prompt the LLM to break down q i into a set of search queries (Figure 2 4 ) and the searched results will be evaluated using a rule-based filter according to\\n\\nthe Wikipedia guideline 8 to exclude untrustworthy sources (Figure 2 5 ). Finally, the LLM synthesizes the trustworthy sources to generate the answer a i , and these sources will also be added to R for full article generation (§3.4).\\n\", raw_embedding=array([-0.02983093,  0.00745392,  0.00804138, ..., -0.01041412,\n",
       "         0.04107666, -0.0272522 ], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='88940039-fee5-407d-8902-ac962e6de175', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.4 STORM Implementation', 'sequence': 19}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 787707), raw='                            | 3.90             | 4.79             | 4.09             | 4.70             |\\n| STORM             | 45.82                                    | 16.70                                    | 14.10 †                                  | 3.99 †           | 4.82             | 4.45 †           | 4.88 †           |\\n| w/o Outline Stage | 26.77                                    | 12.77                                    | 7.39                                     | 3.33             | 4.87             | 3.35             | 4.37             |\\n\\nTable 2: Results of automatic article quality evaluation. † denotes significant differences ( p &lt; 0 . 05 ) from a paired t -test between STORM and the best baseline, i.e. , oRAG. The rubric grading uses a 1-5 scale.\\n\\n|         |                  |', raw_embedding=array([-0.02026367,  0.01174927, -0.02503967, ..., -0.02423096,\n",
       "         0.01422119, -0.00982666], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='8cc49292-0ee5-4320-80fe-94804843a564', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '6 Human Evaluation', 'sequence': 25}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 787707), raw=\"6 Human Evaluation\\n\\nTo better understand the strengths and weaknesses of STORM, we conduct human evaluation by collaborating with 10 experienced Wikipedia editors\\n\\n|                | oRAG   | oRAG      | STORM   | STORM     | p -value   |\\n|----------------|--------|-----------|---------|-----------|------------|\\n|                | Avg.   | ≥ 4 Rates | Av.g.   | ≥ 4 Rates |            |\\n| Interest Level | 3.63   | 57.5%     | 4.03    | 70.0%     | 0.077      |\\n| Organization   | 3.25   | 45.0%     | 4.00    | 70.0%     | 0.005      |\\n| Relevance      | 3.93   | 62.5%     | 4.15    | 65.0%     | 0.347      |\\n| Coverage       | 3.58   | 57.5%     | 4.00    | 67.5%     | 0.084      |\\n| Verifiability  | 3.85   | 67.5%     | 3.80    | 67.5%     | 0.843      |\\n| #Preferred     |        | 14        |         | 26        |            |\\n\\nTable 6: Human evaluation results on 20 pairs of articles generated by STORM and oRAG . Each pair of articles is evaluated by two Wikipedia editors. The ratings are given on a scale between 1 and 7, with values ≥ 4 indicating good quality (see Table 10). We conduct paired t -test and report the p -value.\\n\\nwho have made at least 500 edits on Wikipedia and have more than 1 year of experience. We randomly sample 20 topics from our dataset and evaluate the articles generated by our method and oRAG , the best baseline according to the automatic evaluation. Each pair of articles is assigned to 2 editors.\\n\\nWe request editors to judge each article from the same five aspects defined in §4.2, but using a 1 to 7 scale for more fine-grained evaluation. While our automatic evaluation uses citation quality as a proxy to evaluate Verifiability , we stick to the Wikipedia standard of 'verifiable with no original research' in human evaluation. Besides rating the articles, editors are asked to provide open-ended feedback and\", raw_embedding=array([ 1.1806488e-03, -9.0484619e-03, -5.5351257e-03, ...,\n",
       "         1.2580872e-02, -4.4036865e-02,  2.9563904e-05],\n",
       "       shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='8df97f14-258d-489f-a84e-187651a308c3', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Abstract', 'sequence': 2}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 785699), raw=\"Abstract\\n\\nWe study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM , a writing system for the S ynthesis of T opic O utlines through R etrieval and M ulti-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.\\n\\nFor evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from experienced Wikipedia editors. Compared to articles generated by an outlinedriven retrieval-augmented baseline, more of STORM's articles are deemed to be organized (by a 25% absolute increase) and broad in coverage (by 10%). The expert feedback also helps identify new challenges for generating grounded long articles, such as source bias transfer and over-association of unrelated facts.\\n\", raw_embedding=array([ 0.00972748, -0.00285721, -0.03041077, ..., -0.00402069,\n",
       "         0.00608826, -0.0168457 ], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='9122be4f-39a6-4067-a526-d19596a7774f', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2 FreshWiki', 'sequence': 6}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 786707), raw='   | No            |\\n| Ours                       | All      | Full article | No               | No            |\\n\\nwriting approach which has prompted some educators to view Wikipedia article writing as an educational exercise for academic training (Tardy, 2010).\\n\\nTable 1 compares our work against prior benchmarks for Wikipedia generation. Existing work has generally focused on evaluating the generation of shorter snippets ( e.g. , one paragraph), within a narrower scope ( e.g. , a specific domain or two), or when an explicit outline or reference documents are supplied. A notable example is WikiSum (Liu et al., 2018), which treats generating Wikipedia articles as a multi-document summarization problem, with respect to the reference documents.\\n\\nOur setup emphasizes the capability of longform grounded writing systems to research and curate content. Specifically, given a topic t , the task is to find a set of references R and generate a full-length article S = s 1 s 2 ...s n , where each sentence s i cites a list of documents in R . 2\\n', raw_embedding=array([-0.02328491,  0.00917816, -0.01597595, ...,  0.00784302,\n",
       "        -0.02090454,  0.02293396], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='91a5ed85-3fe0-45cb-b737-7b22b2f097d1', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '5.2 Ablation Studies', 'sequence': 24}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 787707), raw=\"5.2 Ablation Studies\\n\\nAs introduced in §3, STORM prompts LLMs to ask effective questions by discovering specific perspectives and simulating multi-turn conversations. We conduct the ablation study on outline creation by comparing STORM with two variants: (1) 'STORM w/o Perspective', which omits perspective in the question generation prompt; (2) 'STORMw/oConversation', which prompts LLMs to generate a set number of questions altogether. To ensure a fair comparison, we control an equal total number of generated questions across all variants. Table 3 shows the ablation results and full STORM pipeline produces outlines with the highest recall. Also, 'STORM w/o Conversation' gives much worse results, indicating reading relevant information is crucial to generating effective questions. We further examine how many unique sources are collected in R via different variants. As shown in Table 5, the full pipeline discovers more different sources and the trend is in accord with the automatic metrics for outline quality.\\n\\nWe also verify whether having an outline stage is necessary with STORM. In Table 2, 'STORM w/o Outline Stage' denotes the results of generating the entire article given the topic and the simulated conversations. Removing the outline stage significantly deteriorates the performance across all metrics.\\n\", raw_embedding=array([ 0.00309563, -0.0112915 , -0.02674866, ..., -0.01707458,\n",
       "         0.05126953, -0.03689575], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='96d48d30-889b-4e0e-9a76-fd9bdd7f96b6', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'References', 'sequence': 34}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw='biographies on Wikipedia: The impact of gender bias on the retrieval-based generation of women biographies. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8561-8576, Dublin, Ireland. Association for Computational Linguistics.\\n\\nXiaocheng Feng, Ming Liu, Jiahao Liu, Bing Qin, Yibo Sun, and Ting Liu. 2018. Topic-to-essay generation with neural networks. In IJCAI , pages 4078-4084.\\n\\nTira Nur Fitria. 2023. Artificial intelligence (ai) technology in openai chatgpt application: A review of chatgpt in writing english essay. In ELT Forum: Journal of English Language Teaching , volume 12, pages 44-58.\\n\\nPasi Fränti and Radu Mariescu-Istodor. 2023. Soft precision and recall. Pattern Recognition Letters , 167:115121.\\n\\n- R Edward Freeman, Jeffrey S Harrison, Andrew C Wicks, Bidhan L Parmar, and Simone De Colle. 2010. Stakeholder theory: The state of the art.\\n\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 6465-6488, Singapore. Association for Computational Linguistics.\\n\\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.\\n\\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research , 24(251):1-43.\\n\\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023a. Mistral 7b. arXiv preprint arXiv:2310.06825 .\\n\\nZhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023b. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 7969-7992, Singapore. Association for Computational Linguistics.\\n\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning , pages 15696-15707. PMLR.\\n\\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive NLP. arXiv preprint arXiv:2212.14024 .\\n\\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714 .\\n\\nSeungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. 2023. Prometheus: Inducing fine-grained evaluation capability in language models. arXiv preprint arXiv:2310.08491 .\\n\\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022. Internet-augmented dialogue generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8460-8478, Dublin, Ireland. Association for Computational Linguistics.\\n\\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi,', raw_embedding=array([-0.06036377, -0.0319519 , -0.00804901, ...,  0.03146362,\n",
       "         0.01437378,  0.01560211], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='997b5692-b389-413b-9656-9e8e9a7e7cbc', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 54}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw=\"                              |\\n| Score 5 Description    | Exceptionally focused and entirely on topic; the article is tightly centered on the subject, with every piece of information contributing to a comprehensive understanding of the topic. |\\n| Criteria Description   | Broad Coverage : Does the article provide an in-depth exploration of the topic and have good coverage?                                                                                   |\\n| Score 1 Description    | Severely lacking; offers little to no coverage of the topic's primary aspects, resulting in a very narrow perspective.                                                                   |\\n| Score 2 Description    | Partial coverage; includes some of the topic's main aspects but misses others, resulting in an incomplete portrayal.                                                                     |\\n| Score 3 Description    | Acceptable breadth; covers most main aspects, though it may stray into minor unnecessary details or overlook some relevant points.                                                       |\\n| Score 4 Description    | Good coverage; achieves broad coverage of the topic, hitting on all major points with minimal extraneous information.                                       \", raw_embedding=array([-0.04632568, -0.01290131, -0.01294708, ..., -0.04223633,\n",
       "        -0.01068115, -0.00723648], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='a1c65360-fb5d-4247-a151-c614cbff96e8', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 58}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw=\"4 reports the citation quality of articles produced by our approach, showing that around 15% sentences in generated articles are unsupported by citations. We further investigate the failure cases by randomly sampling 10 articles and an author manually examines all the unsupported sentences in these articles. Besides sentences that are incorrectly split 16 , lack citations, or are deemed supported by the author's judgment, our analysis identifies three main error categories (examples are given in Table 9): improper inferential linking , inaccurate paraphrasing , and citing irrelevant sources .\\n\\nWe show the error distribution in Figure 6. Notably, the most common errors stem from the tendency of LLMs to form improper inferential links between different pieces of information presented in the context window. Our analysis of citation quality suggests that, in addition to avoiding hallucinations, future research in grounded text generation should also focus on preventing LLMs from making overly inferential leaps based on the provided information.\\n\", raw_embedding=array([-0.07275391, -0.01200104, -0.0018425 , ..., -0.01820374,\n",
       "         0.03295898, -0.01029205], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='a4f72d25-e0ab-4618-a897-b65441839edd', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Interest Level', 'sequence': 70}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw='                                                                                                                                                                                                                                                            |\\n| Section organization problem                       | 5                | I do not like how the article is organized, with too many headers cluttering the article, making it not as readable. Other than that, the AI did great work on the piece. (comment on article 2022 Crimean Bridge explosion )                                                                                                                                                                    ', raw_embedding=array([ 0.01330566, -0.01934814, -0.02500916, ...,  0.02934265,\n",
       "        -0.00302505,  0.00897217], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='a597315b-800e-413f-8609-d20a84d8ee9f', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'References', 'sequence': 36}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw='Association for Computational Linguistics.\\n\\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations .\\n\\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy CampbellGillingham, Geoffrey Irving, and Nat McAleese. 2022. Teaching language models to support answers with verified quotes.\\n\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 12076-12100, Singapore. Association for Computational Linguistics.\\n\\nJulià Minguillón, Maura Lerga, Eduard Aibar, Josep Lladós-Masllorens, and Antoni Meseguer-Artola. 2017. Semi-automatic generation of a corpus of wikipedia articles on science and technology. Profesional de la Información , 26(5):995-1005.\\n\\nRosa Munoz-Luna. 2015. Main ingredients for success in l2 academic writing: Outlining, drafting and proofreading. PloS one , 10(6):e0128309.\\n\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022. Webgpt: Browserassisted question-answering with human feedback.\\n\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744.\\n\\nAaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm: Tool augmented language models.\\n\\nJohn V Pavlik. 2023. Collaborating with chatgpt: Considering the implications of generative artificial intelligence for journalism and media education. Journalism &amp; Mass Communication Educator , 78(1):84-93.\\n\\nGabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable code generation from pre-trained language models. In International Conference on Learning Representations .\\n\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 5687-5711, Singapore. Association for Computational Linguistics.\\n\\nPeng Qi, Yuhao Zhang, and Christopher D. Manning. 2020. Stay hungry, stay focused: Generating informative and specific questions in information-seeking conversations. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 2540, Online. Association for Computational Linguistics.\\n\\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu, Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus.\\n\\nHossein A. Rahmani, Xi Wang, Yue Feng, Qiang Zhang, Emine Yilmaz, and Aldo Lipani. 2023. A survey on asking clarification questions datasets in conversational systems. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2698-2716, Toronto, Canada. Association for Computational Linguistics.\\n\\nAshwin Ram. 1991. A theory of questions and question asking. Journal of the Learning Sciences , 1(3-4):273318.\\n\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,', raw_embedding=array([-0.06964111, -0.00740433, -0.0151062 , ...,  0.02920532,\n",
       "         0.01824951,  0.01425934], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='b0f4ead8-0b5f-43b0-8a57-fd7a23b31b0b', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.3 Baselines', 'sequence': 16}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 787707), raw='4.3 Baselines\\n\\nAs prior works use different setups and do not use LLMs, they are hard to compare directly. Instead, we use the following three LLM-based baselines.\\n\\n- 1. Direct Gen , a baseline that directly prompts the LLM to generate an outline, which is then used to generate the full-length article.\\n- 2. RAG , a retrieval-augmented generation baseline that searches with the topic and uses the searched results together with the topic t to generate an outline or the entire article.\\n- 3. Outline-driven RAG (oRAG) , which is identical to RAG in outline creation, but further searches additional information with section titles to generate the article section by section.\\n', raw_embedding=array([ 0.00847626,  0.01020813, -0.0519104 , ...,  0.02120972,\n",
       "         0.07775879, -0.05029297], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='b18c77b3-66fa-4e25-8241-3bc822d92b71', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '## Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models', 'sequence': 0}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 785699), raw='## Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models\\n', raw_embedding=array([-0.01737976, -0.01397705, -0.01728821, ...,  0.03369141,\n",
       "        -0.02104187,  0.03610229], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='b19094ba-c099-491c-9de7-37d148d74331', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2.1 The FreshWiki Dataset', 'sequence': 7}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 786707), raw='2.1 The FreshWiki Dataset\\n\\nCreating a new Wikipedia-like article demands not only fluent writing but also good research skills. As modern LLMs are generally trained on Wikipedia text, we mitigate data leakage by explicitly seeking out recent Wikipedia articles that were created (or very heavily edited) after the training cutoff of the LLMs we test. Our process can be repeated at future dates when new LLMs emerge.\\n\\nTo apply our date criteria, we focus on the top 100 most-edited pages, based on edit counts, for each month from February 2022 to September 2023 3 . To ensure high-quality references, we filter these articles to keep only those having B-class quality or above assessed by ORES 4 . We also ex-\\n\\n2 In practice, S also includes organizational elements such as section and subsection titles, which do not require citations.\\n\\n3 Obtained from https://wikimedia. org/api/rest\\\\_v1/metrics/edited-pages/ top-by-edits/en.wikipedia/all-editor-types/ content/ {year}/{month}/all-days\\n\\nclude list articles 5 and articles that have no subsections. While high-quality Wikipedia articles usually contain structured data ( e.g. , tables) and are multi-modal, we only consider the plain text component in constructing the dataset to simplify our task. More details of the dataset are in Appendix A.\\n', raw_embedding=array([-0.06088257, -0.02519226, -0.03753662, ...,  0.01763916,\n",
       "        -0.01482391,  0.01007843], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='b1b4b564-110f-4b8c-b2af-0b755df50745', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'D Human Evaluation Details', 'sequence': 59}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw=\"D Human Evaluation Details\\n\\nWe recruited 10 experienced Wikipedia editors to participate in our study by creating a research page on Meta-Wiki 17 and reaching out to active editors who have recently approved articles for Wikipedia. 18 Our participation group includes 3 editors with 1-5 years of experience, 4 with 6-10 years, and 3 with over 15 years of contribution. The study was approved by the Institutional Review Board of our institution and the participants signed the consent form through Qualtrics questionnaires before the study started.\\n\\nTo streamline the evaluation of grounded articles, we developed a web application, which features a side-by-side display of the article and its citation snippets, to gather ratings and open-ended feedback\\n\\n15 https://huggingface.co/mistralai/\\n\\nMistral-7B-Instruct-v0.1\\n\\n16 Following Gao et al. (2023), we check citation quality in the sentence level and split articles into sentences using NLTK sent\\\\_tokenize . sent\\\\_tokenize sometimes fails to split sentences correctly when the article contains special words like 'No.12847', 'Bhatia et al.', etc.\\n\\n17 https://meta.wikimedia.org\\n\\n18 Since evaluating Wikipedia-like articles is timeconsuming and requires expertise, we paid each participant 50$ for our study.\\n\\nfor each article. Figure 7 shows the screenshot of our web application and the full article produced by STORM is included in Table 12. For human evaluation, we use a 1 to 7 scale for more finegrained evaluation. The grading rubric is included in Table 10.\\n\\nWe collected the pairwise preferences and the perceived usefulness of STORM via an online questionnaire. Specifically, for the perceived usefulness, we request editors to rate their agreement with statements 'I think it can be specifically helpful for my pre-writing stage ( e.g. , collecting relevant sources, outlining, drafting).', 'I think it will help me edit a Wikipedia article for a new topic', 'I think it can be a potentially useful tool for the Wikipedia community' on a Likert scale of 1-5, corresponding to Strongly disagree , Somewhat disagree , Neither agree nor disagree , Somewhat agree , Strongly agree .\\n\", raw_embedding=array([-0.00603485, -0.00928497, -0.01708984, ...,  0.00564194,\n",
       "        -0.02235413,  0.00374794], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='bc17a979-7bef-4305-8160-9be3a39cf0ca', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'A Dataset Details', 'sequence': 40}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 789699), raw=\"A Dataset Details\\n\\nAs discussed in §2.1, we curate the FreshWiki dataset by collecting recent and high-quality English Wikipedia articles. We select the most-edited pages over a specific period rather than using creation dates as a cutoff because most of Wikipedia articles are 'stubs' or are of low quality when they were created. For quality, we consider articles predicted to be of B-class quality or above. According to Wikipedia statistics 12 , only around 3% of existing Wikipedia pages meet this quality standard. As LLMs can generate reasonably good outputs, we think it is important to use high-quality humanwritten articles as references for further research.\\n\\nFor experiments in this work, we randomly select 100 samples with human-written articles under 3000 words to have a meaningful comparison. Table 7 gives the data statistics. Notably, humanauthored articles have a large number of references but they require numerous edits to achieve this. Figure 4 illustrates the evolution of the reference count in the article edit process and Figure 5 gives the distribution of edit counts for human-authored articles used in our experiments.\\n\\n12 https://en.wikipedia.org/wiki/Wikipedia: Content\\\\_assessment\\n\\nFigure 5: Distribution of edit counts for Wikipedia articles in our experiments ( n = 100 ).\\n\\n<!-- image -->\\n\", raw_embedding=array([-0.06072998, -0.02783203, -0.026474  , ...,  0.009552  ,\n",
       "        -0.00738525,  0.00871277], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='bc7ab971-f18f-416e-918c-8ba04b701af5', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 52}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw='                                |\\n| Score 5 Description    | Excellently organized; the article is logically structured with seamless transitions and a clear argument.                                                                               |\\n| Criteria Description   | Relevance and Focus : Does the article stay on topic and maintain a clear focus?                                                                                                         |\\n| Score 1 Description    | Off-topic; the content does not align with the headline or core subject.                                                                                                                 |\\n| Score 2 Description    | Somewhat on topic but with several digressions; the core subject is evident but not consistently adhered to.                                                                             |\\n| Score 3 Description    | Generally on', raw_embedding=array([-0.01686096, -0.02345276,  0.00739288, ..., -0.03076172,\n",
       "         0.02648926, -0.02381897], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='c22ba83e-c64e-4d1d-8747-1a29dbe74024', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Early Life and Background', 'sequence': 73}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw='Early Life and Background\\n\\nOliver Taylor Hawkins, known as Taylor Hawkins, was born and raised in Fort Walton, Texas[3]. His family moved to Laguna Beach, California when he was four years old[3]. He has two younger siblings, brother named Jason;, and lived in Jackson, Mississippi[1]\\n\\nDuring his high school music was nurtured from an early age, particularly after watching a Queen concert in 1982 which inspired him to learn to play the drums[2][5]. He noted that music was constant presencein his family home[5] days\\n\\nFigure 7: Screenshot of the web application for evaluating the grounded article.\\n', raw_embedding=array([ 0.03384399, -0.0087204 , -0.05175781, ...,  0.0274353 ,\n",
       "         0.0519104 , -0.03540039], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='c2d05f57-b6dc-4133-98db-da7f1db34e67', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 55}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw='     |\\n| Score 3 Description    | Acceptable breadth; covers most main aspects, though it may stray into minor unnecessary details or overlook some relevant points.                                                       |\\n| Score 4 Description    | Good coverage; achieves broad coverage of the topic, hitting on all major points with minimal extraneous information.                                                                    |\\n| Score 5 Description    | Exemplary in breadth; delivers outstanding coverage, thoroughly detailing all crucial aspects of the topic without including irrelevant information.                                     |\\n\\nTable 9: Examples of different error types of unsupported sentences.\\n\\n| Error Type                   | Topic                         | Unsupported Sentence                                                                                                                                                         | Source                                               ', raw_embedding=array([-0.01741028, -0.00162697, -0.00553513, ..., -0.01890564,\n",
       "         0.00108433, -0.01374817], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='c4bbb858-3217-4044-98ab-049a5d28c9bc', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Interest Level', 'sequence': 68}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw=\"should be linked to. (comment on article 2022 AFL Grand Final ) 'One study, conducted by Sinéad Griffin, a physicist at the Lawrence Berkeley National Laboratory, provided some analysis of LK-99's abilities using supercomputer simulations[20].' This is not enough information about the analysis, which would have been very useful in the article. (comment on article LK-99 )                                                                                                                                                      |\\n| Improper handling of                               | 5                | (comment on article 2022 West Java earthquake ) Words like 'now' should be avoided in Wikipedia articles to prevent them from becoming dated and phrases such as, 'as of December 2023' should be used instead. (comment on article Cyclone Batsirai ) 'as of December 13' doesn't specify a year, and is old information                                                                                                                                                                                             \", raw_embedding=array([-0.0357666 ,  0.01311493, -0.00071621, ...,  0.00079918,\n",
       "         0.02053833, -0.01898193], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='c5f6e893-d910-4906-ba3c-11d1f5824c31', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'References', 'sequence': 39}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw='4393-4479, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations .\\n\\nCyril Zakka, Akash Chaurasia, Rohan Shad, Alex R Dalal, Jennifer L Kim, Michael Moor, Kevin Alexander, Euan Ashley, Jack Boyd, Kathleen Boyd, et al. 2023. Almanac: Retrieval-augmented language models for clinical medicine. Research Square .\\n\\nShuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, and Graham Neubig. 2023. Docprompting: Generating code by retrieving the docs. In The Eleventh International Conference on Learning Representations .\\n\\nTable 7: Statistics of the dataset used in our experiments.\\n\\n| Average Numer of Sections            |    8.4 |\\n|--------------------------------------|--------|\\n| Average Number of All-level Headings |   15.8 |\\n| Average Length of a Section          |  327.8 |\\n| Average Length of Total Article      | 2159.1 |\\n| Average Number of References         |   90.1 |\\n\\nFigure 4: Evolution of reference count in the Wikipedia article editing process.\\n\\n<!-- image -->\\n', raw_embedding=array([-0.03439331, -0.00010306, -0.02050781, ...,  0.03381348,\n",
       "         0.01800537,  0.0670166 ], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='c762555c-fd66-4af5-b016-958de1748497', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'B Pseudo Code of STORM', 'sequence': 41}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 789699), raw='B Pseudo Code of STORM\\n\\nIn §3, we introduce STORM, a framework that automates the pre-writing stage by discovering different perspectives, simulating information-seeking conversations, and creating a comprehensive outline. Algorithm 1 displays the skeleton of STORM.\\n\\nWe implement STORM with zero-shot prompting using the DSPy framework (Khattab et al., 2023). Listing 1 and 2 show the prompts used in our implementation. We highlight that STORM offers a general framework designed to assist the creation of grounded, long-form articles, without depending extensively on prompt engineering for a single domain.\\n', raw_embedding=array([-0.00764084,  0.01361847, -0.01593018, ..., -0.01559448,\n",
       "         0.01034546, -0.01670837], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='cde5eabe-28e7-498e-9b72-a3f9497de85a', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Yijia Shao Yucheng Jiang Theodore A. Kanell Peter Xu Omar Khattab Monica S. Lam', 'sequence': 1}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 785699), raw='Yijia Shao Yucheng Jiang Theodore A. Kanell Peter Xu Omar Khattab Monica S. Lam\\n\\nStanford University\\n\\n{shaoyj, yuchengj, tkanell, peterxu, okhattab}@stanford.edu lam@cs.stanford.edu\\n', raw_embedding=array([-0.05307007,  0.00985718, -0.03594971, ...,  0.00118256,\n",
       "        -0.00830841, -0.00843048], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='ce7683f0-b857-40c8-80d9-628c60a4cdf8', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 51}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw='   | Fairly organized; a basic structure is present but not consistently followed.                                                                                                            |\\n| Score 3 Description    | Organized; a clear structure is mostly followed with some lapses in coherence.                                                                                                           |\\n| Score 4 Description    | Good organization; a clear structure with minor lapses in coherence.                                                                                                                     |\\n| Score 5 Description    | Excellently organized; the article is logically structured with seamless transitions and a clear argument.                                                                               |\\n| Criteria Description   | Relevance and Focus : Does the article stay on topic and maintain', raw_embedding=array([-0.00741577, -0.01104736, -0.01377106, ..., -0.01036072,\n",
       "         0.00913239, -0.00481796], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='d2e47184-652f-47d6-8ffb-f471aa59313e', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Coherence and Organization', 'sequence': 61}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 790700), raw='Coherence and Organization\\n\\n- 1: Disorganized; lacks logical structure and coherence.\\n- 2: Poor organization; some structure is evident but very weak.\\n- 3: Fairly organized; a basic structure is present but not consistently followed.\\n- 4: Organized; a clear structure is mostly followed with some lapses in coherence.\\n- 5: Good organization; a clear structure with minor lapses in coherence.\\n- 6: Very well-organized; a logical structure with transitions that effectively guide the reader.\\n- 7: Excellently organized; the article is logically structured with seamless transitions and a clear argument.\\n', raw_embedding=array([ 0.0171814 ,  0.00887299, -0.00395584, ...,  0.00405884,\n",
       "         0.00509644, -0.03216553], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='d547f69b-1067-4dc6-ad2e-c949b1683b8d', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Ethics Statement', 'sequence': 32}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw='Ethics Statement\\n\\nDifferent from the creative generation, grounded article generation may impact how people learn about topics or consume source information. All the studies and the evaluation in this work are designed to prevent the dissemination of misinformation by not publishing generated content online and implementing strict accuracy checks. We avoid any disruption to Wikipedia or related communities, as our system does not interact with live pages. Also, although we try to generate grounded articles, we believe there is no privacy issue related to this work as we only use information publicly available on the Internet.\\n\\nThe primary risk of our work is that the Wikipedia articles written by our system are grounded on information on the Internet which contains some biased or discriminative content on its own. Currently, our system relies on the search engine to retrieve information but does not include any post-processing module. We believe improving the retrieval module to have good coverage of different viewpoints and adding a content sifting module to the current system will be a critical next step to achieve better neutrality and balance in the generated articles.\\n\\nAnother limitation we see from an ethical point of view is that we only consider writing English Wikipedia articles in this work. Extending the current system to a multilingual setup is a meaningful direction for future work as more topics do not have Wikipedia pages in non-English languages.\\n', raw_embedding=array([-0.01670837, -0.034729  , -0.00750732, ...,  0.00522614,\n",
       "        -0.00676727,  0.00396347], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='deb7b9ea-0eea-4905-8acf-1f60885421ac', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.4 STORM Implementation', 'sequence': 22}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 787707), raw=\"               | 42.70                   |\\n|         | w/o Conversation | 88.75                 | 39.30                   |\\n\\nTable 3: Results of outline quality evaluation (%). † denotes significant differences ( p &lt; 0 . 05 ) from a paired t -test between STORM and baselines.\\n\\nin STORM are both set as 5. We use the chat model gpt-3.5-turbo for question asking and use gpt-3.5-turbo-instruct for other parts of STORM. We also experiment with using gpt-4 for drafting and refining the outline (Figure 2 7 - 8 ). For reported results, the simulated topic expert in STORM is grounded on the You.com search API 10 , although the proposed pipeline is compatible with other search engines. The ground truth Wikipedia article is excluded from the search results.\\n\\nhigh heading soft recall, indicating LLMs' ability to grasp high-level aspects of a topic through their rich parametric knowledge. However, STORM, by asking effective questions to research the topic, can create higher recall outlines that cover more topicspecific aspects. Notably, although RAG leverages additional information, presenting unorganized information in the context window makes outline generation more challenging for the weaker model, i.e. , GPT-3.5, leading to worse performance. To test the limit of the RAG baseline, we further expand the retrieved sources by starting with the outline produced by RAG , using its section titles as search queries to collect more sources, and inputting the newly collected sources together with the initial outline to LLM to generate a polished outline. This modified approach is referred to as ' RAG-expand ' in Table 3. The experiment results indicate that even though having an additional round of search and refinement can improve the outline produced by RAG , our proposed STORM still surpasses its performance.\\n\\nFor final article generation, we only report the results using gpt-4 as gpt-3.5 is not faithful to sources when generating text with citations (Gao et al., 2023). We set temperature as 1.0 and top\\\\_p as 0.9 for all experiments.\\n\", raw_embedding=array([-0.01623535, -0.03103638, -0.0226593 , ..., -0.00823212,\n",
       "         0.03811646, -0.03900146], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='ded1eb8f-d182-4f02-91ba-5c344a812055', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '3.4 Writing the Full-Length Article', 'sequence': 13}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 786707), raw=\"3.4 Writing the Full-Length Article\\n\\nBuilding upon the references R collected and the outline O developed during the pre-writing stage, the full-length article can be composed section by section. Since it is usually impossible to fit the entire R within the context window of the LLM, we use the section title and headings of its all-level subsections to retrieve relevant documents from R based on semantic similarity calculated from Sentence-BERT embeddings. With the relevant information at hand, the LLM is then prompted to generate the section with citations. Once all sections are generated, they are concatenated to form the full-length article. Since the sections are generated in parallel, we prompt the LLM with the concatenated article to delete repeated information to improve coherence. Furthermore, in alignment with Wikipedia's stylistic norms, the LLM is also utilized to synthesize a summary of the entire article, forming the lead section at the beginning.\\n\", raw_embedding=array([-0.00957489,  0.02908325, -0.01067352, ...,  0.03808594,\n",
       "        -0.01869202,  0.03216553], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='e31326c6-cb11-4ab6-a213-5578b2246ea3', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '# Collaborations and Guest Appearances', 'sequence': 84}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 792700), raw=\"# Collaborations and Guest Appearances\\n\\nThroughout his career, Taylor Hawkins collaborated with various prominent artists and bands. The Coattail Riders' albums notably featured appearances from luminaries such as Brian May and Roger Taylor of Queen, Chrissie Hynde, Nancy Wilson of Heart, Sex Pistol Steve Jones and James Gang's Joe Walsh[28]. Hawkins also fronted another group, The Birds of Satan, which evolved from his heavy rock covers band, Chevy Metal[28].\\n\\nDespite his diverse musical engagements, Hawkins always maintained a close allegiance with the Foo Fighters, which remained the center of his music life[7][28].\\n\", raw_embedding=array([-0.01113892, -0.03466797, -0.05371094, ...,  0.0562439 ,\n",
       "         0.05731201,  0.00104904], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='e34aa8d5-b102-4ab8-8b7a-3effc7194962', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Interest Level', 'sequence': 69}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw=\"                                                                                                                                                                          |\\n| time-sensitive information                         |                  | (comment on article 2022 West Java earthquake ) too many subsections in the 'Recovery and Rehabilitation' section (comment on article 2022 West Java earthquake )                                                                                                                                                                                                                                                                   \", raw_embedding=array([-0.02485657,  0.05276489,  0.00991821, ..., -0.02445984,\n",
       "         0.0322876 ,  0.03387451], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='e6d35669-70a1-4964-ae06-0b7e497c5d7c', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'References', 'sequence': 37}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 788701), raw='for Computational Linguistics.\\n\\nPeng Qi, Yuhao Zhang, and Christopher D. Manning. 2020. Stay hungry, stay focused: Generating informative and specific questions in information-seeking conversations. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 2540, Online. Association for Computational Linguistics.\\n\\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu, Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus.\\n\\nHossein A. Rahmani, Xi Wang, Yue Feng, Qiang Zhang, Emine Yilmaz, and Aldo Lipani. 2023. A survey on asking clarification questions datasets in conversational systems. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 2698-2716, Toronto, Canada. Association for Computational Linguistics.\\n\\nAshwin Ram. 1991. A theory of questions and question asking. Journal of the Learning Sciences , 1(3-4):273318.\\n\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics .\\n\\nNils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.\\n\\nD Gordon Rohman. 1965. Pre-writing the stage of discovery in the writing process. College composition and communication , 16(2):106-112.\\n\\nChristina Sauper and Regina Barzilay. 2009. Automatically generating Wikipedia articles: A structureaware approach. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , pages 208-216, Suntec, Singapore. Association for Computational Linguistics.\\n\\nSina Semnani, Violet Yao, Heidi Zhang, and Monica Lam. 2023. WikiChat: Stopping the hallucination of large language model chatbots by few-shot grounding on Wikipedia. In Findings of the Association for Computational Linguistics: EMNLP 2023 , pages 2387-2413, Singapore. Association for Computational Linguistics.\\n\\nZejiang Shen, Tal August, Pao Siangliulue, Kyle Lo, Jonathan Bragg, Jeff Hammerbacher, Doug Downey, Joseph Chee Chang, and David Sontag. 2023. Beyond summarization: Designing ai support for realworld expository writing tasks.\\n\\nWeijia Shi, Julian Michael, Suchin Gururangan, and Luke Zettlemoyer. 2022. Nearest neighbor zero-shot inference. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 3254-3265, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. 2022. Language models that seek for knowledge: Modular search &amp; generation for dialogue and prompt completion. In Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 373-393, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021 , pages 3784-3803, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nChristine M Tardy. 2010. Writing for the world: Wikipedia as an introduction to academic writing. In English teaching forum , volume 48, page', raw_embedding=array([-0.04608154,  0.00906372, -0.00215912, ...,  0.00996399,\n",
       "         0.03320312,  0.01225281], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='e78d3c44-e552-4a1d-b6ac-c18b95ac6bd2', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '1 Introduction', 'sequence': 3}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 785699), raw=\"1 Introduction\\n\\nLarge language models (LLMs) have demonstrated impressive writing capabilities (Yang et al., 2023; Pavlik, 2023; Wenzlaff and Spaeth, 2022; Fitria, 2023), but it is unclear how we can use them to write grounded, long-form articles, like full-length Wikipedia pages. Such expository writing, which seeks to inform the reader on a topic in an organized manner (Weaver III and Kintsch, 1991; Balepur et al., 2023), requires thorough research and planning in the pre-writing stage (Rohman,\\n\\n<!-- image -->\\n\\nRole1\\n\\nFigure 1: We explore writing Wikipedia-like articles from scratch, which demands a pre-writing stage before producing the article. In this stage, simpler approaches like Direct Prompting have limited planning capacity. In contrast, STORM researches the topic via perspectiveguided question asking in simulated conversations.\\n\\n1965), even before the actual writing process can start. However, prior work on generating Wikipedia articles (Banerjee and Mitra, 2015; Minguillón et al., 2017; Liu et al., 2018; Fan and Gardent, 2022) has generally bypassed the pre-writing stage: for instance, Liu et al. (2018) presume reference documents are provided in advance, while Fan and Gardent (2022) assume an article outline is available and focus on expanding each section. These assumptions do not hold in general, as collecting references and crafting outlines demand advanced information literacy skills (Doyle, 1994) to iden- tify, evaluate, and organize external sources - a task that is challenging even for experienced writers. Automating this process can facilitate individuals in initiating in-depth learning about a topic and greatly reduce the expensive expert hours necessary for their expository writing.\\n\\nWe explore these challenges by focusing on how to generate Wikipedia-like articles from scratch . We decompose this problem into two tasks. The first is to conduct research to generate an outline, i.e. , a list of multi-level sections, and collect a set of reference documents. The second uses the outline and the references to produce the full-length article. Such a task decomposition mirrors the human writing process which usually includes phases of pre-writing, drafting, and revising (Rohman, 1965; Munoz-Luna, 2015).\\n\\nAs pre-trained language models inherently possess a wealth of knowledge, a direct approach is to rely on their parametric knowledge for generating outlines or even entire articles ( Direct Gen ). However, this approach is limited by a lack of details and hallucinations (Xu et al., 2023), particularly in addressing long-tail topics (Kandpal et al., 2023). This underscores the importance of leveraging external sources, and current strategies often involve retrieval-augmented generation ( RAG ), which circles back to the problem of researching the topic in the pre-writing stage, as much information cannot be surfaced through simple topic searches.\\n\\nHuman learning theories (Tawfik et al., 2020; Booth et al., 2003) highlight asking effective questions in information acquisition. Although instruction-tuned models (Ouyang et al., 2022) can be prompted directly to generate questions, we find that they typically produce basic 'What', 'When', and 'Where' questions (Figure 1 (A)) which often only address surface-level facts about the topic. To endow LLMs with the capacity to conduct better research, we propose the STORM paradigm for the S ynthesis\", raw_embedding=array([-0.04742432, -0.00644302, -0.02682495, ...,  0.01528168,\n",
       "        -0.00431442, -0.01130676], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='e97dcbfe-f043-4947-b5fd-5db91e3c2c29', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Interest Level', 'sequence': 65}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw='Interest Level\\n\\nTable 11: Summary of major issues found on articles produced by STORM.\\n\\n| Issue                                              | Mentioned Time   | Example Comments                                                                                                                                                                                                                                                                                                                                                                                                                                                 ', raw_embedding=array([ 0.01957703, -0.02037048,  0.01259613, ..., -0.00424957,\n",
       "         0.00701523, -0.03533936], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='eb958d73-94f9-400c-874f-57e13b668d12', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Interest Level', 'sequence': 66}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 791709), raw=\"                                                                                                                                                                                                                               |\\n|----------------------------------------------------|------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| Use of emotional words, unneutral                  | 12               | The word 'significant' is used 17 times in this article. Vague and unsupported claims are made about broader political importance and 'pivotal role[s]', and is unencyclopedic. (comment on article Lahaina, Hawaii ) [...] but they still have not fixed the issue of neutral point of view. It is also evident in this article that the writer's standpoint is biased towards Taylor Swift. Other than that, it did a good job at summarizing key points and putting depth into this. (comment on article Speak Now (Taylor's Version) ) |\\n| Red herring fallacy, associating unrelated sources | 11               | Polling from America shouldn't be included and links to climate change shouldn't be made unless explicitly connected by the source. (comment on article Typhoon Hinnamnor ) Sourcing seems mostly fine, though some aren't directly related (Ex. 39,40). (comment on article Gehraiyaan )                                                                                    \", raw_embedding=array([-0.02197266,  0.01516724, -0.03585815, ..., -0.00401688,\n",
       "        -0.0196228 ,  0.00583649], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='eca6a5a7-c3c3-48f7-bfae-2ccaf8eac7e9', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.2 LLMEvaluator', 'sequence': 47}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 789699), raw=\"C.2 LLMEvaluator\\n\\nWe use Prometheus 14 (Kim et al., 2023), a 13B open-source evaluator LLM that can assess longform text based on customized 1-5 scale rubric, to grade the article from the aspects of Interest level , Coherence and Organization , Relevance and Focus , and Coverage . Table 8 gives our grading rubric. While Prometheus is best used with a score 5 reference answer, we find adding the reference will exceed the context length limit of the model. Since Kim et al. (2023) show Prometheus ratings without reference also correlate well with human preferences, we omit the reference and trim the input article to be within 2000 words by iteratively removing contents from the shortest section to ensure the input can fit into the model's context window.\\n\", raw_embedding=array([-0.03939819, -0.00269127, -0.00822449, ..., -0.03201294,\n",
       "        -0.019104  ,  0.01199341], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='ed0d4f47-87d3-4605-b68e-9ab3479bf52d', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '### Taylor Hawkins &amp; The Coattail Riders', 'sequence': 81}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 792700), raw=\"### Taylor Hawkins &amp; The Coattail Riders\\n\\nTaylor Hawkins &amp; The Coattail Riders, a band formed in 2004, have released three albums and their music spans genres including Hard Rock, Art Rock, and Alternative Rock[24][25][26]. The band grew from an initial casual jamming session, gradually evolving into a more formal arrangement that led to the production of record albums. Notably, these albums featured guest appearances by renowned musicians such as Dave Grohl, Queen's Brian May and Roger Taylor, The Cars' Elliot Easton, Perry Farrell, and Jon Davison, who is a school friend of Hawkins'[10].\\n\", raw_embedding=array([ 0.01930237,  0.00230789, -0.04711914, ...,  0.05215454,\n",
       "         0.02601624, -0.02738953], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='f0055321-e772-41c9-9c3e-f81e42e2b9ca', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.4 STORM Implementation', 'sequence': 20}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 787707), raw='                           | 3.33             | 4.87             | 3.35             | 4.37             |\\n\\nTable 2: Results of automatic article quality evaluation. † denotes significant differences ( p &lt; 0 . 05 ) from a paired t -test between STORM and the best baseline, i.e. , oRAG. The rubric grading uses a 1-5 scale.\\n\\n|         |                  | Heading Soft Recall   | Heading Entity Recall   |\\n|---------|------------------|-----------------------|-------------------------|\\n| GPT-3.5 | Direct Gen       | 80.23                 | 32.39                   |\\n| GPT-3.5 | RAG/oRAG         | 73.59                 | 33.85                   |\\n| GPT-3.5 | RAG-expand       | 74.40                 | 33.85                   |\\n|         | STORM            | 86.26 †               | 40.52 †                 |\\n|         | w/o Perspective  | 84.49                 | 40.12                   |\\n|         | w/o Conversation | 77.97                 | 31.98                   |\\n| GPT-4   | Direct Gen       | 87.66                 | 34.78', raw_embedding=array([-0.0406189 , -0.01194   , -0.01957703, ..., -0.0158844 ,\n",
       "        -0.00035095, -0.02954102], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='f48e577d-7919-4fa2-b603-00b821d28fb1', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '2.2 Outline Creation and Evaluation', 'sequence': 8}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 786707), raw='2.2 Outline Creation and Evaluation\\n\\nA full-length article is hard to generate or evaluate (Xu et al., 2023; Krishna et al., 2023). When human educators teach students academic writing, they sometimes supervise students at the outline stage (Eriksson and Mäkitalo, 2015) because an extensive outline indicates a comprehensive understanding of the topic and provides a solid foundation for writing the full-length article (Dietz and Foley, 2019). Inspired by this, we decompose the generation of S into two stages. In the pre-writing stage, we require the system to create an outline O , which is defined as a list of multi-level section headings 6 . In the writing stage, the system uses the topic t , the references R , and an outline O to produce the full-length article S .\\n\\nTo evaluate the outline coverage, we introduce two metrics: heading soft recall and heading entity recall . These metrics compare the multi-level section headings of the human-written article, considered as ground truth, and those in O . Recognizing that an exact match between elements in these two sets of headings is unnecessary, we calculate the heading soft recall (Fränti and MariescuIstodor, 2023) using cosine similarity derived from Sentence-BERT (Reimers and Gurevych, 2019) embeddings of the headings (details in Appendix C.1). We also compute the heading entity recall which is quantified as the percentage of named entities in human-written article headings covered by O . We extract entities with FLAIR named entity recognition (NER) (Akbik et al., 2019).\\n', raw_embedding=array([-0.0308075 , -0.00959778, -0.01702881, ...,  0.02653503,\n",
       "         0.02966309, -0.00178146], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='f820c8c9-5393-490b-9546-6d9f67d269e8', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '3.3 Creating the Article Outline', 'sequence': 12}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 786707), raw='3.3 Creating the Article Outline\\n\\nAfter thoroughly researching the topic through N + 1 simulated conversations, denoted as {C 0 , C 1 , ..., C N } , STORM creates an outline before the actual writing starts. To fully leverage the internal knowledge of LLMs, we first prompt the model to generate a draft outline O D given only the topic t (Figure 2 7 ). O D typically provides a general but organized framework. Subsequently, the LLM is prompted with the topic t , the draft outline O D, and the simulated conversations {C 0 , C 1 , ..., C N } to refine the outline (Figure 2 8 ). This results in an improved outline O which will be used for producing the full-length article.\\n', raw_embedding=array([-0.01121521, -0.00078535, -0.03494263, ...,  0.00326729,\n",
       "         0.07012939, -0.06185913], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='f8ee3a32-cf3e-49c4-9afc-d30943d1c10f', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': '6 Human Evaluation', 'sequence': 26}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 787707), raw=\"editors. The ratings are given on a scale between 1 and 7, with values ≥ 4 indicating good quality (see Table 10). We conduct paired t -test and report the p -value.\\n\\nwho have made at least 500 edits on Wikipedia and have more than 1 year of experience. We randomly sample 20 topics from our dataset and evaluate the articles generated by our method and oRAG , the best baseline according to the automatic evaluation. Each pair of articles is assigned to 2 editors.\\n\\nWe request editors to judge each article from the same five aspects defined in §4.2, but using a 1 to 7 scale for more fine-grained evaluation. While our automatic evaluation uses citation quality as a proxy to evaluate Verifiability , we stick to the Wikipedia standard of 'verifiable with no original research' in human evaluation. Besides rating the articles, editors are asked to provide open-ended feedback and pairwise preference. After the evaluation finishes, they are further requested to compare an article produced by our method, which they have just reviewed, with its human-written counterpart, and report their perceived usefulness of STORM using a 1-5 Likert scale. More human evaluation details are included in Appendix D. Table 6 presents the rating and pairwise comparison results. 11\\n\\nArticles produced by STORM exhibit greater breadth and depth than oRAG outputs. In accord with the finding in §5.1, editors judge articles produced by STORM as more interesting, organized, and having broader coverage compared to oRAG outputs. Specifically, 25% more articles produced by STORM are considered organized ( Organization rating ≥ 4 ), and 10% more are deemed to have good coverage ( Coverage rating ≥ 4 ). Even in comparison with human-written articles, one editor praises our result as providing 'a bit more\\n\\n11 For the 1-7 scale rating results on each criterion, we calculate the Krippendorff's Alpha to measure the inter annotator agreement (IAA), and the results are as follows: Interest Level (0.349), Organization (0.221), Relevance (0.256), Coverage (0.346), Verifiability (0.388).\\n\\nFigure 3: Survey results of the perceived usefulness of STORM ( n = 10 ).\\n\\n<!-- image -->\\n\\nbackground information' and another notes that 'I found that the AI articles had more depth compared to the Wikipedia articles'. STORM also outperforms the best baseline in pairwise comparison.\\n\\nMore information in |R| poses challenges beyond factual hallucination. We examine 14 pairwise comparison responses where editors prefer oRAG outputs over STORM. Excluding 3 cases where pairwise preferences do not align with their ratings, editors assign lower Verifiability scores to articles from our approach in over 50% of the cases. Through analyzing the articles and editors' freeform feedback, we discover that low Verifiability scores stem from red herring fallacy or overspeculation issues . These arise when the generated articles introduce unverifiable connections between different pieces of information in |R| or between the information and the topic (examples included in Table 11). Compared to the widely discussed factual hallucination (Shuster et al., 2021; Huang et al., 2023), addressing such verifiability issues is more nuanced, surpassing basic fact-checking (Min et\", raw_embedding=array([-0.01004791, -0.0007062 , -0.01448822, ..., -0.00416946,\n",
       "        -0.03204346,  0.00568771], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652)),\n",
       " LongTerm(id='fc2ca9ca-8d16-4de4-9a4f-481182a703ca', enrich=None, combo=None, meta={'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'Algorithm 1: STORM', 'sequence': 44}, created_at=datetime.datetime(2025, 6, 28, 16, 18, 23, 789699), raw='Algorithm 1: STORM\\n\\nInput : Topic t , maximum perspective N , maximum conversation round M\\n\\nOutput: Outline O , references R\\n\\n- 1 P0 = \"basic fact writer ...\" // Constant.\\n- 2 R← [ ]\\n- 3 // Discover perspectives P .\\n- 4 related\\\\_topics ← gen\\\\_related\\\\_topics ( t )\\n- 5 tocs ← [ ]\\n- 6 foreach related\\\\_t in related\\\\_topics do\\n- 7 article ← get\\\\_wiki\\\\_article (related\\\\_t) if article then\\n\\n8\\n\\ntocs.append(\\n\\nextract\\\\_toc\\n\\n(article))\\n\\n- 9 end\\n', raw_embedding=array([ 0.02217102,  0.00322151, -0.00559616, ..., -0.00317764,\n",
       "         0.01451111, -0.03469849], shape=(1024,), dtype=float32), enrich_embedding=None, combo_embedding=None, document_id='3ec94e05-d642-433c-b4c5-74740b3f636d', updated_at=datetime.datetime(2025, 6, 28, 16, 18, 26, 208652))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.read_document_longterms(document_id=document1.id, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cee90623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown headings: max(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\broai-arai\\backend\\package\\utils\\data_loder.py:22: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: split_markdown\n",
      "  chunks = split_markdown(text)\n",
      "d:\\broai-arai\\backend\\package\\utils\\data_loder.py:23: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: consolidate_markdown\n",
      "  consolidated_chunks = consolidate_markdown(chunks)\n",
      "d:\\broai-arai\\backend\\package\\utils\\data_loder.py:24: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: get_markdown_sections\n",
      "  sections = get_markdown_sections(consolidated_chunks)\n",
      "d:\\broai-arai\\backend\\package\\utils\\data_loder.py:30: UserWarning: [EXPERIMENT] You're using an experimental module, which is subject to change in future.: split_overlap\n",
      "  new_contexts = split_overlap(contexts, max_tokens=max_tokens, overlap=overlap)\n"
     ]
    }
   ],
   "source": [
    "source_ops = SourceOptions(path=\"./sources/storm.pdf\", type=\"pdf\")\n",
    "loader = PDFLoader(\n",
    "    source=source_ops\n",
    ")\n",
    "contexts = loader.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8683b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "olf = OfflineFlow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbeefae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "longterms = olf.run(document_id=document1.id, contexts=contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55a211ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm = LongTermManagement()\n",
    "ltm.create_raws(longterms, session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13b62e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm.embed_texts(document_id=document1.id, embed_method=\"raw\", session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57733bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"What does ROUGE Score do?\"\"\"\n",
    "results = ltm.read_similar_text(query, limit=15, embed_method=\"raw\", sources=[document1.source], session=Depends(get_session))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0ddf6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.4 STORM Implementation', 'sequence': 20}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '6 Human Evaluation', 'sequence': 27}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 52}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'E Error Analysis', 'sequence': 60}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 49}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '6 Human Evaluation', 'sequence': 26}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.2 Automatic Metrics', 'sequence': 15}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.4 STORM Implementation', 'sequence': 19}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 51}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '6 Human Evaluation', 'sequence': 25}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.4 STORM Implementation', 'sequence': 17}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 53}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 54}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': 'C.3 More Discussion of the Citation Quality', 'sequence': 50}\n",
      "{'type': 'pdf', 'source': './sources/storm.pdf', 'section': '4.4 STORM Implementation', 'sequence': 18}\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f1c35ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All tables dropped.\n"
     ]
    }
   ],
   "source": [
    "from package.databases.destroy import drop_all_tables\n",
    "\n",
    "drop_all_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa901e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
